<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>python与机器学习入门二 | Pokemonlei的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="写在开头参考机器学习算法分类：  监督学习（预测）：有特征值和目标值，有标准答案 分类：对应数据类型为离散型 k-近邻算法 贝叶斯分类 决策树与随机森林 逻辑回归 神经网络   回归：对应数据类型为连续型 线性回归 岭回归   标注 隐马尔科夫模型     无监督学习：只有特征值 聚类 k-means      特征选择的方式：  过滤式： 低方差特征   嵌入式： 正则化 决策树 神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="python与机器学习入门二">
<meta property="og:url" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/index.html">
<meta property="og:site_name" content="Pokemonlei的博客">
<meta property="og:description" content="写在开头参考机器学习算法分类：  监督学习（预测）：有特征值和目标值，有标准答案 分类：对应数据类型为离散型 k-近邻算法 贝叶斯分类 决策树与随机森林 逻辑回归 神经网络   回归：对应数据类型为连续型 线性回归 岭回归   标注 隐马尔科夫模型     无监督学习：只有特征值 聚类 k-means      特征选择的方式：  过滤式： 低方差特征   嵌入式： 正则化 决策树 神经网络">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%B2%BE%E7%A1%AE%E7%8E%87%E4%B8%8E%E5%8F%AC%E5%9B%9E%E7%8E%87.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/F1.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/KNN01.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/KNN%E5%85%AC%E5%BC%8F.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/KNN%E6%95%B0%E6%8D%AE%E6%A6%82%E8%A7%88.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91%E7%B3%BB%E6%95%B0.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%A4%BA%E4%BE%8B.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B5%E5%85%AC%E5%BC%8F.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B5%E5%85%AC%E5%BC%8F2.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E8%B4%B7%E6%AC%BE%E6%95%B0%E6%8D%AE.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B51.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B52.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%86%B3%E7%AD%96%E6%A0%91%E7%BB%93%E6%9E%9C%E5%9B%BE.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%9E%8B.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%9A%E7%94%A8%E5%85%AC%E5%BC%8F.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1%E7%9A%84%E6%A0%87%E5%87%86%E5%BD%A2%E5%BC%8F.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%A0%B7%E6%9C%AC%E6%95%B0%E4%B8%BAn%E6%97%B6%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%9D%87%E6%96%B9%E5%B7%AE.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%AF%B9%E6%AF%94.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%BE%93%E5%85%A5.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/sigmoid%E5%87%BD%E6%95%B0.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%85%AC%E5%BC%8F.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%AF%B9%E6%95%B0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%9B%AE%E6%A0%87%E5%80%BC%E4%B8%BA1.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%9B%AE%E6%A0%87%E5%80%BC%E6%98%AF0.jpg">
<meta property="article:published_time" content="2020-01-11T14:12:44.000Z">
<meta property="article:modified_time" content="2020-07-07T14:07:32.305Z">
<meta property="article:author" content="pokemonlei">
<meta property="article:tag" content="python">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Pokemonlei的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/plugin/bganimation/bg.css">

  

  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <div class="widget-wrap mobile-header">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://avatars0.githubusercontent.com/u/20333903?v=3&amp;s=460">
    <h2 class="author">pokemonlei</h2>
    <h3 class="description">陈磊的博客 | pokemonlei</h3>
    <div class="count-box">
      <a href="/archives"><div><strong>11</strong><br>文章</div></a>
      <a href="/categories"><div><strong>6</strong><br>分类</div></a>
      <a href="/tags"><div><strong>11</strong><br>标签</div></a>
    </div>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

        <section id="main"><article id="post-python与机器学习入门二" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/" class="article-date">
  <time class="post-time" datetime="2020-01-11T14:12:44.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">11</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      python与机器学习入门二
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="写在开头参考"><a href="#写在开头参考" class="headerlink" title="写在开头参考"></a>写在开头参考</h1><p>机器学习算法分类：</p>
<ul>
<li>监督学习（预测）：有<strong>特征值和目标值</strong>，有标准答案<ul>
<li>分类：对应数据类型为<strong>离散型</strong><ul>
<li>k-近邻算法</li>
<li>贝叶斯分类</li>
<li>决策树与随机森林</li>
<li>逻辑回归</li>
<li>神经网络</li>
</ul>
</li>
<li>回归：对应数据类型为<strong>连续型</strong><ul>
<li>线性回归</li>
<li>岭回归</li>
</ul>
</li>
<li>标注<ul>
<li>隐马尔科夫模型</li>
</ul>
</li>
</ul>
</li>
<li>无监督学习：<strong>只有特征值</strong><ul>
<li>聚类<ul>
<li>k-means</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>特征选择的方式：</p>
<ul>
<li>过滤式：<ul>
<li>低方差特征</li>
</ul>
</li>
<li>嵌入式：<ul>
<li>正则化</li>
<li>决策树</li>
<li>神经网络</li>
</ul>
</li>
</ul>
<h4 id="精确率与召回率"><a href="#精确率与召回率" class="headerlink" title="精确率与召回率"></a>精确率与召回率</h4><p>先熟悉一个概念：混淆矩阵<br>在分类任务下，预测结果与正确标记之间存在四种不同的组合，构成混淆矩阵（适用于多分类），构成如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.jpg" class="" title="混淆矩阵">

<p>而精确率与召回率都是通过这个矩阵来算的：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%B2%BE%E7%A1%AE%E7%8E%87%E4%B8%8E%E5%8F%AC%E5%9B%9E%E7%8E%87.jpg" class="" title="精确率与召回率">

<p>其他标准：<br>F1-score，反映了模型的稳健性，是一个综合评判标准，公式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/F1.jpg" class="" title="F1">

<p>分类评估模型API：</p>
<blockquote>
<p>sklearn.metrics.classification_report(y_true,y_pred,target_name=None)</p>
</blockquote>
<ul>
<li>y_true:真实目标值</li>
<li>y_pred:估计器预测的目标值</li>
<li>target_name:目标类别名称</li>
<li>return:每个类别精确率与召回率，F1，support(预测的数量)</li>
</ul>
<h4 id="模型的选择与调优"><a href="#模型的选择与调优" class="headerlink" title="模型的选择与调优"></a>模型的选择与调优</h4><p>1.交叉验证：为了让被评估的模型更加准确可信</p>
<ul>
<li>1.将所有训练集数据分成N等分，其中一份作为验证集，其余的当作训练集，然后可以得出一个准确率</li>
<li>2.把另一份作为验证集，其余的作为训练集，再得出一个准确率</li>
<li>3.一直重复，直到所有数据都作了一次验证集，而此时可以得到N个准确率，将N个准确率求平均值。<br>分为几等分则成为 <strong>几折交叉验证</strong></li>
</ul>
<p>2.网格搜索（超参数搜索）：调参数，如k-近邻超参数K的调整<br> 通常情况下，有很多参数是需要手动指定的，如k-近邻算法中的k值，这种叫超参数。但是手动过程繁琐，所以需要对模型预设几种超参数组合。<strong>每组超参数都采用交叉验证来进行评估</strong>。最后选出最优参数组合建立模型。</p>
<blockquote>
<p>API：sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)</p>
</blockquote>
<ul>
<li>estimator:估计器对象，如实例化的KNN估计器knn = KNeighborsClassifier()</li>
<li>param_grid：估计器的参数(dict),如knn算法中指定的邻近多少个 {“n_neighbors”:{1,3,5}}</li>
<li>cv:指定几折交叉验证</li>
</ul>
<p>返回的对象可以执行</p>
<ul>
<li>fit:输入训练数据</li>
<li>score:准确率</li>
<li>best_score_ : 在交叉验证中的最好结果</li>
<li>best_estimator : 最好的参数模型</li>
<li>cv_results_ :每次交叉验证后的测试集准确率结果和训练集准确率结果</li>
</ul>
<h4 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h4><p>问题：训练数据训练的很好，误差也很小，为什么在测试集上有问题呢？</p>
<p>机器学习中一个重要的话题便是模型的泛化能力，泛化能力强的模型才是好模型，对于训练好的模型，若在训练集表现差，不必说在测试集表现同样会很差，这可能是欠拟合导致；若模型在训练集表现非常好，却在测试集上差强人意，则这便是过拟合导致的，过拟合与欠拟合也可以用 Bias 与 Variance 的角度来解释，欠拟合会导致高 Bias ，过拟合会导致高 Variance ，所以模型需要在 Bias 与 Variance 之间做出一个权衡。</p>
<p>使用简单的模型去拟合复杂数据时，会导致模型很难拟合数据的真实分布，这时模型便欠拟合了，或者说有很大的 Bias，<strong>Bias 即为模型的期望输出与其真实输出之间的差异</strong>；有时为了得到比较精确的模型而过度拟合训练数据，或者模型复杂度过高时，可能连训练数据的噪音也拟合了，导致模型在训练集上效果非常好，但泛化性能却很差，这时模型便过拟合了，或者说有很大的 Variance，这时模型在不同训练集上得到的模型波动比较大，<strong>Variance 刻画了不同训练集得到的模型的输出与这些模型期望输出的差异</strong>。</p>
<p>欠拟合原因及解决办法：</p>
<ul>
<li>原因：学习到的数据特征特少</li>
<li>解决办法：增加数据的特征数量</li>
</ul>
<p>过拟合的原因及解决办法：</p>
<ul>
<li>原因：<ul>
<li>原始特征过多，存在一些嘈杂特征</li>
</ul>
</li>
<li>解决办法：可通过交叉验证检查是否过拟合<ul>
<li>进行特征选择，消除关联性大的特征（很难做）</li>
<li>正则化</li>
</ul>
</li>
</ul>
<p><strong>L2正则化：</strong><br>作用：可以使得回归系数w的每个元素都很小，都接近0<br>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合</p>
<p>更详细的如何分辨是过拟合还是欠拟合以及如何防止，参考文章：<a href="https://zhuanlan.zhihu.com/p/29707029" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29707029</a></p>
<h4 id="模型的保存和加载"><a href="#模型的保存和加载" class="headerlink" title="模型的保存和加载"></a>模型的保存和加载</h4><blockquote>
<p>from sklearn.externals import joblib<br>scikit-learn中的文件保存格式是 pkl<br>保存：<br>joblib.dump(rf,’test.pkl’)</p>
</blockquote>
<ul>
<li>rf为fit训练好的估计器模型实例</li>
</ul>
<p>加载：<br>estimator = joblib.load(‘test.pkl’)</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。</p>
<ul>
<li>log对数损失函数：逻辑回归</li>
<li>：平方损失函数（最小二乘法）：线性回归</li>
<li>指数损失函数：Adaboost</li>
<li>Hinge损失函数：SVM</li>
<li>其它损失函数：0-1损失函数、绝对值损失函数等<br>参考：<a href="http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/" target="_blank" rel="noopener">http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/</a></li>
</ul>
<h1 id="k-近邻算法（KNN）"><a href="#k-近邻算法（KNN）" class="headerlink" title="k-近邻算法（KNN）"></a>k-近邻算法（KNN）</h1><p>####原理与优缺点<br>KNN是通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的<strong>k个最相似</strong>(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别，其中K通常是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。</p>
<p><strong>KNN算法的结果很大程度取决于K的选择</strong>，如下图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/KNN01.jpg" class="" title="KNN01">

<p>公式：在KNN中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/KNN%E5%85%AC%E5%BC%8F.jpg" class="" title="KNN公式">
<blockquote>
<p>k-近邻算法需要做标准化处理</p>
</blockquote>
<p>优点：</p>
<ul>
<li>简单，易于理解，无需参数估计，无需训练只需要一次计算就可以得出结果</li>
<li>对异常值不敏感</li>
<li>适合对稀有事件进行分类</li>
<li>可以处理多分类问题</li>
</ul>
<p>缺点：</p>
<ul>
<li>对测试样本分类时的计算量大，内存开销大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本; </li>
<li>可解释性差，无法告诉你哪个变量更重要，无法给出决策树那样的规则; </li>
<li>K值的选择：k太小，容易受异常点影响，k取值太大，容易受K值数量(类别)波动。可以采用权值的方法（和该样本距离小的邻居权值大）来改进; </li>
<li>KNN是一种消极学习方法、懒惰算法。</li>
</ul>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>样本数据：</p>
<table>
<thead>
<tr>
<th align="left">电影名称</th>
<th align="left">打斗镜头</th>
<th align="left">接吻镜头</th>
<th align="left">电影类型</th>
</tr>
</thead>
<tbody><tr>
<td align="left">California Man</td>
<td align="left">3</td>
<td align="left">104</td>
<td align="left">爱情片</td>
</tr>
<tr>
<td align="left">He’s Not Really into Dudes</td>
<td align="left">2</td>
<td align="left">100</td>
<td align="left">爱情片</td>
</tr>
<tr>
<td align="left">Beautiful woman</td>
<td align="left">1</td>
<td align="left">81</td>
<td align="left">爱情片</td>
</tr>
<tr>
<td align="left">Kevin Longblade</td>
<td align="left">101</td>
<td align="left">10</td>
<td align="left">动作片</td>
</tr>
<tr>
<td align="left">Robo Slayer 3000</td>
<td align="left">99</td>
<td align="left">5</td>
<td align="left">动作片</td>
</tr>
<tr>
<td align="left">Amped II</td>
<td align="left">98</td>
<td align="left">22</td>
<td align="left">动作片</td>
</tr>
<tr>
<td align="left">？</td>
<td align="left">18</td>
<td align="left">90</td>
<td align="left">未知</td>
</tr>
</tbody></table>
<p>如果我们通过公式（如欧氏距离）计算出已知电影与未知电影的距离如下：</p>
<table>
<thead>
<tr>
<th align="left">电影名称</th>
<th align="left">与未知电影的距离</th>
</tr>
</thead>
<tbody><tr>
<td align="left">California Man</td>
<td align="left">20.5</td>
</tr>
<tr>
<td align="left">He’s Not Really into Dudes</td>
<td align="left">18.7</td>
</tr>
<tr>
<td align="left">Beautiful woman</td>
<td align="left">19.2</td>
</tr>
<tr>
<td align="left">Kevin Longblade</td>
<td align="left">115.3</td>
</tr>
<tr>
<td align="left">Robo Slayer 3000</td>
<td align="left">117.4</td>
</tr>
<tr>
<td align="left">Amped II</td>
<td align="left">118.9</td>
</tr>
</tbody></table>
<p>按照距离递增排序，可以找到k个距离最近的电影。假定k=3，则三个最靠近的电影依次是：</p>
<ul>
<li>He’s Not Really into Dudes</li>
<li>Beautiful woman</li>
<li>California Man<br>kNN按照距离最近的三部电影的类型，决定未知电影的类型——爱情片</li>
</ul>
<h4 id="API与实战demo"><a href="#API与实战demo" class="headerlink" title="API与实战demo"></a>API与实战demo</h4><p>API：sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=’auto’)</p>
<ul>
<li>n_neighbors:int，可选，默认为5，设置查询默认使用的邻居数</li>
<li>algorithm：{‘auto’,’ball_tree’,’kd_tree’,’brute’}，可选用计算最近邻居的算法，’ball_tree’将会使用BallTree，’kd_tree’会使用KDTree，auto根据传递给fit的值来自动决定</li>
</ul>
<p>示例：facebook题目:k近邻算法预测入住位置</p>
<p>数据集介绍：<br>本次实验的数据集来自于Kaggle与Facebook合作的机器学习竞赛，旨在通过由facebook提供的2000多万条数据信息预测一个人想要登记入住的地方。<br>数据来源：<a href="https://www.kaggle.com/c/facebook-v-predicting-check-ins/data" target="_blank" rel="noopener">https://www.kaggle.com/c/facebook-v-predicting-check-ins/data</a><br>其中：<br>train.csv</p>
<ul>
<li>row_id：登记事件的ID</li>
<li>xy：坐标</li>
<li>accuracy：定位准确性</li>
<li>time：时间戳</li>
<li>place_id：业务的ID，也是预测的目标值</li>
</ul>
<p>分析思路：</p>
<ul>
<li>数据集预处理<ul>
<li>数据量太 → 缩小坐标范围</li>
<li>时间数据 → 转化为年月日（增加新的时间特征，便于后续计算）</li>
<li>类别太多 → 按照指定条件进行转换</li>
</ul>
</li>
<li>分割数据及标准化<ul>
<li>选定”place_id”为目标值，数据集预处理结果为目标值。</li>
<li>使用StandardScaler类对数据进行<strong>标准化</strong>处理。</li>
</ul>
</li>
<li>KNN分类预测<ul>
<li>在分类预测过程中，使用超参数搜索API对模型进行选择和调优。</li>
</ul>
</li>
</ul>
<p>数据的原始内容为：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/KNN%E6%95%B0%E6%8D%AE%E6%A6%82%E8%A7%88.jpg" class="" title="KNN数据概览">
<br>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knncls</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    k-近邻算法预测用户入住位置</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 读取数据</span></span><br><span class="line">    data = pd.read_csv(<span class="string">"./train.csv"</span>)</span><br><span class="line">    <span class="comment"># print(data.head(10))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征工程</span></span><br><span class="line">    <span class="comment"># 1.通过设定xy范围缩小数据</span></span><br><span class="line">    data = data.query(<span class="string">"x &gt; 1.0 &amp; x &lt; 1.25 &amp; y &gt; 2.5 &amp; y &lt; 2.75"</span>)  <span class="comment"># query相当于一个查询语句，参数为查询条件</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.处理时间戳数据</span></span><br><span class="line">    time_value = pd.to_datetime(data[<span class="string">'time'</span>], unit=<span class="string">'s'</span>)  <span class="comment"># 将时间戳转换为年-月-日 时-分-秒格式</span></span><br><span class="line">    <span class="comment"># print(time_value)</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    timevalue的值为：</span></span><br><span class="line"><span class="string">    0          1970-01-06 10:45:02</span></span><br><span class="line"><span class="string">    1          1970-01-03 03:49:15</span></span><br><span class="line"><span class="string">    2          1970-01-04 17:37:28</span></span><br><span class="line"><span class="string">    3          1970-01-09 03:43:07</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 把日期转换为字典格式</span></span><br><span class="line">    time_value = pd.DatetimeIndex(time_value)</span><br><span class="line">    <span class="comment"># 3.添加构造一些特征</span></span><br><span class="line">    data[<span class="string">'day'</span>] = time_value.day</span><br><span class="line">    data[<span class="string">'hour'</span>] = time_value.hour</span><br><span class="line">    data[<span class="string">'weekday'</span>] = time_value.weekday</span><br><span class="line">    <span class="comment"># 4.把时间戳特征删除,sklearn中axis为1表示列</span></span><br><span class="line">    data = data.drop([<span class="string">'time'</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.只保留入住人数大于3的目标位置,生成新的data</span></span><br><span class="line">    place_count = data.groupby(<span class="string">"place_id"</span>).count()  <span class="comment"># 按照'place_id'进行分组,然后统计次数。groupby使用参考：https://blog.csdn.net/m0_37870649/article/details/80979809</span></span><br><span class="line">    <span class="comment"># print(place_count)</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    此时print的place_count如下：</span></span><br><span class="line"><span class="string">    此时place_id为索引，而原来的特征值都变为了对应索引的place_id出现了多少次</span></span><br><span class="line"><span class="string">    place_id    row_id    x    y  accuracy  day  hour  weekday                                        </span></span><br><span class="line"><span class="string">    1000015801      78   78   78        78   78    78       78</span></span><br><span class="line"><span class="string">    1000017288      95   95   95        95   95    95       95</span></span><br><span class="line"><span class="string">    1000025138     563  563  563       563  563   563      563</span></span><br><span class="line"><span class="string">    1000052096     961  961  961       961  961   961      961</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    place_count_r = place_count[</span><br><span class="line">        place_count.row_id &gt; <span class="number">3</span>].reset_index()  <span class="comment"># 筛选place_count中row_id大于3的，然后通过reset_index()把place_id重新设为特征而不是索引</span></span><br><span class="line">    train_data = data[data[<span class="string">"place_id"</span>].isin(place_count_r[<span class="string">"place_id"</span>])]     <span class="comment">#通过对比筛选</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取特征值和目标值</span></span><br><span class="line">    x = train_data.drop([<span class="string">"place_id"</span>, <span class="string">"row_id"</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    y = train_data[<span class="string">"place_id"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分割数据集</span></span><br><span class="line">    <span class="comment">#参数：特征值，目标值，测试集比例。返回值依次为：训练集特征值、测试集特征值，训练集目标值，测试集目标值</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对训练集和测试集的特征值进行标准化</span></span><br><span class="line">    std = StandardScaler()</span><br><span class="line"></span><br><span class="line">    x_train = std.fit_transform(x_train)</span><br><span class="line">    x_test = std.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化knn估计器</span></span><br><span class="line">    knn = KNeighborsClassifier()    <span class="comment"># n_neighbors，默认5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#knn.fit(x_train, y_train)   #参数：训练集特征值、训练集目标值</span></span><br><span class="line">    <span class="comment"># 预测结果</span></span><br><span class="line">    <span class="comment">#y_predict = knn.predict(x_test) #参数：测试集</span></span><br><span class="line">    <span class="comment"># 打印准确率</span></span><br><span class="line">    <span class="comment">#print("准确率为:", knn.score(x_test, y_test))</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    准确率为: 0.4732860520094563</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进阶思考：</span></span><br><span class="line">    <span class="comment"># 交叉验证与网格搜索对K-近邻算法调优</span></span><br><span class="line">    param = &#123;<span class="string">"n_neighbors"</span>: [i * <span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">5</span>)]&#125;</span><br><span class="line">    gc = GridSearchCV(knn, param_grid=param, cv=<span class="number">5</span>)</span><br><span class="line">    gc.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"在测试集中的准确率："</span>, gc.score(x_test, y_test))</span><br><span class="line">    print(<span class="string">"在交叉验证中验证集的最好结果："</span>, gc.best_score_)</span><br><span class="line">    print(<span class="string">"使用的最好的模型："</span>, gc.best_estimator_)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    在测试集中的准确率： 0.47848699763593383</span></span><br><span class="line"><span class="string">    在交叉验证中验证集的最好结果： 0.47540983606557374</span></span><br><span class="line"><span class="string">    使用的最好的模型： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',</span></span><br><span class="line"><span class="string">           metric_params=None, n_jobs=None, n_neighbors=8, p=2,</span></span><br><span class="line"><span class="string">           weights='uniform')</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    knncls()</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h1><blockquote>
<p>使用前提：特征之间独立，不互相影响</p>
</blockquote>
<p>该算法是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题</p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>条件概率中： P(A|B)表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。其基本求解公式为：P(A|B) = P(AB)/P(B)。<br>通常我们可以很容易直接得出P(A|B)，P(B|A)则很难直接得出，但我们更关心P(B|A)，贝叶斯定理就为我们打通从P(A|B)获得P(B|A)的道路。<br>贝叶斯公式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F.jpg" class="" title="贝叶斯公式">
<p>其中：</p>
<ul>
<li>P(类别)指每个文档类别的概率，计算方式 (某文档类别数 / 总文档数量)</li>
<li>P(特征|类别) = 指给定类别下特征的概率，计算方式：P(F1|C) = Ni/N<ul>
<li>Ni为该F1特征词在C类别所有文档中出现的次数</li>
<li>N为所属类别C下的文档所有词出现的次数和</li>
</ul>
</li>
<li>在比较一个文章属于不同文章类型的概率时，会发现分母P(特征)是相同的，所以可以省略掉</li>
</ul>
<p><strong>拉普拉斯平滑：</strong><br>如果词频列表里面有很多出现次数都为0的词，在计算的时候可能使计算结果都变为0，这是不合理的，解决方式就是增加拉普拉斯修正</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91%E7%B3%BB%E6%95%B0.jpg" class="" title="拉普拉斯平滑系数">
<p>其中，α为指定的系数，一般为1，m为训练文档中统计出的特征词个数。</p>
<p>朴素贝叶斯分类的流程可以由下图表示：</p>


<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>优点：</p>
<ul>
<li>算法逻辑简单,易于实现</li>
<li>分类过程中时空开销小</li>
<li>对缺失数据不太敏感</li>
<li>对小规模的数据表现很好，能处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练。</li>
</ul>
<p>缺点：<br>理论上，<strong>朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好</strong>。</p>
<h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><p>一个简单示例如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%A4%BA%E4%BE%8B.jpg" class="" title="贝叶斯示例">
<p>可以看到由于云计算在娱乐文章里出现的频率是0，所以导致最后结果是0，这里需要进行拉普拉斯平滑处理。<br>即将P(影院，支付宝，云计算|娱乐)计算时的 分子+1，分母+4，</p>
<h4 id="API与实战demo-1"><a href="#API与实战demo-1" class="headerlink" title="API与实战demo"></a>API与实战demo</h4><p>pythonAPI：sklearn.naive_bayes.MultinomialNB(alpha = 1.0)</p>
<ul>
<li>alpha：拉普拉斯平滑系数，默认1</li>
</ul>
<p>此处的实例用的还是之前内置的sklearn20类新闻分类的数据，20个新闻组数据包含了20个主题的18000个新闻组帖子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naviebayes</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    朴素贝叶斯对新闻进行分类</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    news = fetch_20newsgroups(subset=<span class="string">'all'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行分割</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对数据集进行特征抽取,用tf-idf方法</span></span><br><span class="line">    tf = TfidfVectorizer()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 以训练集中的词的列表进行每篇文章重要性统计</span></span><br><span class="line">    x_train = tf.fit_transform(x_train)  <span class="comment"># 此时x_train为sparse矩阵，大概内容就是记录着:(行，列) 对应数值</span></span><br><span class="line">    print(tf.get_feature_names())</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    特征名，也就是每个单词</span></span><br><span class="line"><span class="string">    'davidst', 'davidstern', 'davidw', 'davie', 'daviel', 'davies', 'davinci', 'davis', 'davison',</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    x_test = tf.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行朴素贝叶斯算法的预测</span></span><br><span class="line">    mlt = MultinomialNB(alpha=<span class="number">1.0</span>)</span><br><span class="line">    print(x_train.toarray())</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    大多数是0，因为一篇文章不是所有的词都有，每一行代表一篇文章，列就是上面的特征名，即单词，行列对应值为TF-IDF值</span></span><br><span class="line"><span class="string">    [[0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="string">    [0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="string">    ..................</span></span><br><span class="line"><span class="string">    [0. 0. 0. ... 0. 0. 0.]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    mlt.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">    y_predict = mlt.predict(x_test)</span><br><span class="line">    print(<span class="string">"预测的文章类别为："</span>, y_predict)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    预测的文章类别为： [ 7 16  0 ...  4  8  1]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得出准确率</span></span><br><span class="line">    print(<span class="string">"准确率为："</span>, mlt.score(x_test, y_test))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    准确率为： 0.8293718166383701</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    naviebayes()</span><br></pre></td></tr></table></figure>


<h5 id="番外篇，R语言与朴素贝叶斯"><a href="#番外篇，R语言与朴素贝叶斯" class="headerlink" title="番外篇，R语言与朴素贝叶斯"></a>番外篇，R语言与朴素贝叶斯</h5><blockquote>
<p>R语言中的klaR包提供了朴素贝叶斯算法实现的函数NaiveBayes<br>NaiveBayes(formula, data, …, subset, na.action= na.pass)<br>NaiveBayes(x, grouping, prior, usekernel= FALSE, fL = 0, …)</p>
</blockquote>
<ul>
<li>formula指定参与模型计算的变量，以公式形式给出，类似于y=x1+x2+x3；</li>
<li>data用于指定需要分析的数据对象；</li>
<li>na.action指定缺失值的处理方法，默认情况下不将缺失值纳入模型计算，也不会发生报错信息，当设为“na.omit”时则会删除含有缺失值的样本；</li>
<li>x指定需要处理的数据，可以是数据框形式，也可以是矩阵形式；</li>
<li>grouping为每个观测样本指定所属类别；</li>
<li>prior可为各个类别指定先验概率，默认情况下用各个类别的样本比例作为先验概率；</li>
<li>usekernel指定密度估计的方法（在无法判断数据的分布时，采用密度密度估计方法），默认情况下使用正态分布密度估计，设为TRUE时，则使用核密度估计方法；</li>
<li>fL指定是否进行拉普拉斯修正，默认情况下不对数据进行修正，当数据量较小时，可以设置该参数为1，即进行拉普拉斯修正。</li>
</ul>
<h1 id="决策树-Decision-Tree-和随机森林-Random-Forest"><a href="#决策树-Decision-Tree-和随机森林-Random-Forest" class="headerlink" title="决策树(Decision Tree)和随机森林(Random Forest)"></a>决策树(Decision Tree)和随机森林(Random Forest)</h1><ul>
<li>决策树是用树的结构来构建分类模型，每个节点代表着一个属性，根据这个属性的划分，进入这个节点的儿子节点，直至叶子节点，每个叶子节点都表征着一定的类别，从而达到分类的目的。<ul>
<li>常用的决策树有ID4，C4.5，CART等。在生成树的过程中，需要选择用那个特征进行剖分，一般来说，选取的原则是，分开后能尽可能地提升纯度，可以用信息增益，增益率，以及基尼系数等指标来衡量。如果是一棵树的话，为了避免过拟合，还要进行剪枝（prunning），取消那些可能会导致验证集误差上升的节点。</li>
</ul>
</li>
<li>随机森林实际上是一种特殊的bagging方法，它将决策树用作bagging中的模型。首先，用bootstrap方法生成m个训练集，然后，对于每个训练集，构造一颗决策树，在节点找特征进行分裂的时候，并不是对所有特征找到能使得指标（如信息增益）最大的，而是在特征中随机抽取一部分特征，在抽到的特征中间找到最优解，应用于节点，进行分裂。随机森林的方法由于有了bagging，也就是集成的思想在，实际上相当于对于样本和特征都进行了采样（如果把训练数据看成矩阵，就像实际中常见的那样，那么就是一个行和列都进行采样的过程），所以可以避免过拟合。</li>
</ul>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h4 id="信息论基础"><a href="#信息论基础" class="headerlink" title="信息论基础"></a>信息论基础</h4><p>在信息论中，熵（英语：entropy）是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。<br>熵最好理解为不确定性的量度而不是确定性的量度，因为<strong>越随机的信源的熵越大</strong>。</p>
<p><strong>香农的信息熵理论：</strong><br>如果n个事件发生的概率分别为p1,p2,p3….pn则他们的准确信息量应该是：<br>H = -(p1Logp1 + p2logp2 + p3logp3 + … + pnlogpn)<br>H的专业术语为信息熵，单位为比特，公式：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B5%E5%85%AC%E5%BC%8F.jpg" class="" title="信息熵公式">

<p>例如，当有32支球队比赛时，猜测谁是冠军，在没有任何信息的情况下，每个球队获胜的概率都是 1/32，此时信息熵为<br>H = -(1/32Log1/32 + 1/32log1/32 + 1/32log1/32 + … + 1/32log1/32) = 5<br>而当我们得到一些信息的情况下，比如不同球队的获胜几率为多少，那我们最终得出的信息熵H将会小于5。</p>
<p><strong>信息增益：</strong><br>当得知一个特征条件之后，减少的信息熵的大小。</p>
<h4 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h4><p><strong>决策树划分依据</strong></p>
<ol>
<li>ID3：信息增益最大<br>特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与 特征A给出条件下D的信息条件熵H(D|A) 之差，即公式:<br>g(D,A) = H(D) - H(D|A)<br>此处信息增益表示，得知特征A的信息而使得类D的信息的不确定性减少的程度，计算H(D)和H(D|A)公式：<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B5%E5%85%AC%E5%BC%8F2.jpg" class="" title="信息熵公式2">

</li>
</ol>
<p>上面这公式有点复杂，举个例子：<br>有如下银行贷款的数据，通过前面的特征来决定是否可以贷款。</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E8%B4%B7%E6%AC%BE%E6%95%B0%E6%8D%AE.jpg" class="" title="贷款数据">
<p>对于没有加入特征之前，只看是否贷款数据的信息熵：一共有15条数据，9条可以贷款，6条不可以。所以此时总的信息熵为：0.971，计算方式如下</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B51.jpg" class="" title="信息熵1">
<p>然后我们让A1，A2,A3,A4分别表示年龄，工作，有自己房子和信贷情况四个特征，则分别计算他们的信息增益即可选择下一次决策用什么特征。<br>此处以年龄为例，而年龄中青年、中年、老年各占5个，即 1/3，根据公式，年龄的信息增益计算为：<br>g(D,A1) = H(D) - H(D’|年龄) = 0.971 - [1/3H(青年) + 1/3H(中年)+ 1/3H(老年)]，<br>此时需要计算H(青年)针对类别的信息熵，由于5个青年中，有3个类别是‘否’，2个对应类别是‘是’，所以计算方式如下：<br>H(青年)= -(2/log2/5 + 3/5log3/5)<br>中年和老年的计算方式类似，最终结果为：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B52.jpg" class="" title="信息熵2">
<p>同理，其他的特征信息增益也可以计算出来，g(D,A2)=0.324,g(D,A3)= 0.420,g(D,A4)= 0.363，相比较A3所对应的有自己的房子这个特征的信息增益最大，所以特征A3为最有特征的一个属性。</p>
<ol start="2">
<li>C4.5：信息增益比最大</li>
<li>CART：回归树<ul>
<li>回归树：平方误差最小</li>
<li>分类树：基尼系数（Gini）最小，基尼系数的划分更加细致</li>
</ul>
</li>
</ol>
<h4 id="决策树优缺点"><a href="#决策树优缺点" class="headerlink" title="决策树优缺点"></a>决策树优缺点</h4><p>优点：</p>
<ul>
<li>简单的理解和解释，树木可视化</li>
<li>不需要太多的数据准备操作，其他技术通常需要数据归一化等操作</li>
</ul>
<p>缺点：</p>
<ul>
<li>决策树可能过于复杂，而且训练集中正确率高不代表测试集中正确率高，有些异常点会导致结果有误。过拟合</li>
</ul>
<p>改进：</p>
<ul>
<li>剪枝cart算法</li>
<li>随机森林</li>
</ul>
<h4 id="经典案例：泰坦尼克号乘幸存预测"><a href="#经典案例：泰坦尼克号乘幸存预测" class="headerlink" title="经典案例：泰坦尼克号乘幸存预测"></a>经典案例：泰坦尼克号乘幸存预测</h4><blockquote>
<p>决策树分类器API：sklearn.tree.DecisionTreeClassifier(criterion=’gini’,max_depth=None,random_state=None)</p>
</blockquote>
<ul>
<li>criterion:默认是‘gini’系数，也可以选择信息增益的熵’entropy’</li>
<li>max_depth:树的深度大小</li>
<li>random_state:随机数种子<br>method：</li>
<li>decision_path:返回决策树的路径</li>
</ul>
<blockquote>
<p>决策树的结构，本地保存API：sklearn.tree.export_graphviz(extimator,outfile=”tree.dot”,feature_names=[‘’,’’])</p>
</blockquote>
<ul>
<li>extimator：实例化的估计器对象</li>
<li>outfile：保存路径和文件，必须是后缀dot格式，这格式课方便的转换为pdf，png等格式</li>
<li>feature_names:特征名，可自己指定</li>
</ul>
<ul>
<li>用软件graphviz运行命令：dot -Tpng tree.dot -o tree.png 来查看，tree.png改为tree.pdf则转换为pdf</li>
</ul>
<p>案例数据：<a href="http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt" target="_blank" rel="noopener">http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt</a><br>数据中包含的特征：票的类别，存活结果，乘坐版（pclass），年龄，登陆，home.dest，房间，票，船和性别等。其中乘坐班是指乘客班(1,2,3),是社会经济阶层的代表。其中age数据还存在缺失需要处理。<br>处理流程：</p>
<ul>
<li>pd读取数据</li>
<li>选择有影响的特征，处理缺失值</li>
<li>进行特征工程，pd转换字典，特征抽取x_train.to_dict(orient=”recored”)</li>
<li>决策树估计器流程</li>
</ul>
<p>案例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier,export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decision</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    决策树对泰坦尼克号乘客进行预测生死</span></span><br><span class="line"><span class="string">    :return:None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    titan = pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理数据，找出特征值和目标值,这里以三个为例</span></span><br><span class="line">    x = titan[[<span class="string">'pclass'</span>, <span class="string">'age'</span>, <span class="string">'sex'</span>]]</span><br><span class="line">    print(x)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">         pclass      age     sex</span></span><br><span class="line"><span class="string">    0       1st  29.0000  female</span></span><br><span class="line"><span class="string">    1       1st   2.0000  female</span></span><br><span class="line"><span class="string">    2       1st  30.0000    male</span></span><br><span class="line"><span class="string">    3       1st  25.0000  female</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    y = titan[<span class="string">'survived'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 缺失值处理</span></span><br><span class="line">    x[<span class="string">'age'</span>].fillna(x[<span class="string">'age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分割数据集到训练集和测试集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行特征工程，当特征是类别的时候，要进行ont-hot编码</span></span><br><span class="line">    dict = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    x_train = dict.fit_transform(x_train.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">    print(dict.get_feature_names())</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(x_train)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    刚好与上面对应</span></span><br><span class="line"><span class="string">    [[29.         1.          0.          0.          1.          0.        ]</span></span><br><span class="line"><span class="string">    [ 2.          1.          0.          0.          1.          0.        ]</span></span><br><span class="line"><span class="string">    [30.          1.          0.          0.          0.          1.        ]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x_test = dict.transform(x_test.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用决策树估计器进行预测</span></span><br><span class="line">    dec = DecisionTreeClassifier()  <span class="comment"># max_depth是个超参数，值可以影响结果，也决定了决策树的深度</span></span><br><span class="line">    dec.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#预测准确率</span></span><br><span class="line">    print(<span class="string">"DecisionTreeClassifier无参数时，预测的准确率为："</span>,dec.score(x_test,y_test))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    DecisionTreeClassifier无参数时，预测的准确率为： 0.8206686930091185</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#导出决策树的结构，截图在之后给出</span></span><br><span class="line">    export_graphviz(dec,out_file=<span class="string">"./tree.dot"</span>,feature_names=[<span class="string">'age'</span>, <span class="string">'pclass=1st'</span>, <span class="string">'pclass=2nd'</span>, <span class="string">'pclass=3rd'</span>, <span class="string">'sex=女性'</span>, <span class="string">'sex=男性'</span>])</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    decision()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上述代码保存后的决策树结构图</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%86%B3%E7%AD%96%E6%A0%91%E7%BB%93%E6%9E%9C%E5%9B%BE.jpg" class="" title="决策树结果图">


<h2 id="随机森林-集成学习方法"><a href="#随机森林-集成学习方法" class="headerlink" title="随机森林-集成学习方法"></a>随机森林-集成学习方法</h2><h4 id="集成学习方法"><a href="#集成学习方法" class="headerlink" title="集成学习方法"></a>集成学习方法</h4><p>集成学习（Ensemble learning）通过组合几种模型来提高机器学习的效果。与单一模型相比，该方法可以提供更好的预测结果。<br>集成学习有一个重要的概念叫<strong>Diversity</strong>，也就是说每个分类器要长得尽量不一样。就像一个团队，有的人擅长策划，有人擅长拉赞助，有人擅长执行，这样才是一个牛逼的团队。集成学习中每个分类器也要有一定差异，这样才是一个好的集成。</p>
<p>集成学习算法主要有Boosting和Bagging两种类型：</p>
<ul>
<li>Boosting：通过迭代地训练一系列的分类器，<strong>每个分类器采用的样本的选择方式都和上一轮的学习结果有关</strong>。例如在AdaBoost中，之前分类错误的样本有较高的可能性被选到，在之前分类正确的样本有较小的概率被选到。就像你在背单词，你今天总会选择前面几天背不下来的单词，Boosting也会选择前面轮数没学下来的样本。这个类别下面的主要算法是AdaBoost和GBDT。</li>
<li>Bagging：每个分类器都随机从原样本中做<strong>有放回的采样</strong>，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。随机森林就是这个类别的一个经典算法，仔细学习你会发现它的每棵树和基本的决策树的不同，非常神奇的算法。</li>
</ul>
<p>额外拓展：集成方法是将几种机器学习技术组合成一个预测模型的元算法，以达到减小方差（bagging）、偏差（boosting）或改进预测（stacking）的效果。</p>
<h4 id="随机森林的定义，过程，优势"><a href="#随机森林的定义，过程，优势" class="headerlink" title="随机森林的定义，过程，优势"></a>随机森林的定义，过程，优势</h4><p>定义：在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。</p>
<p>过程：</p>
<ol>
<li>单个树的建立过程：假如有N个样本，M个特征，随机有放回的抽样，即:Bagging:Bootstrap Aggregating<ul>
<li>随机在N个样本中有放回的选择一个样本，重复N次，因为有放回所以可能有重复</li>
<li>随机在M个特征当中选出m个特征<ul>
<li>m的取值：m&lt;&lt;M</li>
</ul>
</li>
</ul>
</li>
<li>重复1过程，建立多颗决策树，他们的样本和特征大多不一样。 </li>
</ol>
<p>优势：</p>
<ul>
<li>当前所有算法中，具有极好的准确率</li>
<li>能够有效地运行在大数据集上</li>
<li>能够处理具有高维特征的输入样本，而且不需要降维</li>
<li>能够评估各个特征在分类问题上的重要性</li>
</ul>
<h4 id="随机森林与泰坦尼克号乘客幸存预测"><a href="#随机森林与泰坦尼克号乘客幸存预测" class="headerlink" title="随机森林与泰坦尼克号乘客幸存预测"></a>随机森林与泰坦尼克号乘客幸存预测</h4><blockquote>
<p>随机森林API:class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)<br>列举几个重要参数，参数实在太多，超参数也有很多了，需要的时候再搜索好了：</p>
</blockquote>
<ul>
<li>n_estimators : integer，可选default=10,森林里（决策）树的数目</li>
<li>criterion : string, 可选(默认值为“gini”)，衡量分裂质量的性能（函数）。 受支持的标准是基尼不纯度的”gini”,和信息增益的”entropy”（熵）</li>
<li>max_features： int, float, string or None,可选default=”auto”，寻找最佳分割时需要考虑的特征数目，不同的值代表不同的考虑方式：这里n_features为总的特征数<ul>
<li>如果是int，就要考虑每一次分割处的max_feature特征</li>
<li>如果是float，那么max_features就是一个百分比，那么（max_feature*n_features）特征整数值是在每个分割处考虑的</li>
<li>如果是auto，那么max_features=sqrt(n_features)，即n_features的平方根值。</li>
<li>如果是log2，那么max_features=log2(n_features)</li>
<li>如果是None,那么max_features=n_features</li>
</ul>
</li>
<li>max_depth : integer or None, 可选的（默认为None），（决策）树的最大深度。如果值为None，那么会扩展节点，直到所有的叶子是纯净的，或者直到所有叶子包含少于min_sample_split的样本。</li>
<li>bootstrap : boolean, 可选的(default=True)，建立决策树时，是否使用有放回抽样。</li>
</ul>
<p>代码示例：处理数据与之前的决策树一样，只是把决策树估计器改成了随机森林，然后进行了网格搜索与交叉验证</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decision</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    决策树对泰坦尼克号乘客进行预测生死</span></span><br><span class="line"><span class="string">    :return:None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    titan = pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理数据，找出特征值和目标值,这里以三个为例</span></span><br><span class="line">    x = titan[[<span class="string">'pclass'</span>, <span class="string">'age'</span>, <span class="string">'sex'</span>]]</span><br><span class="line">    print(x)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">         pclass      age     sex</span></span><br><span class="line"><span class="string">    0       1st  29.0000  female</span></span><br><span class="line"><span class="string">    1       1st   2.0000  female</span></span><br><span class="line"><span class="string">    2       1st  30.0000    male</span></span><br><span class="line"><span class="string">    3       1st  25.0000  female</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    y = titan[<span class="string">'survived'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 缺失值处理</span></span><br><span class="line">    x[<span class="string">'age'</span>].fillna(x[<span class="string">'age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分割数据集到训练集和测试集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行特征工程，当特征是类别的时候，要进行ont-hot编码</span></span><br><span class="line">    dict = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    x_train = dict.fit_transform(x_train.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">    print(dict.get_feature_names())</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(x_train)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    刚好与上面对应</span></span><br><span class="line"><span class="string">    [[29.         1.          0.          0.          1.          0.        ]</span></span><br><span class="line"><span class="string">    [ 2.          1.          0.          0.          1.          0.        ]</span></span><br><span class="line"><span class="string">    [30.          1.          0.          0.          0.          1.        ]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x_test = dict.transform(x_test.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机森林估计器进行预测，超参数调优(这里以两个参数为例）</span></span><br><span class="line">    rf = RandomForestClassifier()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#网格搜索与交叉验证寻找最佳参数</span></span><br><span class="line">    param = &#123;<span class="string">"n_estimators"</span>:[<span class="number">10</span>,<span class="number">20</span>,<span class="number">100</span>,<span class="number">120</span>,<span class="number">200</span>,<span class="number">300</span>,<span class="number">500</span>,<span class="number">800</span>],<span class="string">"max_depth"</span>:[<span class="number">5</span>,<span class="number">8</span>,<span class="number">15</span>,<span class="number">20</span>,<span class="number">30</span>]&#125;</span><br><span class="line">    gc = GridSearchCV(rf,param_grid=param,cv=<span class="number">2</span>)</span><br><span class="line">    gc.fit(x_train,y_train)</span><br><span class="line">    print(<span class="string">"准确率为："</span>,gc.score(x_test,y_test))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    准确率为： 0.8267477203647416</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"选择的参数模型："</span>,gc.best_params_)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    选择的参数模型： &#123;'max_depth': 5, 'n_estimators': 10&#125;</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    decision()</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="线性回归，迭代的算法"><a href="#线性回归，迭代的算法" class="headerlink" title="线性回归，迭代的算法"></a>线性回归，迭代的算法</h1><blockquote>
<p>回归问题：目标值是连续的，如房价，营业额等。</p>
</blockquote>
<h4 id="定义与原理"><a href="#定义与原理" class="headerlink" title="定义与原理"></a>定义与原理</h4><p>线性关系模型：<br>一个通过属性的线性组合来进行预测的函数:</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%9E%8B.jpg" class="" title="线性关系模型">
<p>w为权重，b称为偏置项，可理解为w0*1</p>
<p>线性回归定义：</p>
<blockquote>
<p>在统计学中，线性回归（Linear regression）是利用称为线性回归方程的<a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95" target="_blank" rel="noopener">最小二乘</a> 函数对一个或多个<strong>自变量和因变量之间关系进行建模</strong>的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。</p>
</blockquote>
<p>通用公式：<br>x为特征值，w为要求的值</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%9A%E7%94%A8%E5%85%AC%E5%BC%8F.jpg" class="" title="线性回归通用公式">


<h4 id="线性回归的损失函数：平方损失函数（最小二乘法-Ordinary-Least-Squares-）"><a href="#线性回归的损失函数：平方损失函数（最小二乘法-Ordinary-Least-Squares-）" class="headerlink" title="线性回归的损失函数：平方损失函数（最小二乘法, Ordinary Least Squares ）"></a>线性回归的损失函数：平方损失函数（最小二乘法, Ordinary Least Squares ）</h4><p>最小二乘法是线性回归的一种，OLS将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从<a href="https://zh.wikipedia.org/wiki/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83" target="_blank" rel="noopener">高斯分布</a>（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：<strong>最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小</strong>。换言之，OLS是基于距离的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：</p>
<ul>
<li>简单，计算方便</li>
<li>欧氏距离是一种很好的相似性度量标准</li>
<li>在不同的表示域变换后特征性质不变</li>
</ul>
<p>下面的式子，f(x)：模型的预测值。Y：真实值<br>平方损失（Square loss）的标准形式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1%E7%9A%84%E6%A0%87%E5%87%86%E5%BD%A2%E5%BC%8F.jpg" class="" title="平方损失的标准形式">

<p>当样本个数为n时，此时的损失函数变为：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%A0%B7%E6%9C%AC%E6%95%B0%E4%B8%BAn%E6%97%B6%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.jpg" class="" title="样本数为n时的损失函数">

<p>Y-f(X)表示的是残差，整个式子表示的是残差的平方和，而我们的目的就是<strong>最小化这个目标函数值</strong>（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。</p>
<p><strong>而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标</strong>，公式如下:假设有n个数据，后面的减法为 预测值-真实值</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%9D%87%E6%96%B9%E5%B7%AE.jpg" class="" title="均方差">

<p>我们通常说的线性有两种情况，一种是因变量y是自变量x的线性函数，一种是因变量y是参数α的线性函数。在机器学习中，通常指的都是后一种情况。</p>
<h4 id="两种求解方法"><a href="#两种求解方法" class="headerlink" title="两种求解方法"></a>两种求解方法</h4><p>从前面得知，线性回归要求每个特征对应的权重w值，使得线性回归的损失函数值最小，有两种方法：</p>
<ol>
<li>最小二乘法之正规方程（不常用）<br>X为特征值矩阵，y为目标值矩阵，求解方程如下：</li>
</ol>
<p>缺点：当特征过于复杂，求解速度太慢</p>
<ol start="2">
<li>最小二乘法之梯度下降SGD（理解过程）<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.jpg" class="" title="梯度下降">
线性回归中一直在迭代，刚开始是原始值减去后面的学习速率*方向，得到一个新的值，下次迭代用这个新的值继续减去 学习速率*方向。<br>使用：面对训练数据规模十分庞大的任务</li>
</ol>
<p>正规方程与梯度下降对比：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%AF%B9%E6%AF%94.jpg" class="" title="对比">


<h4 id="API与案例"><a href="#API与案例" class="headerlink" title="API与案例"></a>API与案例</h4><blockquote>
<p>sklearn.linear_model.LinearRegression()</p>
</blockquote>
<ul>
<li>普通最小二乘线性回归</li>
<li>coef_：求出的回归系数</li>
</ul>
<blockquote>
<p>sklearn.linear_model.SGDRegressor()</p>
</blockquote>
<ul>
<li>通过使用SGD最小化线性模型</li>
<li>coef_:求出的回归系数</li>
</ul>
<p>均方差算回归损失的API：</p>
<blockquote>
<p>sklearn.metrics.mean_squared_error(y_true,y_pred)</p>
</blockquote>
<ul>
<li>y_true:真实值</li>
<li>y_pred:预测值</li>
<li>return：浮点数结果</li>
<li>真实值和预测值为标准化之前的值</li>
</ul>
<p>案例：波士顿房价预测<br>流程：</p>
<ol>
<li>数据获取</li>
<li>数据分割</li>
<li>训练与测试数据<strong>标准化处理</strong></li>
<li>使用最简单的线性回归模型LinearRegression和SGDRegressor对房价进行预测</li>
</ol>
<p>代码案例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, SGDRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mylinear</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    线性回归预测房子价格</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    lb = load_boston()</span><br><span class="line">    <span class="comment"># 分割数据集到训练集和测试集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(lb.data, lb.target, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行标准化</span></span><br><span class="line">    <span class="comment"># 这里特征值和目标值都要进行</span></span><br><span class="line">    std_x = StandardScaler()  <span class="comment"># 对特征值进行标准化</span></span><br><span class="line">    x_train = std_x.fit_transform(x_train)</span><br><span class="line">    x_test = std_x.transform(x_test)</span><br><span class="line"></span><br><span class="line">    std_y = StandardScaler()  <span class="comment"># 对目标值进行标准化</span></span><br><span class="line">    y_train = std_y.fit_transform(y_train.reshape(<span class="number">-1</span>, <span class="number">1</span>))  <span class="comment"># 要求是二维</span></span><br><span class="line">    y_test = std_y.fit_transform(y_test.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># estimator预测</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 正规方程求解方式预测</span></span><br><span class="line">    lr = LinearRegression()</span><br><span class="line">    lr.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"回归系数为："</span>, lr.coef_)</span><br><span class="line">    <span class="string">''''</span></span><br><span class="line"><span class="string">    回归系数为： [[-0.13482789  0.14684301  0.01549439  0.03522313 -0.21053828  0.32880086</span></span><br><span class="line"><span class="string">    -0.00599198 -0.39363529  0.25896495 -0.2026098  -0.21658332  0.09938895</span></span><br><span class="line"><span class="string">    -0.38005182]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 预测测试集的房子价格</span></span><br><span class="line">    y_lr_predict = std_y.inverse_transform(lr.predict(x_test))  <span class="comment"># 此时的值还是标准化后的,所以要inverse</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"测试集里每个房子的预测价格："</span>, y_lr_predict)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    测试集里每个房子的预测价格： [[-5.23859608]</span></span><br><span class="line"><span class="string">    [39.61950876]</span></span><br><span class="line"><span class="string">    [35.86414606]</span></span><br><span class="line"><span class="string">    ......</span></span><br><span class="line"><span class="string">    [20.31808398]</span></span><br><span class="line"><span class="string">    [17.36664138]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"正规方程的均方差："</span>, mean_squared_error(std_y.inverse_transform(y_test), y_lr_predict))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    正规方程的均方差： 24.203056713653485</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 梯度下降进行房价预测</span></span><br><span class="line">    sgd = SGDRegressor()  <span class="comment"># 这里可以指定学习率</span></span><br><span class="line">    sgd.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"SGD预测回归系数为："</span>, sgd.coef_)</span><br><span class="line">    <span class="string">''''</span></span><br><span class="line"><span class="string">    SGD预测回归系数为： [-0.08189344  0.07032661 -0.03413881  0.10527339 -0.09947185  0.33781714</span></span><br><span class="line"><span class="string">    -0.04008106 -0.21279579  0.06362954 -0.05064323 -0.19061834  0.10570017</span></span><br><span class="line"><span class="string">    -0.36071628]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 预测测试集的房子价格</span></span><br><span class="line">    y_sgd_predict = std_y.inverse_transform(sgd.predict(x_test))  <span class="comment"># 此时的值还是标准化后的,所以要inverse</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"测试集里每个房子的预测价格："</span>, y_sgd_predict)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    测试集里每个房子的预测价格：[22.25491159 30.38773853 12.72306638 35.68389966 29.69842243 21.70338892</span></span><br><span class="line"><span class="string">    15.23925266 27.85422604 24.72436984 31.05806439 16.91816538 28.40787834</span></span><br><span class="line"><span class="string">    ......</span></span><br><span class="line"><span class="string">    21.3051416  23.92421059 20.83558394 23.80267794 28.17431605 32.1886237</span></span><br><span class="line"><span class="string">    25.89847536]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"梯度下降的均方差："</span>, mean_squared_error(std_y.inverse_transform(y_test), y_sgd_predict))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    梯度下降的均方差： 24.303529166136546</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    mylinear()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="岭回归解决过拟合"><a href="#岭回归解决过拟合" class="headerlink" title="岭回归解决过拟合"></a>岭回归解决过拟合</h2><p>线性回归LinearRegression 容易出现过拟合，为了把训练集数据表现更好<br>而岭回归是带有正则化的线性回归</p>
<p>线性回归与岭回归Ridge对比：<br>岭回归得到的<strong>回归系数更符合实际，更可靠</strong>。另外，能让估计参数的波动范围变小，变的稳定。在存在病态数据偏多的研究中有较大的实用价值。</p>
<blockquote>
<p>具有L2正则化的线性最小二乘法：sklearn.linear_model.Ridge(alpha=1.0)</p>
</blockquote>
<ul>
<li>alpha：正则化力度</li>
<li>coef_:求解的回归系数w</li>
</ul>
<p>上面的案例用岭回归解决：</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 岭回归进行房价预测</span><br><span class="line">rd &#x3D; Ridge(alpha&#x3D;1.0)  # 这里可以指定学习率</span><br><span class="line">rd.fit(x_train, y_train)</span><br><span class="line">print(&quot;岭回归预测回归系数为：&quot;, rd.coef_)</span><br><span class="line">&#39;&#39;&#39;&#39;</span><br><span class="line">岭回归预测回归系数为： [[-0.07357443  0.11581584  0.0248411   0.06555144 -0.17895055  0.27716807</span><br><span class="line">-0.01529063 -0.32589572  0.24111193 -0.19075882 -0.21855738  0.09673899</span><br><span class="line">-0.44673051]]</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"># 预测测试集的房子价格</span><br><span class="line">y_rd_predict &#x3D; std_y.inverse_transform(rd.predict(x_test))  # 此时的值还是标准化后的,所以要inverse</span><br><span class="line"></span><br><span class="line">print(&quot;测试集里每个房子的预测价格：&quot;, y_rd_predict)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">测试集里每个房子的预测价格： [[33.05501377]</span><br><span class="line">[29.49658894]</span><br><span class="line">......</span><br><span class="line">[19.62373227]</span><br><span class="line">[12.41679022]]</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">print(&quot;岭回归的均方差：&quot;, mean_squared_error(std_y.inverse_transform(y_test), y_sgd_predict))</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">岭回归的均方差： 27.097530207872403</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>逻辑回归的输入就是线性回归的式子，而且也是自我学习的过程，要迭代回归<br>逻辑回归可以做什么，适应场景：二分类问题</p>
<ul>
<li>广告是否会被点击</li>
<li>是否为垃圾邮件</li>
<li>是否患病</li>
<li>金融诈骗</li>
</ul>
<p>逻辑回归的输入：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%BE%93%E5%85%A5.jpg" class="" title="逻辑回归输入">

<h4 id="过程：如何从线性回归的输入变为逻辑回归的分类"><a href="#过程：如何从线性回归的输入变为逻辑回归的分类" class="headerlink" title="过程：如何从线性回归的输入变为逻辑回归的分类"></a>过程：如何从线性回归的输入变为逻辑回归的分类</h4><p>先了解一下什么事sigmoid函数：将输入转换为0~1，与y轴交叉点默认为0.5</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/sigmoid%E5%87%BD%E6%95%B0.jpg" class="" title="sigmoid函数">
<ul>
<li>横坐标是一个具体的值</li>
<li>输出纵坐标是0~1之间的值，可看做概率值</li>
</ul>
<p>而sigmoid的公式和逻辑回归的公式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%85%AC%E5%BC%8F.jpg" class="" title="逻辑回归公式">
<ul>
<li>e:常数2.71</li>
<li>Z：回归的结果</li>
</ul>
<h4 id="逻辑回归的损失函数：log对数似然损失函数"><a href="#逻辑回归的损失函数：log对数似然损失函数" class="headerlink" title="逻辑回归的损失函数：log对数似然损失函数"></a>逻辑回归的损失函数：log对数似然损失函数</h4><p>有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即max F(y, f(x)) —&gt; min -F(y, f(x)))。从损失函数的视角来看，它就成了log损失函数了。</p>
<p>损失函数公式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%AF%B9%E6%95%B0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.jpg" class="" title="对数损失函数">

<p>当目标值是1，此时损失值与预测结果为1的概率（x轴）的关系如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%9B%AE%E6%A0%87%E5%80%BC%E4%B8%BA1.jpg" class="" title="目标值为1">

<p>当目标值是0，此时损失值与预测结果为0的概率（x轴）的关系如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%9B%AE%E6%A0%87%E5%80%BC%E6%98%AF0.jpg" class="" title="目标值是0">


<h4 id="API与案例："><a href="#API与案例：" class="headerlink" title="API与案例："></a>API与案例：</h4><blockquote>
<p>逻辑回归API：sklearn.linear_model.LogisticRegression(penalty=’l2’,C=1.0)</p>
</blockquote>
<ul>
<li>penalty:正则化方式</li>
<li>c:正则化粒度</li>
</ul>
<p>案例：预测癌症肿瘤（乳腺癌）<br>数据地址：<a href="http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data" target="_blank" rel="noopener">http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data</a><br>数据内容：</p>
<ul>
<li>每个案例11列数据，第一列是检索Id，后9列是与病情相关的医学特征，最后1列表示肿瘤类型的数值</li>
<li>肿瘤类型数值：2为良性，占65.5%，4为恶性，占34.5%<ul>
<li>注：逻辑回归中，因为针对的是2分类型，所以只有两个结果，只用计算一个概率值来表示属于某一种类型的概率，另一个为(1-概率),而这个概率值一般是数据中占比比较少的那一个类型，比如这里的恶性</li>
</ul>
</li>
<li>其中包含几个缺失值，用“？”标出</li>
</ul>
<p>流程：</p>
<ul>
<li>从网上获取数据</li>
<li>数据缺失值处理、标准化</li>
<li>估计器流程</li>
</ul>
<p>代码案例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    逻辑回归做二分类进行癌症肿瘤是否是恶性的预测（根据属性特征）</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 构造列标签名字,注：如果数据中没有标签，则pandas会默认把第一行数据作为特征名，这里数据是没有标签名的，所以这里手动添加</span></span><br><span class="line">    column = [<span class="string">'Sample code number'</span>,<span class="string">'Clump Thickness'</span>,<span class="string">'Uniformity of Cell Size'</span>,<span class="string">'Uniformity of Cell Shape'</span>,<span class="string">'Marginal Adhesion'</span>,<span class="string">'Single Epithelial Cell Size'</span></span><br><span class="line">              ,<span class="string">'Bare Nuclei'</span>,<span class="string">'Bland Chromatin'</span>,<span class="string">'Normal Nucleoli'</span>,<span class="string">'Mitoses'</span>,<span class="string">'Class'</span>]</span><br><span class="line">    data = pd.read_csv(<span class="string">"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"</span></span><br><span class="line">                , names=column)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#缺失值进行处理</span></span><br><span class="line">    data = data.replace(to_replace=<span class="string">'?'</span>,value=np.nan)</span><br><span class="line">    data = data.dropna()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据分割</span></span><br><span class="line">    x_train,x_test,y_train,y_test = train_test_split(data[column[<span class="number">1</span>:<span class="number">10</span>]],data[column[<span class="number">10</span>]],test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#进行标准化</span></span><br><span class="line">    std = StandardScaler()</span><br><span class="line">    x_train = std.fit_transform(x_train)</span><br><span class="line">    x_test = std.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#逻辑回归预测</span></span><br><span class="line">    lg = LogisticRegression(penalty=<span class="string">'l2'</span>,C=<span class="number">1.0</span>)</span><br><span class="line">    lg.fit(x_train,y_train)</span><br><span class="line">    y_predict = lg.predict(x_test)</span><br><span class="line">    print(<span class="string">"计算的回归系数为："</span>,lg.coef_)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算的回归系数为： [[1.07139763 0.920439   1.03352718 0.88427896 0.07783286 1.15006497</span></span><br><span class="line"><span class="string">    0.94210002 0.59530658 0.81971895]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"准确率为："</span>,lg.score(x_test,y_test))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    准确率为： 0.9590643274853801</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"召回率："</span>,classification_report(y_test,y_predict,labels=[<span class="number">2</span>,<span class="number">4</span>],target_names=[<span class="string">"良性"</span>,<span class="string">"恶性"</span>]))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    召回率：       precision    recall  f1-score   support</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">          良性       0.96      0.97      0.97       110</span></span><br><span class="line"><span class="string">          恶性       0.95      0.93      0.94        61</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     micro avg       0.96      0.96      0.96       171</span></span><br><span class="line"><span class="string">     macro avg       0.96      0.95      0.96       171</span></span><br><span class="line"><span class="string">  weighted avg       0.96      0.96      0.96       171</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    logistic()</span><br></pre></td></tr></table></figure>


<h1 id="非监督学习：K-均值聚类（k-means）算法"><a href="#非监督学习：K-均值聚类（k-means）算法" class="headerlink" title="非监督学习：K-均值聚类（k-means）算法"></a>非监督学习：K-均值聚类（k-means）算法</h1><blockquote>
<p>K-均值聚类属于无监督学习。那么监督学习和无监督学习的区别在哪儿呢？监督学习知道从对象（数据）中学习什么，而无监督学习无需知道所要搜寻的目标，它是根据算法得到数据的共同特征。比如用分类和聚类来说，分类事先就知道所要得到的类别，而聚类则不一样，只是以相似度为基础，将对象分得不同的簇。</p>
</blockquote>
<p>K-Means算法有大量的变体，包括初始化优化K-Means++, 距离计算优化elkan K-Means算法和大数据情况下的优化Mini Batch K-Means算法。</p>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><p>K-means是一个反复迭代的过程，算法分为四个步骤：</p>
<ol>
<li>选取数据空间中的K个对象作为初始中心，每个对象代表一个聚类中心；<ul>
<li>我们会根据对数据的先验经验选择一个合适的k值，如果没有什么先验知识，则可以通过交叉验证选择一个合适的k值。</li>
</ul>
</li>
<li>对于样本中的数据对象，根据它们与这些聚类中心的欧氏距离，按距离最近的准则将它们分到距离它们最近的聚类中心（最相似）所对应的类；</li>
<li>更新聚类中心：将每个类别中所有对象所对应的均值作为该类别的聚类中心，计算目标函数的值；</li>
<li>判断聚类中心和目标函数的值是否发生改变，若不变，则输出结果，若改变，则返回2）</li>
</ol>
<p>这里发现了一个python2.7版本的简易实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span>      <span class="comment">#general function to parse tab -delimited floats</span></span><br><span class="line">    dataMat = []                <span class="comment">#assume last column is target value</span></span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        fltLine = map(float,curLine) <span class="comment">#map all elements to float()</span></span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br><span class="line">\<span class="comment">#计算欧氏距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum(power(vecA - vecB, <span class="number">2</span>))) <span class="comment">#la.norm(vecA-vecB)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">    n = shape(dataSet)[<span class="number">1</span>]</span><br><span class="line">    centroids = mat(zeros((k,n)))<span class="comment">#create centroid mat</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):<span class="comment">#create random cluster centers, within bounds of each dimension</span></span><br><span class="line">        minJ = min(dataSet[:,j]) </span><br><span class="line">        rangeJ = float(max(dataSet[:,j]) - minJ)</span><br><span class="line">        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line">\<span class="comment">#dataSet样本点,k 簇的个数</span></span><br><span class="line">\<span class="comment">#disMeas距离量度，默认为欧几里得距离</span></span><br><span class="line">\<span class="comment">#createCent,初始点的选取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>] <span class="comment">#样本数</span></span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="number">2</span>))) <span class="comment">#m*2的矩阵                   </span></span><br><span class="line">    centroids = createCent(dataSet, k) <span class="comment">#初始化k个中心</span></span><br><span class="line">    clusterChanged = <span class="literal">True</span>             </span><br><span class="line">    <span class="keyword">while</span> clusterChanged:      <span class="comment">#当聚类不再变化</span></span><br><span class="line">        clusterChanged = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            minDist = inf; minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k): <span class="comment">#找到最近的质心</span></span><br><span class="line">                distJI = distMeas(centroids[j,:],dataSet[i,:])</span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:</span><br><span class="line">                    minDist = distJI; minIndex = j</span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i,<span class="number">0</span>] != minIndex: clusterChanged = <span class="literal">True</span></span><br><span class="line">            <span class="comment"># 第1列为所属质心，第2列为距离</span></span><br><span class="line">            clusterAssment[i,:] = minIndex,minDist**<span class="number">2</span></span><br><span class="line">        <span class="keyword">print</span> centroids</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更改质心位置</span></span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):</span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[:,<span class="number">0</span>].A==cent)[<span class="number">0</span>]]</span><br><span class="line">            centroids[cent,:] = mean(ptsInClust, axis=<span class="number">0</span>) </span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>伪代码如下：<br>function K-Means(输入数据，中心点个数K)<br>    获取输入数据的维度Dim和个数N<br>    随机生成K个Dim维的点<br>    while(算法未收敛)<br>        对N个点：计算每个点属于哪一类。<br>        对于K个中心点：<br>            1，找出所有属于自己这一类的所有数据点<br>            2，把自己的坐标修改为这些数据点的中心点坐标<br>    end<br>    输出结果：<br>end</p>
<blockquote>
<p>sklearn.cluster.KMeans(n_clusters=8,init=’k-means++’) </p>
</blockquote>
<ul>
<li>n_clusters:开始的聚类中心数量</li>
<li>init:初始化方法，默认为’k-means++’</li>
<li>labels_:默认标记的类型，可以和真实值比较（不是值比较）</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/" data-id="ckccph4ih0007y81rb1hr9e8x" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/01/12/python%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          python与深度学习入门一
        
      </div>
    </a>
  
  
    <a href="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">python与机器学习入门一</div>
    </a>
  
</nav>

  
</article>



</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <h1 class="blog-title">Pokemonlei的博客</h1>
    <h2 class="blog-subtitle"></h2>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://avatars0.githubusercontent.com/u/20333903?v=3&amp;s=460">
    <h2 class="author">pokemonlei</h2>
    <h3 class="description">陈磊的博客 | pokemonlei</h3>
    <div class="count-box">
      <a href="/archives"><div><strong>11</strong><br>文章</div></a>
      <a href="/categories"><div><strong>6</strong><br>分类</div></a>
      <a href="/tags"><div><strong>11</strong><br>标签</div></a>
    </div>



    <div class="social-link">
      
        <a class="hvr-bounce-in" href="https://user.qzone.qq.com/1176014533/infocenter" target="_blank" title="QQZone">
          QQZone
        </a>
      
    </div>

    <div class="friend-link">
      <h2>友情链接</h2>
      
        <a class="hvr-bounce-in" href="http://blog.shanamaid.top/" target="_blank" title="ShanaMaid">
          ShanaMaid
        </a>
      
    </div>
  </div>
</div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy;2019 - 2020 pokemonlei<br>
      由<a href="http://hexo.io/" target="_blank">Hexo</a>强力驱动 | 
      主题-<a href="https://github.com/ShanaMaid/hexo-theme-shana" target="_blank" rel="noopener">Shana</a>
      
    </div>
    
  </div>
</footer>
    </div>
    

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="//apps.bdimg.com/libs/wow/0.1.6/wow.min.js"></script>
<script>
new WOW().init();
</script>   


  
<link rel="stylesheet" href="/plugin/fancybox/jquery.fancybox.css">

  
<script src="/plugin/fancybox/jquery.fancybox.pack.js"></script>




  
<link rel="stylesheet" href="/plugin/galmenu/GalMenu.css">

  
<script src="/plugin/galmenu/GalMenu.js"></script>

  <div class="GalMenu GalDropDown">
      <div class="circle" id="gal">
        <div class="ring">
          
            <a href="/" title="" class="menuItem">首页</a>
          
            <a href="/tags" title="" class="menuItem">标签</a>
          
            <a href="/categories" title="" class="menuItem">分类</a>
          
            <a href="/archives" title="" class="menuItem">归档</a>
          
            <a href="/xxxxxxxxx" title="" class="menuItem">占位1</a>
          
            <a href="/xxxxxxx" title="" class="menuItem">占位2</a>
          
        </div>
        
          <audio id="audio" src="#"></audio>
        
      </div> 
</div>
<div id="overlay" style="opacity: 1; cursor: pointer;"></div>
  <script type="text/javascript">var items = document.querySelectorAll('.menuItem');
    for (var i = 0,
    l = items.length; i < l; i++) {
      items[i].style.left = (50 - 35 * Math.cos( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%";
      items[i].style.top = (50 + 35 * Math.sin( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%"
    }</script>
<script type="text/javascript">
  $(document).ready(function() {
    $('body').GalMenu({
      'menu': 'GalDropDown'
    })
  });
</script>

  <section class="hidden-xs"> 
  <ul class="cb-slideshow"> 
    <li><span>苟利</span></li> 
    <li><span>国家</span></li> 
    <li><span>生死以</span></li> 
    <li><span>岂能</span></li> 
    <li><span>祸福</span></li> 
    <li><span>趋避之</span></li> 
  </ul>
</section>

<script src="/js/script.js"></script>




  </div>
</body>
</html>
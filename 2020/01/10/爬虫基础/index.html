<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>爬虫基础 | Pokemonlei的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="前面http和https的介绍，可直接跳过看下面的案例  HTTP与HTTPS为什么要先简要复习一下http和https？因为要发送请求，模拟浏览器，获取和浏览器一模一样的响应。 HTTP：超文本传输协议，默认端口：80，是一种建立在TCP上的无状态连接，说白了就是个协议，规定了一些列的规则，整个基本的工作流程是客户端发送一个HTTP请求，说明客户端想要访问的资源和请求的动作，服务端收到请求之后">
<meta property="og:type" content="article">
<meta property="og:title" content="爬虫基础">
<meta property="og:url" content="http://yoursite.com/2020/01/10/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="Pokemonlei的博客">
<meta property="og:description" content="前面http和https的介绍，可直接跳过看下面的案例  HTTP与HTTPS为什么要先简要复习一下http和https？因为要发送请求，模拟浏览器，获取和浏览器一模一样的响应。 HTTP：超文本传输协议，默认端口：80，是一种建立在TCP上的无状态连接，说白了就是个协议，规定了一些列的规则，整个基本的工作流程是客户端发送一个HTTP请求，说明客户端想要访问的资源和请求的动作，服务端收到请求之后">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2020/01/10/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/python_spider_01.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/10/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/python_spider_02.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/10/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/qiubai_01.jpg">
<meta property="og:image" content="http://yoursite.com/2020/01/10/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/qiubai_02.jpg">
<meta property="article:published_time" content="2020-01-10T15:05:33.000Z">
<meta property="article:modified_time" content="2020-01-11T05:25:36.382Z">
<meta property="article:author" content="pokemonlei">
<meta property="article:tag" content="爬虫">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/01/10/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/python_spider_01.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Pokemonlei的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/plugin/bganimation/bg.css">

  

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <div class="widget-wrap mobile-header">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://avatars0.githubusercontent.com/u/13980642?s=460&amp;v=4">
    <h2 class="author">pokemonlei</h2>
    <h3 class="description">陈磊的博客 | pokemonlei</h3>
    <div class="count-box">
      <a href="/archives"><div><strong>8</strong><br>文章</div></a>
      <a href="/categories"><div><strong>4</strong><br>分类</div></a>
      <a href="/tags"><div><strong>8</strong><br>标签</div></a>
    </div>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

        <section id="main"><article id="post-爬虫基础" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/10/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/" class="article-date">
  <time class="post-time" datetime="2020-01-10T15:05:33.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">10</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      爬虫基础
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>前面http和https的介绍，可直接跳过看下面的案例</p>
</blockquote>
<h2 id="HTTP与HTTPS"><a href="#HTTP与HTTPS" class="headerlink" title="HTTP与HTTPS"></a>HTTP与HTTPS</h2><p>为什么要先简要复习一下http和https？因为要发送请求，模拟浏览器，获取和浏览器一模一样的响应。</p>
<pre><code>HTTP：超文本传输协议，默认端口：80，是一种建立在TCP上的无状态连接，说白了就是个协议，规定了一些列的规则，整个基本的工作流程是客户端发送一个HTTP请求，说明客户端想要访问的资源和</code></pre><p>请求的动作，服务端收到请求之后，服务端开始处理请求，并根据请求做出相应的动作访问服务器资源,最后通过发送HTTP响应把结果返回给客户端。</p>
<pre><code>HTTPS：即HTTP+SSL（安全套接字层），默认端口号：443，即在HTTP上多了个安全套接字层SSL，SSL是一个提供数据安全和完整性的协议，负责网络连接的加密。    </code></pre><blockquote>
<p>   题外话：HTTPS通信中的几个概念：加密分为对称和非对称<br>   对称加密：信息的加密和解密都是通过同一个密钥进行的，实际通信中，一个服务器可以同时对应好几个客户端，也就是A客户端获得的加解密算法同样可以加解密B客户端<br>的消息，这跟没加密一样，为了防止这种情况，就只能不同的客户端使用不同密钥，这样的话密钥将会有很多，并且在通信刚开始，服务器和A客户端就要协商好用什么密钥，而这个协商过程是不能加密<br>的，不然A客户端就读不懂服务器消息了，因此还是存在风险。</p>
<p>   非对称加密：应用最广的加密机制“非对称加密”，特点是私钥加密后的密文，只要是公钥，都可以解密，但是反过来公钥加密后的密文，只有私钥可以解密。私钥只有一个<br>人有，而公钥可以发给所有的人。所以公钥不需要加密，而私钥只存在于服务器。这样只需要一套公钥和私钥就可以了。那么现在的问题是，如何让客户端安全的获取公钥？如果服务器直接明文发送给客户<br>端，那么可能发生被劫持的情况，例如客户端A和服务器S通信，S给A的公钥被B劫持后修改了，那么之后A将会用假的公钥进行加密，将消息发给服务器时B继续劫持，查看内容或者修改之后用真公钥加密然<br>后发给服务器，这就是中间人攻击。这时候风险依然存在，而为了解决这个问题，采用了一种SSL 证书（需要购买）和CA机构的方法，涉及防伪和证书链的概念，大概流程是：<br>   在客户端第一次请求服务器时，服务器发送回一个SSL证书给客户端，SSL 证书中包含的具体内容有<strong>证书的颁发机构的证书</strong>、有效期、<strong>公钥</strong>、证书持有者、<strong>签名</strong>。防伪的步骤：浏览器<br>拿到这个证书后读取证书中的证书所有者、有效期等信息进行一一校验，开始查找操作系统中已内置的受信任的证书发布机构CA，与服务器发来的证书中的颁发者CA比对，用于校验证书是否为合法机构颁<br>发，如果没找到，则报错，如果找到了，浏览器会从操作系统中取出颁发者CA的公钥，然后对服务器发来的证书里面的<strong>签名</strong>进行解密，如果能够解密则一定是证书颁发机构颁发的证书如果解密后信息<br>与Server信息一致则确实是颁发给该Server的，综上校验通过，这就是防伪的流程，而证书链的作用可以保证正在通信的Server确实是证书颁发机构指定的Server，流程是：通过和Server证书验证同<br>的过程通过内嵌在浏览器或者JDK中的根证书验证下中级证书的合法性就好了。因为根证书是内嵌的具有绝对的合法性，如果根证书信任该中级机构，则该中级证书颁发的证书也是可信的这就是证书链了。<br>这样通过第三方的校验保证了身份的合法，解决了公钥获取的安全性。什么你问我怎么申请CA证书？国内的阿里云和腾讯云上找去</p>
</blockquote>
<h2 id="举个栗子：爬取某贴吧前1000页内容"><a href="#举个栗子：爬取某贴吧前1000页内容" class="headerlink" title="举个栗子：爬取某贴吧前1000页内容"></a>举个栗子：爬取某贴吧前1000页内容</h2><p>先来一个简单的例子熟悉一下api和爬虫基本流程</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">class TiebaSpider:</span><br><span class="line">    def __init__(self, tieba_name):</span><br><span class="line">        self.tieba_name &#x3D; tieba_name</span><br><span class="line">        self.url_temp &#x3D; &quot;https:&#x2F;&#x2F;tieba.baidu.com&#x2F;f?kw&#x3D;&quot; + tieba_name + &quot;&amp;ie&#x3D;utf-8&amp;pn&#x3D;&#123;&#125;&quot;</span><br><span class="line">        self.headers&#x3D;&#123;&quot;User-Agent&quot;:&quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;68.0.3440.75 Safari&#x2F;537.36&quot;&#125;</span><br><span class="line"></span><br><span class="line">    def get_url_list(self):  #构造url列表</span><br><span class="line">        # url_list &#x3D; []</span><br><span class="line">        # for i in range(1000):</span><br><span class="line">        #     url_list.append(self.url_temp.format(i*50))</span><br><span class="line">        # return url_list</span><br><span class="line">        return [self.url_temp.format(i*50) for i in range(1000)]</span><br><span class="line"></span><br><span class="line">    def parse_url(self, url):  #发送请求获取相应</span><br><span class="line">        print(url)</span><br><span class="line">        response &#x3D; requests.get(url, headers&#x3D;self.headers)</span><br><span class="line">        return response.content.decode()</span><br><span class="line"></span><br><span class="line">    def save_html(self, html_str, page_num):</span><br><span class="line">        file_path &#x3D; &quot;&#123;&#125;第&#123;&#125;页.html&quot;.format(self.tieba_name, page_num)</span><br><span class="line">        with open(file_path, &quot;w&quot;,encoding&#x3D;&quot;utf-8&quot;) as f:</span><br><span class="line">            f.write(html_str)</span><br><span class="line"></span><br><span class="line">    def run(self):</span><br><span class="line">        # 构造url列表</span><br><span class="line">        url_list &#x3D; self.get_url_list()</span><br><span class="line">        # 遍历，发送请求，获取响应</span><br><span class="line">        for url in url_list:</span><br><span class="line">            html_str &#x3D; self.parse_url(url)</span><br><span class="line">            page_num &#x3D; url_list.index(url)+1</span><br><span class="line">            self.save_html(html_str, page_num)</span><br><span class="line">        # 保存</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    tieba_spider &#x3D; TiebaSpider(&quot;李毅&quot;)</span><br><span class="line">    tieba_spider.run()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这里爬取下来的内容是没问题的，但保存下来的html直接打开是有问题的，因为万恶的百度在其中把大量有用内容添加了注释，后续是通过js来去掉注释的。这里暂时不处理，只展示基本api的使用。</p>
<h4 id="发送POST请求"><a href="#发送POST请求" class="headerlink" title="发送POST请求"></a>发送POST请求</h4><p>什么时候需要发送post请求？</p>
<ul>
<li>登录注册等有敏感信息的时候，POST比GET更安全</li>
<li>需要传输大文本的时候（POST请求对数据长度没有要求）<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response &#x3D; requests.post(&quot;&quot;,data&#x3D;data,headers&#x3D;headers)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="使用代理"><a href="#使用代理" class="headerlink" title="使用代理"></a>使用代理</h4><p>为什么爬虫需要代理？</p>
<ul>
<li>让服务器以为不是同一个客户端在不断请求</li>
<li>防止我们真实地址倍泄露，防止被追究</li>
</ul>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response &#x3D; requests.get(&quot;url&quot;,proxies&#x3D;proxies)</span><br></pre></td></tr></table></figure>
<p>proxies是一个字典，</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">proxies&#x3D;&#123;</span><br><span class="line">    &quot;http&quot;:&quot;http:&#x2F;&#x2F;xx.xx.xx.xx:xxx&quot;,</span><br><span class="line">    &quot;https&quot;:&quot;https:&#x2F;&#x2F;xx.xx.xx.xx:xxxx&quot;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="request模拟登陆"><a href="#request模拟登陆" class="headerlink" title="request模拟登陆"></a>request模拟登陆</h4><p>cookie和session区别：</p>
<ul>
<li>cookie数据存放在客户的浏览器上，session数据存放在服务器上</li>
<li>cookie不安全，别人可以分析存放在本地的cookie并进行cookie欺骗</li>
<li>session会在一定时间内保存在服务器上，当访问增多，会比较占用服务器性能</li>
<li>单个cookie保存的数据不能超过4k，很多浏览器会限制一个站点最多保存20个cookie</li>
</ul>
<p>1）request提供了一个session类，来实现客户端和服务器的会话保持,先实例化一个session，用session发送post请求登陆网站，把cookie保存在session中，再使用session请求登陆之后才能访问的网站，session能够自动的携带登陆成功时保存在其中的cookie。<br>使用方法：</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">session &#x3D; requests.session()</span><br><span class="line">response &#x3D; session.get(url,headers)</span><br></pre></td></tr></table></figure>

<p>2）如果请求页面时不发送post请求的情况下，可以在headers中添加cookie键值对，要注意cookie的有效时间</p>
<p>3）也可以把cookie作为request的参数。此时cookie参数时一个字典。</p>
<h4 id="request小技巧"><a href="#request小技巧" class="headerlink" title="request小技巧"></a>request小技巧</h4><ul>
<li>request.utils.dict_from_cookiejar(response.cookies)  把cookie转换为字典，request.utils.cookiejar_from_dict ,将字典转化为cookie</li>
<li>request.utils.unquote(“编码后的url”)  将url地址解码，反之quote为编码</li>
<li>忽视证书错误   request.get(“url”,verify=false)</li>
<li>设置超时    request.get(“url”,timeout)</li>
<li>刷新网页 使用第三方模块retrying，还可以定义最大重新请求次数</li>
</ul>
<h2 id="爬虫的基本套路"><a href="#爬虫的基本套路" class="headerlink" title="爬虫的基本套路"></a>爬虫的基本套路</h2><ul>
<li><p>准备url</p>
<ul>
<li>准备start_url<ul>
<li>url地址规律不明显，总数不确定的情况下</li>
<li>通过代码提取下一页的url地址<ul>
<li>当下一页的地址在网页的响应中时，可以通过xpath</li>
<li>通过其他方式寻找url地址，比如通过js生成，这种情况下部分参数是在当前响应中</li>
</ul>
</li>
</ul>
</li>
<li>准备url_list<ul>
<li>页码总数明显的时候</li>
<li>url地址规律很明显</li>
</ul>
</li>
</ul>
</li>
<li><p>发送请求，获取响应</p>
<ul>
<li>添加随机的User-Agent，防反爬虫</li>
<li>添加随机的代理ip，防反爬虫</li>
<li>在对方判断出是爬虫之后，应该添加更多的header字段，包括cookies</li>
<li>cookie可以用session来解决</li>
<li>如果不登录的话<ul>
<li>准备能成功请求对方网站的cookie，即接收对方网站设置在response的cookie</li>
<li>下次请求的时候，使用之前的列表中的cookie来请求</li>
</ul>
</li>
<li>如果登录的话<ul>
<li>准备多个账号</li>
<li>获取多个账号cookie</li>
<li>之后随机选择cookie来请求登录之后才能访问的网站</li>
</ul>
</li>
</ul>
</li>
<li><p>提取数据</p>
<ul>
<li>确定数据位置，确定数据是否在当前的url地址响应中<ul>
<li>如果在当前url地址响应中<ul>
<li>提取的是列表页的数据<ul>
<li>直接请求列表页的url地址，不需要进入详情页</li>
</ul>
</li>
<li>提取的是详情页的数据<ul>
<li>1.确定详情页url地址</li>
<li>2.发送请求</li>
<li>3.提取数据</li>
<li>4.返回</li>
</ul>
</li>
</ul>
</li>
<li>如果不在当前url响应中<ul>
<li>在其他响应中，寻找数据位置</li>
</ul>
</li>
</ul>
</li>
<li>数据的提取<ul>
<li>xpath，从html中提取整块的数据，先分组，然后针对每一组进行提取</li>
<li>json</li>
<li>re，提取html中的json字符串或者某些容易用正则区分出的属性等</li>
</ul>
</li>
</ul>
</li>
<li><p>保存数据</p>
</li>
</ul>
<h2 id="一个小案例，爬去豆瓣上最近热播的英美剧，国产剧，动漫以及综艺的节目名称和评分"><a href="#一个小案例，爬去豆瓣上最近热播的英美剧，国产剧，动漫以及综艺的节目名称和评分" class="headerlink" title="一个小案例，爬去豆瓣上最近热播的英美剧，国产剧，动漫以及综艺的节目名称和评分"></a>一个小案例，爬去豆瓣上最近热播的英美剧，国产剧，动漫以及综艺的节目名称和评分</h2><p>网页截图以及爬去后的效果图如下：</p>
<img src="/2020/01/10/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/python_spider_01.jpg" class="" title="网页截图">
<img src="/2020/01/10/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/python_spider_02.jpg" class="" title="爬取后文档截图">

<p>代码如下：</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"># coding&#x3D;utf-8</span><br><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">class DoubanSpider:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.headers &#x3D; &#123;</span><br><span class="line">            &quot;User-Agent&quot;: &quot;Mozilla&#x2F;5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit&#x2F;604.1.38 (KHTML, like Gecko) Version&#x2F;11.0 Mobile&#x2F;15A372 Safari&#x2F;604.1&quot;,</span><br><span class="line">            &quot;Referer&quot;: &quot;https:&#x2F;&#x2F;m.douban.com&#x2F;tv&#x2F;american&quot;&#125;</span><br><span class="line">        self.url_temp_list &#x3D; [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;url_temp&quot;: &quot;https:&#x2F;&#x2F;m.douban.com&#x2F;rexxar&#x2F;api&#x2F;v2&#x2F;subject_collection&#x2F;tv_american&#x2F;items?start&#123;&#125;&amp;count&#x3D;18&amp;loc_id&#x3D;108288&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;英美剧&quot;,</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;url_temp&quot;: &quot;https:&#x2F;&#x2F;m.douban.com&#x2F;rexxar&#x2F;api&#x2F;v2&#x2F;subject_collection&#x2F;tv_domestic&#x2F;items?start&#123;&#125;&amp;count&#x3D;18&amp;loc_id&#x3D;108288&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;国产剧&quot;,</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;url_temp&quot;: &quot;https:&#x2F;&#x2F;m.douban.com&#x2F;rexxar&#x2F;api&#x2F;v2&#x2F;subject_collection&#x2F;tv_animation&#x2F;items?start&#123;&#125;&amp;count&#x3D;18&amp;loc_id&#x3D;108288&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;动漫&quot;,</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;url_temp&quot;: &quot;https:&#x2F;&#x2F;m.douban.com&#x2F;rexxar&#x2F;api&#x2F;v2&#x2F;subject_collection&#x2F;tv_variety_show&#x2F;items?start&#123;&#125;&amp;count&#x3D;18&amp;loc_id&#x3D;108288&quot;,</span><br><span class="line">                &quot;type&quot;: &quot;综艺&quot;,</span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">        self.curIndex&#x3D;1</span><br><span class="line"></span><br><span class="line">    def parse_url(self, url):</span><br><span class="line">        print(url)</span><br><span class="line">        response &#x3D; requests.get(url, headers&#x3D;self.headers)</span><br><span class="line">        return response.content.decode()</span><br><span class="line"></span><br><span class="line">    def get_content_list(self, json_str):</span><br><span class="line">        dict_ret &#x3D; json.loads(json_str)</span><br><span class="line">        content_list &#x3D; dict_ret[&quot;subject_collection_items&quot;]</span><br><span class="line">        total &#x3D; dict_ret[&quot;total&quot;]</span><br><span class="line">        return content_list, total</span><br><span class="line"></span><br><span class="line">    def save_content_list(self, content_list):</span><br><span class="line">        with open(&quot;douban.txt&quot;, &quot;a&quot;, encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">            for content in content_list:</span><br><span class="line">                #f.write(json.dumps(content, ensure_ascii&#x3D;False))</span><br><span class="line">                f.write(self.curIndex.__str__()+&quot; :&quot; + content[&quot;title&quot;]+&quot; 评分：&quot;+content[&quot;rating&quot;][&quot;value&quot;].__str__())</span><br><span class="line">                f.write(&quot;\n&quot;)</span><br><span class="line">                self.curIndex +&#x3D; 1</span><br><span class="line"></span><br><span class="line">    def run(self):  # 主逻辑</span><br><span class="line">        for url_temp in self.url_temp_list:</span><br><span class="line">            with open(&quot;douban.txt&quot;, &quot;a&quot;, encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">                f.write(url_temp[&quot;type&quot;] + &quot;:&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&quot;)  # 分割线，方便数据查看</span><br><span class="line">                f.write(&quot;\n&quot;)</span><br><span class="line">            self.curIndex &#x3D; 1</span><br><span class="line">            num &#x3D; 0</span><br><span class="line">            total &#x3D; 1  # 假设刚开始的条件成立</span><br><span class="line">            while num &lt; total + 18:</span><br><span class="line">                # 1.构造一个start_rul</span><br><span class="line">                url &#x3D; url_temp[&quot;url_temp&quot;].format(num)</span><br><span class="line">                # 2.发送请求，获取响应</span><br><span class="line">                json_str &#x3D; self.parse_url(url)</span><br><span class="line">                # 3.提取数据，保存</span><br><span class="line">                content_list, total &#x3D; self.get_content_list(json_str)</span><br><span class="line">                self.save_content_list(content_list)</span><br><span class="line">                # if len(content_list &lt; 18):</span><br><span class="line">                #    break</span><br><span class="line">                # 4，构造下一页的url地址，进入2，3循环</span><br><span class="line">                num +&#x3D; 18</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    douban &#x3D; DoubanSpider()</span><br><span class="line">    douban.run()</span><br><span class="line">    print(&quot;complete&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<blockquote>
<p>2019-02-25 更新</p>
</blockquote>
<h2 id="通用案例之糗事百科段子"><a href="#通用案例之糗事百科段子" class="headerlink" title="通用案例之糗事百科段子"></a>通用案例之糗事百科段子</h2><p>作为一名段子手- -不应该只会刷段子，还要学会爬取。。<br>其中用到了 lxml模块和xpath相关内容，爬取到的信息不止有文字内容，还有图片以及作者名字和性别，但此处只在txt文档里放了段子内容。<br>效果图如下：</p>
<img src="/2020/01/10/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/qiubai_01.jpg" class="" title="网页截图">
<img src="/2020/01/10/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/qiubai_02.jpg" class="" title="爬取后文档截图">

<p>代码：</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"># coding&#x3D;utf-8</span><br><span class="line">import requests</span><br><span class="line">from lxml import html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class QuibaiSpdier:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.url_temp &#x3D; &quot;https:&#x2F;&#x2F;www.qiushibaike.com&#x2F;hot&#x2F;page&#x2F;&#123;&#125;&#x2F;&quot;</span><br><span class="line">        self.headers &#x3D; &#123;</span><br><span class="line">            &quot;User-Agent&quot;: &quot;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;68.0.3440.75 Safari&#x2F;537.36&quot;&#125;</span><br><span class="line"></span><br><span class="line">    def get_url_list(self):</span><br><span class="line">        return [self.url_temp.format(i) for i in range(1, 14)]</span><br><span class="line"></span><br><span class="line">    def parse_url(self, url):</span><br><span class="line">        response &#x3D; requests.get(url, headers&#x3D;self.headers)</span><br><span class="line">        return response.content.decode()</span><br><span class="line"></span><br><span class="line">    def get_content_list(self, html_str):</span><br><span class="line">        html_elements &#x3D; html.etree.HTML(html_str)</span><br><span class="line">        div_list &#x3D; html_elements.xpath(&quot;&#x2F;&#x2F;div[@id&#x3D;&#39;content-left&#39;]&#x2F;div&quot;)</span><br><span class="line">        content_list &#x3D; []</span><br><span class="line">        for div in div_list:</span><br><span class="line">            item &#x3D; &#123;&#125;</span><br><span class="line">            item[&quot;content&quot;] &#x3D; div.xpath(&quot;.&#x2F;&#x2F;div[@class&#x3D;&#39;content&#39;]&#x2F;span&#x2F;text()&quot;)</span><br><span class="line">            item[&quot;author_gender&quot;] &#x3D; div.xpath(&quot;.&#x2F;&#x2F;div[contains(@class,&#39;articleGender&#39;)]&#x2F;@class&quot;)</span><br><span class="line">            item[&quot;author_gender&quot;] &#x3D; item[&quot;author_gender&quot;][0].split(&quot; &quot;)[-1].replace(&quot;Icon&quot;, &quot;&quot;) if len(</span><br><span class="line">                item[&quot;author_gender&quot;]) &gt; 0 else None</span><br><span class="line">            item[&quot;content_img&quot;] &#x3D; div.xpath(&quot;.&#x2F;&#x2F;div[@class&#x3D;&#39;thumb&#39;]&#x2F;a&#x2F;img&#x2F;@src&quot;)</span><br><span class="line">            item[&quot;content_img&quot;] &#x3D; &quot;https:&quot; + item[&quot;content_img&quot;][0] if len(item[&quot;content_img&quot;]) &gt; 0 else None</span><br><span class="line">            item[&quot;author_img&quot;] &#x3D; div.xpath(&quot;.&#x2F;&#x2F;div[@class&#x3D;&#39;author clearfix&#39;]&#x2F;&#x2F;img&#x2F;@src&quot;)</span><br><span class="line">            item[&quot;author_img&quot;] &#x3D; &quot;https:&quot; + item[&quot;author_img&quot;][0] if len(item[&quot;author_img&quot;]) &gt; 0 else None</span><br><span class="line">            item[&quot;stats_vote&quot;] &#x3D; div.xpath(&quot;.&#x2F;&#x2F;span[@class&#x3D;&#39;stats-vote&#39;]&#x2F;i&#x2F;text()&quot;)</span><br><span class="line">            item[&quot;stats_vote&quot;] &#x3D; item[&quot;stats_vote&quot;][0] if len(item[&quot;stats_vote&quot;]) &gt; 0 else None</span><br><span class="line">            content_list.append(item)</span><br><span class="line">        return content_list</span><br><span class="line"></span><br><span class="line">    def save_content_list(self, content_list):</span><br><span class="line">        with open(&quot;qiubai.txt&quot;, &quot;a&quot;, encoding&#x3D;&#39;utf-8&#39;) as f:</span><br><span class="line">            for content in content_list:</span><br><span class="line">                # f.write(json.dumps(content, ensure_ascii&#x3D;False))</span><br><span class="line">                f.write(&quot;\n&quot;.join(content[&quot;content&quot;]) + &quot;点赞数：&quot; + content[&quot;stats_vote&quot;])</span><br><span class="line">                f.write(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">    def run(self):  # 实现主要逻辑</span><br><span class="line">        # 1.构造url_list</span><br><span class="line">        url_list &#x3D; self.get_url_list()</span><br><span class="line">        # 2.遍历list，发送请求获取相应</span><br><span class="line">        for url in url_list:</span><br><span class="line">            html_str &#x3D; self.parse_url(url)</span><br><span class="line">            # 3.提取数据</span><br><span class="line">            content_list &#x3D; self.get_content_list(html_str)</span><br><span class="line">            # 4.保存数据</span><br><span class="line">            self.save_content_list(content_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    qiubai &#x3D; QuibaiSpdier()</span><br><span class="line">    qiubai.run()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/10/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/" data-id="ck595a5y10000g04sai8h19ss" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/01/11/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E8%99%AB/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">上一篇</strong>
      <div class="article-nav-title">
        
          多线程爬虫
        
      </div>
    </a>
  
  
    <a href="/2020/01/10/python%E9%97%AD%E5%8C%85/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">下一篇</strong>
      <div class="article-nav-title">python闭包</div>
    </a>
  
</nav>

  
</article>



</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <h1 class="blog-title">Pokemonlei的博客</h1>
    <h2 class="blog-subtitle"></h2>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://avatars0.githubusercontent.com/u/13980642?s=460&amp;v=4">
    <h2 class="author">pokemonlei</h2>
    <h3 class="description">陈磊的博客 | pokemonlei</h3>
    <div class="count-box">
      <a href="/archives"><div><strong>8</strong><br>文章</div></a>
      <a href="/categories"><div><strong>4</strong><br>分类</div></a>
      <a href="/tags"><div><strong>8</strong><br>标签</div></a>
    </div>



    <div class="social-link">
      
        <a class="hvr-bounce-in" href="https://github.com/candychenlei" target="_blank" title="Github">
          Github
        </a>
      
    </div>

    <div class="friend-link">
      <h2>友情链接</h2>
      
        <a class="hvr-bounce-in" href="https://blog.csdn.net/candy_xiaolei" target="_blank" title="ShanaMaid">
          ShanaMaid
        </a>
      
    </div>
  </div>
</div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy;2019 - 2020 pokemonlei<br>
      由<a href="http://hexo.io/" target="_blank">Hexo</a>强力驱动 | 
      主题-<a href="https://github.com/ShanaMaid/hexo-theme-shana" target="_blank" rel="noopener">Shana</a>
      
    </div>
    
  </div>
</footer>
    </div>
    

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="//apps.bdimg.com/libs/wow/0.1.6/wow.min.js"></script>
<script>
new WOW().init();
</script>   


  
<link rel="stylesheet" href="/plugin/fancybox/jquery.fancybox.css">

  
<script src="/plugin/fancybox/jquery.fancybox.pack.js"></script>




  
<link rel="stylesheet" href="/plugin/galmenu/GalMenu.css">

  
<script src="/plugin/galmenu/GalMenu.js"></script>

  <div class="GalMenu GalDropDown">
      <div class="circle" id="gal">
        <div class="ring">
          
            <a href="/" title="" class="menuItem">首页</a>
          
            <a href="/tags" title="" class="menuItem">标签</a>
          
            <a href="/categories" title="" class="menuItem">分类</a>
          
            <a href="/archives" title="" class="menuItem">归档</a>
          
            <a href="/xxxxxxxxx" title="" class="menuItem">喵帕斯</a>
          
            <a href="/xxxxxxx" title="" class="menuItem">凑数</a>
          
        </div>
        
          <audio id="audio" src="#"></audio>
        
      </div> 
</div>
<div id="overlay" style="opacity: 1; cursor: pointer;"></div>
  <script type="text/javascript">var items = document.querySelectorAll('.menuItem');
    for (var i = 0,
    l = items.length; i < l; i++) {
      items[i].style.left = (50 - 35 * Math.cos( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%";
      items[i].style.top = (50 + 35 * Math.sin( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%"
    }</script>
<script type="text/javascript">
  $(document).ready(function() {
    $('body').GalMenu({
      'menu': 'GalDropDown'
    })
  });
</script>

  <section class="hidden-xs"> 
  <ul class="cb-slideshow"> 
    <li><span>苟利</span></li> 
    <li><span>国家</span></li> 
    <li><span>生死以</span></li> 
    <li><span>岂能</span></li> 
    <li><span>祸福</span></li> 
    <li><span>趋避之</span></li> 
  </ul>
</section>

<script src="/js/script.js"></script>




  </div>
</body>
</html>
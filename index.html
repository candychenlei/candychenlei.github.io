<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Pokemonlei的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="陈磊的博客 | pokemonlei">
<meta property="og:type" content="website">
<meta property="og:title" content="Pokemonlei的博客">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Pokemonlei的博客">
<meta property="og:description" content="陈磊的博客 | pokemonlei">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="pokemonlei">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Pokemonlei的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/plugin/bganimation/bg.css">

  

  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<meta name="generator" content="Hexo 4.2.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <div class="widget-wrap mobile-header">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://avatars0.githubusercontent.com/u/20333903?v=3&amp;s=460">
    <h2 class="author">pokemonlei</h2>
    <h3 class="description">陈磊的博客 | pokemonlei</h3>
    <div class="count-box">
      <a href="/archives"><div><strong>14</strong><br>文章</div></a>
      <a href="/categories"><div><strong>7</strong><br>分类</div></a>
      <a href="/tags"><div><strong>13</strong><br>标签</div></a>
    </div>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

        <section id="main">
  
    <article id="post-关卡设计流程-空洞骑士笔记" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/" class="article-date">
  <time class="post-time" datetime="2021-06-12T13:54:00.000Z" itemprop="datePublished">
    <span class="post-month">6月</span><br/>
    <span class="post-day">12</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/">关卡设计流程-空洞骑士笔记</a>
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1/">游戏设计</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>用你目前所拥有的常识来理解问题，如果你目前的常识不支持你理解，则说明你缺少一个前置知识</p>
</blockquote>
<p>  文章参考视频：<a href="https://www.bilibili.com/video/BV11J411Q7Zp/?spm_id_from=333.788.videocard.0" title="[external] [title]" target="">B站《空洞骑士》的关卡设计</a></p>
<h1 id="1-任务设计流程"><a href="#1-任务设计流程" class="headerlink" title="1.任务设计流程"></a>1.任务设计流程</h1><p>  什么是任务？一件事，包含了几个要素：玩家要做什么事情、是谁让玩家去做这件事情、做完之后有什么奖励。</p>
<p>  什么是任务流程？多个任务由于前因后果连在一起的时候，就产生了任务流程这个概念。大多数情况下，单个任务不能将一整个事件完成，所以任务流程就显得很关键。任务流程应该组合一个又一个小型的任务来诱导玩家去完成一个事件。如果上来我们就把所有内容都塞到一个任务中，那么他看起来就像是一个攻略，先去干嘛再去干嘛然后干嘛，显得很没意思。</p>
<p>  什么是事件？事件就是一个完整的有前因后果的剧情内容，通常来说，完成了一个事件就等于完成了一个任务流程，游戏就是由大大小小的事件构成的，有的事件和主线相关很重要，有的则是用于丰富游戏剧情的辅助支线。事件和任务流程的关系？任务流程是组织玩家以一个角色的身份如何参与到这个事件的记事本。通常来说，事件就是一个单纯的故事或者剧情，然而任务流程却可以千变万化。如有一个简单的事件：某村子居民由于某些原因需要n个药草，这就是一个事件，那接下来就是为其设计任务流程，可以让玩家去某地采集，也可以让玩家去购买或直接给金币等等，任务流程会慢慢揭示讲述这个事件的进度，却不一定和事件本身有联系。</p>
<p>  <strong>如果我们将事件看作是一个剧本，那么任务流程就是这个剧本当中，玩家的戏份</strong></p>
<h2 id="1-1-任务流程的结构图"><a href="#1-1-任务流程的结构图" class="headerlink" title="1.1 任务流程的结构图"></a>1.1 任务流程的结构图</h2><p>  以《空洞骑士》来看，最初在遗忘十字路口击败第一个boss的过程：<br>  <img src="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/%E9%81%97%E5%BF%98%E7%9A%84%E5%8D%81%E5%AD%97%E8%B7%AF%E5%8F%A3.jpg" class="" title="遗忘的十字路口"><br>  如果我们不击败Boss1，会发现哪里都去不了，右边的水晶峡谷需要荒芜俯冲才能去，左侧的翠绿之径需要击败第一个Boss，拿到第一个远程技能才能打败看守翠绿之径的第二个Boss。所以如果玩家不去打第一个Boss，那么探索路径差不多是这样的：<br>  <img src="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/%E5%8D%81%E5%AD%97%E8%B7%AF%E5%8F%A3%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%8E%A2%E7%B4%A2.jpg" class="" title="十字路口第一次探索"><br>  所以看起来遗忘十字路通向很多地方，但是最初的时候它却是锁死的。我们用方块表示锁，用菱形表示该锁的钥匙。所以假设没有第一个Boss，那么游戏的设计就是这样的，根本没有通关的方法。<br>  <img src="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/%E6%9C%89%E9%94%81%E6%97%A0%E9%92%A5%E5%8C%99.png" class="" title="有锁无钥匙"><br>  只要打败第一个Boss，就可以获得打败Boss2的方法，相当于拿到了前往翠绿之径的钥匙。此时设计图变为：<br>  <img src="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/%E6%9C%89%E9%94%81%E6%9C%89%E9%92%A5%E5%8C%99.jpg" class="" title="有锁有钥匙"></p>
<p>  从遗忘十字路前往翠绿之径需要钥匙1，于是遗忘十字路中必须给出获取钥匙1的方法，无论要求是什么，可能是击败一个Boss，也有可能是别的什么。在开头的视频中，提到了这种构图的思路：<br>   <img src="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E4%BB%BB%E5%8A%A1%E6%B5%81%E7%A8%8B%E6%9E%84%E5%9B%BE%E6%80%9D%E8%B7%AF.jpg" class="" title="空洞骑士任务流程构图思路"><br>  看第一条从上往下的线，Start是开始游戏，那么要到达End，需要同时通过锁A和B，再看第二条线，要拿到钥匙B，则先要通过另外一个锁A，最后第三条线，我们可以在某处找到钥匙A，这样我们可以回到第二条线解开锁A拿到钥匙B，最后同时拿着钥匙A和B去通关。视频中用这种构图来描绘了空洞骑士的任务流程。当然，我们会实现规定一些标记，在空洞骑士中主要是三类，Boss战，钥匙，锁：<br>  <img src="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E6%9E%84%E5%9B%BE%E6%A0%87%E8%AE%B0.jpg" class="" title="空洞骑士构图标记"><br>  那空洞骑士整体的通关流程大概是如下图：<br>  <img src="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E4%BB%BB%E5%8A%A1%E6%B5%81%E7%A8%8B.jpg" class="" title="空洞骑士任务流程"><br>  按照上面的解释，这个图就很容易看懂了，可以注意下有时候锁可以不是硬锁，如当拥有了钥匙2之后，我们可以返回遗忘十字路来解开锁来到真菌之地，但是也可以不拿钥匙2直接通过翠绿之径前往，如果通过翠绿之径前往，但是钥匙2的作用不仅仅是为了让你来到真菌之地，它还是获取钥匙3的前置条件。<br>  <img src="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/%E9%92%A5%E5%8C%992%E6%98%AF%E9%92%A5%E5%8C%993%E7%9A%84%E5%89%8D%E7%BD%AE.jpg" class="" title="钥匙2是钥匙3的前置"></p>
<h2 id="1-2-任务流程的引导性"><a href="#1-2-任务流程的引导性" class="headerlink" title="1.2 任务流程的引导性"></a>1.2 任务流程的引导性</h2><p>  如果玩家按照错误的流程进行游戏，那可能产生意想不到的bug，如做完第二个任务才去做第一个任务，也会让主线故事的叙事变的错乱崩溃，玩家还有可能无所适从不知道下一步要去哪里。此类游戏以及RPG游戏一般都会以各种npc对话、任务、剧情等方式引导玩家下一步要去哪里做什么，并通过硬性阻挡保证玩家按照正确的流程进入设计好的区域和剧情。此类很常见不赘述，后续有时间可以写一下“塞尔达传说”中大世界的引导性方案</p>
<h1 id="2-地图设计"><a href="#2-地图设计" class="headerlink" title="2.地图设计"></a>2.地图设计</h1><ul>
<li><p>绘制简单的草图可以让我们开始进入到制作游戏关卡的状态。在这个阶段，一切都是可以被修改的。草图涵盖的东西可以是关卡，可以是界面UI，也可以是很多其他的东西，它们的作用除了帮助我们自己分析和打磨原型，更主要的作用是用于记录日志和向其他人说清楚策划想法。</p>
</li>
<li><p>要明确差不多要做一个多大的地图，具体有哪些主要的元素构成。然后再将其分割成单独的区域，有了区域之后我们再将整个区域给分为一批关卡设计的组合。从蔚蓝开发者的演讲当中，我们可以看到这个模块化的思想，从Game到Area，再从Area到Levels。每个关卡都在讲述一个非常简单的故事。</p>
<img src="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/%E8%94%9A%E8%93%9D%E5%85%B3%E5%8D%A1%E6%A8%A1%E5%9D%97%E5%8C%96.jpg" class="" title="空洞骑士任务流程"></li>
<li><p>要善于借鉴，且应当善于分析别的作品当中的地图设计的优点，尝试将这种优秀的设计融入到自己的地图设计中。</p>
</li>
<li><p>最好可以明确地图的需求或预期体验，围绕地图是为了满足什么或想要达到什么样的预期体验而设计，会事半功倍。</p>
<h2 id="2-1-引导性在地图设计和任务流程中的体现"><a href="#2-1-引导性在地图设计和任务流程中的体现" class="headerlink" title="2.1 引导性在地图设计和任务流程中的体现"></a>2.1 引导性在地图设计和任务流程中的体现</h2><p> 游戏中，引导性这个概念可以融入不同的元素中。</p>
</li>
<li><p>常见的RPG类游戏是融入到任务中，会有npc告诉我们要去哪里做什么事情，这类游戏地图在局部做稍微的改动不会有太大影响，如换个boss位置和城镇区域替换。</p>
</li>
<li><p>而空洞骑士这种则是融入到了地图中，他们将所有能够对玩家产生引导性的元素都集中到了地图中，这就要求地图的设计精益求精一环扣一环。</p>
 <img src="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E5%9C%B0%E5%9B%BE%E5%85%A8%E8%B2%8C.jpg" class="" title="空洞骑士地图全貌">
<p> 如果我们有一个任务体系来引导玩家前进。在地图设计上就可以稍微松散一点。不需要什么精巧的设计，也不用像空洞骑士一样像齿轮一样组合在一起。如果没有，我们则需要更加细致的去应对地图的设计。但是相对的，在任务的设计上就要更加细致。也就是无论如何，我们的任务流程设计必须是有的。</p>
<h2 id="2-2-地图设计的层次重点：以蔚蓝和空洞骑士为例"><a href="#2-2-地图设计的层次重点：以蔚蓝和空洞骑士为例" class="headerlink" title="2.2 地图设计的层次重点：以蔚蓝和空洞骑士为例"></a>2.2 地图设计的层次重点：以蔚蓝和空洞骑士为例</h2><p> 同样都是对地图设计比较细致，那么蔚蓝和空洞骑士的区别又在哪呢？我们发现蔚蓝的开发者几乎在每一块落脚处的设计都精益求精，而空洞骑士似乎没有如此严苛。在蔚蓝的开发者的GDC访谈中，提到的关于Game，Area，以及Level的三个等级。蔚蓝的开发者则倾向于把重点放在Level上，而空洞骑士的开发者则倾向于把重点放在Area上。这就是两者在地图设计上的核心区别。简单分析下原因：</p>
<p> 首先蔚蓝是一个平台跳跃游戏，玩家的核心体验是流畅的操作感和精准的控制感。这种线性的设计需求让蔚蓝的开发者把所有的重心都放在了单个的小型的关卡的设计上。在绝大部分的蔚蓝的小型关卡中，玩家的核心目标想方设法的从A点移动到B点。在主线中（即玩家不以收集草莓为目标），玩家几乎不怎么会遇到岔路口，即使遇到，这些岔路也不会将玩家带离太远的距离。<br> 在GDC的采访视频中可以看到，蔚蓝的开发者会先将一个个小型的关卡设计完毕。之后通过一个编辑器将它们组合在一起。但是我想空洞骑士的开发者则会相反，比起单个的小型关卡是如何呈现的，他们首先应该确定的是区域之间的联系。将区域划分好之后开始逐步的设计小型的关卡。</p>
 <img src="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/%E8%94%9A%E8%93%9D%E5%85%B3%E5%8D%A1%E6%8B%BC%E6%8E%A5.jpg" class="" title="蔚蓝关卡拼接">
<p> 蔚蓝中的游戏体验是线性的，虽然空洞骑士也在营造一种玩家是在探索线性关卡的假象，但是事实上玩家可以随心所欲的探索，只是关卡的设计的精巧让玩家感觉自己似乎是按照一个既定的路线在探索。但很多时候可能都是误打误撞正好打了一个可以打的Boss，拿到了对应的道具而已。这种设计正是开发者所期待的，他们希望玩家感受到的就是自己在探索一个开放的世界。<br> 总的来说：在模块化的过程中，必然会开始设计大地图，区域，以及区域中的小关卡设计。认识到这个概念，可以结合具体的案例来探讨一下应该把重心放在什么位置。</p>
<h2 id="2-2-单向阀门"><a href="#2-2-单向阀门" class="headerlink" title="2.2 单向阀门"></a>2.2 单向阀门</h2><p> 所谓单向阀门，就是只能从一侧打开的门，但是打开后就是个双向通道。这个概念在游戏中的应用场景很多<br> 依然以空洞骑士中的遗忘十字路口击败第一个Boss的地图探索过程举例：</p>
 <img src="/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/%E9%81%97%E5%BF%98%E5%8D%81%E5%AD%97%E8%B7%AF%E5%8F%A3%E7%9A%84%E5%8D%95%E5%90%91%E9%98%80%E9%97%A8.jpg" class="" title="遗忘十字路口的单向阀门">
<p> 当玩家从达特茅斯下来之后，如果直接去打第一个Boss，必然会绕一个大的圈子。这种设计不是巧合而是有意为之。假设这个单向阀门是封死的，尝试再去游玩一下，会发现，打败了第一个Boss之后，如果想回到达特茅斯，还是要按照原路返回，绕一个大圈子。但是反过来，如果没有这个单向阀门，几乎不需要任何的探索，玩家似乎就可以直接触摸到Boss。有了这个单向阀门，玩家需要通过一个既定的路线的冒险，探索，才能接触到Boss，而不是在家门口就遇见Boss，这样开发者在有限的地图当中，构建了一个路线较长的探索，但是玩家探索结束之后，又不用再原路返回，只要打开了这个单向阀门，以后就可以自由往返了，削弱了重复探索的疲劳感。</p>
<p> 当然游戏中的单向阀门不一定是这么具象的表现，比如勇者斗恶龙里的传送技能，只能传送到去过的镇子，说白了，这样的一个传送装置，当游戏一开始的时候，是哪里也去不了的。也就是去到第二个镇子的时候，玩家还是需要一步步走过去，之后就可以通过传送装置来传送。之后在两个城镇之间往返就不再需要徒步旅行了。这样的设计使得玩家不用再反复的去探索已经走过的地方（这样既花时间，也没有什么意义）。还有各种传送站点等等。</p>
<p> 我们前面提到过一个概念就是往返，如果一个游戏需要在两个点之间往返，就需要设计这样的地图，那么怎样在往返的时候消除重复探索的疲劳感呢？上面的设计可以总结为以下三种：</p>
</li>
<li><p>空洞骑士风格，终点即起点（但必须在终点处改变地图，此处的改变地图具体指打开那个单向阀门）</p>
</li>
<li><p>勇者斗恶龙风格，终点提供了可以传送回起点的装置，之后就可以来回传了</p>
</li>
<li><p>给定一条更简单的从终点返回起点的路线（当然这条路线必须有效的防止玩家把这条路线也当成从起点到终点的路线）</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2021/06/12/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B-%E7%A9%BA%E6%B4%9E%E9%AA%91%E5%A3%AB%E7%AC%94%E8%AE%B0/" data-id="ckpwl2gc50003f81r628q7379" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%85%B3%E5%8D%A1%E8%AE%BE%E8%AE%A1/" rel="tag">关卡设计</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1/" rel="tag">游戏设计</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-游戏设计信条（二）" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/08/09/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1%E4%BF%A1%E6%9D%A1%EF%BC%88%E4%BA%8C%EF%BC%89/" class="article-date">
  <time class="post-time" datetime="2020-08-09T13:34:42.000Z" itemprop="datePublished">
    <span class="post-month">8月</span><br/>
    <span class="post-day">09</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/08/09/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1%E4%BF%A1%E6%9D%A1%EF%BC%88%E4%BA%8C%EF%BC%89/">游戏设计信条（二）</a>
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1/">游戏设计</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li><p>事件必须激发某种情感才有意义</p>
</li>
<li><p>游戏设计师必须能够感知转瞬即逝的情绪，比如气恼、欢乐、厌恶、紧张、刺激等，因为这些情感才是游戏存在的真正意义。</p>
</li>
<li><p>那些教导玩家进行创造、社交活动、以及战斗的游戏总是最受欢迎，因为他们所展示的技巧有助于人类繁衍，从本能上就会让人想要学习掌握</p>
</li>
<li><p>基本的情感触发器包含如下几个：</p>
<ul>
<li>学习引发的情感</li>
<li>角色弧线引发的情感</li>
<li>挑战引发的情感</li>
<li>社交引发的情感</li>
<li>财富引发的情感</li>
<li>音乐引发的情感</li>
<li>场景特效引发的情感</li>
<li>由美而引发的情感</li>
<li>环境引发的情感</li>
<li>新技术引发的情感</li>
<li>原始威胁引发的情感</li>
<li>性暗示引发的情感</li>
</ul>
</li>
<li><p>学习引发的情感：</p>
<ul>
<li>一种知识越是复杂和难以掌握，学会它的成就感就越大。所以对于游戏设计师来说，挑战在于需要创造出具有很多层不明显特性，有待玩家发掘的游戏系统。要设计出一个有深度的游戏，需要层层揭示信息，并且每一层都构建于前一层的基础之上。如MOBA，段位越高所掌握的信息和技巧就会越多越难</li>
<li>领悟：当玩家得到某些新的信息时，如果他突然明白了原有的一些信息的含义，那么他就是有所领悟。领悟是由于获得了一个新信息而引发的其他知识之间的连锁反应。当我们补全了逻辑链上缺少的最后一个部分而使得逻辑条理清晰时，领悟现象就会出现。在经历了一长串的信息构建之后，所有这些信息各归其位，产生了明确的含义，这就是最棒的领悟方式。</li>
</ul>
</li>
<li><p>我们从他人身上感受到的情感会在自己身上反映出来。角色弧线满足了一种特别的求知欲：我们热衷于了解自己的同类，我们对他人的内心挣扎尤其感兴趣，因为只有在冲突过程中，人的内在价值和能力才会展现出来。他们面对的冲突越强烈，我们对他们真实本性的了解就越深入。我们对一个英雄选择脱脂牛奶还是全脂牛奶不感兴趣，可是当他必须在妻子和自己的性命之间做出选择的时候，我们会好奇这个英雄会何去何从。</p>
</li>
<li><p>挑战引发的情感，当我们努力迎接挑战的时候，会进入到一种快乐的、精神高度集中的状态，当通过这些测试时我们会感觉自己充满力量无所不能，即使失败了，只要玩家感觉到有可能成功，他们也会再尝试一次并试图做得更好。</p>
</li>
<li><p>社交引发的情感：</p>
<ul>
<li>社交引发的情感不止局限于游戏内，游戏外也可以，比如：家人之间找一个能够长时间交谈的机会，父子之间一对一交谈时间久了难免别扭，而当他们一起参与一项不需要思考的游戏时，这种障碍就迎刃而解了。所以游戏简单不用动脑子不一定是缺陷，可能就是游戏本身特性。如果游戏过于复杂反而会影响参与者之间的交流。</li>
<li>游戏内的社交，多数游戏都会使用明确的事件来引发玩家之间交互，如击败某玩家，一起合作创造了某个东西或者一起完成了某个挑战。但每一种社交事件都要有效的改变对人来来说有价值的事情的状态，如陌生人变成朋友，地位从低到高等。</li>
</ul>
</li>
<li><p>音乐引发的情感：音乐是一种强大并且灵活的可以产生情感的工具。由于音乐可以轻易地融入其他体验之中，许多媒体都会大量使用音乐。与大多数情感触发器相比音乐是及其微妙的，因为我们在听到音乐时并不会刻意的去关注它，但是即使主观意识没有关注音乐，我们的潜意识仍然会将音乐源源不断的转换成对应的情感。听一下游戏的原生音乐，你就可以重现很多游戏中的感觉。音效可以制造情感，如：</p>
<ul>
<li>金属发出的刺耳声音会让人觉得不舒服</li>
<li>心跳声可以制造悬念</li>
<li>下雨声会让人感觉沉静</li>
<li>挤压流体状的东西会让人感到恶心</li>
</ul>
</li>
<li><p>美并不是一个事物所具有的某种特征，而是事物对我们的影响。即便只是看到美的事物也会让人感觉身心愉悦。</p>
</li>
<li><p>沉浸是最强大的游戏体验之一，沉浸指的是玩家自己的思维和他在游戏中的角色融为了一体，于是发生在游戏角色身上的事件对玩家来说也意义重大，就好像这个事件是真实的发生在玩家自己身上一样；当玩家的体验能够反映游戏角色的体验时，沉浸现象就会出现，这意味着玩家需要听到看到和游戏角色一样的东西，也要想到和感受到和游戏角色一样的东西。如同样害怕、好奇、生气等。可以理解为一种心理镜像。</p>
</li>
<li><p>肉眼与耳朵捕获到的战斗信息经过大脑处理后，获得是否命中，什么时间命中，攻击威力多大，受击者做出怎样的受击表现，攻击者如何卸力收招，这5个问题的答案，如果与游戏给予的反馈一致，那就是好的打击感。</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/08/09/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1%E4%BF%A1%E6%9D%A1%EF%BC%88%E4%BA%8C%EF%BC%89/" data-id="ckpwl2gbv0000f81rbb7a68ub" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1/" rel="tag">游戏设计</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-游戏设计信条（一）" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/07/10/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1%E4%BF%A1%E6%9D%A1%EF%BC%88%E4%B8%80%EF%BC%89/" class="article-date">
  <time class="post-time" datetime="2020-07-10T03:35:59.000Z" itemprop="datePublished">
    <span class="post-month">7月</span><br/>
    <span class="post-day">10</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/07/10/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1%E4%BF%A1%E6%9D%A1%EF%BC%88%E4%B8%80%EF%BC%89/">游戏设计信条（一）</a>
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1/">游戏设计</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li><p>向游戏中注入自己人生的观点和看法</p>
</li>
<li><p>各行各业都接触一些，不要只局限于游戏动漫</p>
</li>
<li><p>游戏应该让玩家感觉到自己拥有在现实中所没有或很难拥有的能力与特点，比如强悍、聪敏、富有、飞翔、成为英雄，又或者养一只宝可梦等等</p>
</li>
<li><p>思考游戏的包装、海报宣传等周边设计，往往可从另一个角度来反馈到游戏玩法上</p>
</li>
<li><p>从一个“有趣”的创意出发，在游戏开发过程中慢慢剃掉“无趣”的部分，剩下的就都是有趣的了</p>
</li>
<li><p>剧情三角论：角色-&gt;世界-&gt;事件，三个只能突出一个，任何多选组合都会让玩家觉得不爽，因为玩家无法聚焦，会导致看不懂在讲什么的问题</p>
</li>
<li><p>问：怎么从一个大概想法，演变到一个成型的地图和玩法的？</p>
<ul>
<li>首先验证最小集合，验证完毕后去在之前的框架中做填充扩展设计，对于新人可以先走第一步，把核心最小集拉爆，看具不具备可玩性，是否有创意</li>
</ul>
</li>
<li><p>游戏角色的动作一定要有个性</p>
</li>
<li><p>如果创造角色时从游戏的个性出发，那么最终你会自然而然的想出很有趣的动画和游戏细节，很多角色的动机也不止一个</p>
</li>
<li><p>画好游戏角色的剪影，清晰鲜明的游戏剪影可以：</p>
<ul>
<li>一眼能看出角色大概的性格</li>
<li>区分于其他角色</li>
<li>辨识敌我</li>
<li>角色在场景中跃然而出</li>
</ul>
</li>
<li><p>游戏应该允许玩家自定义一些东西以展示自己的个性空间，如：</p>
<ul>
<li>名字（角色、宠物、等等）</li>
<li>外貌</li>
<li>时装</li>
<li>家园</li>
<li>武器（外形组装，强化特效，武器装扮等）</li>
</ul>
</li>
<li><p>设置一个NPC最基本的考虑点：玩家在游戏里想要什么，给他提供什么</p>
</li>
<li><p>墨西哥披萨理论：把两种你认为很不搭调的东西放在一起，有可能会产生意想不到的惊喜。</p>
</li>
<li><p>可以多留意主题公园的平面图，主题公园的规划设计往往能用最有效的方式引导玩家从一次冒险进入下一次冒险</p>
</li>
<li><p>确定关卡的主题：逃亡生存、探索、教育等。除了这些目标，还需要问自己：“在这关里玩家的目的是什么？”，是教会玩家一些特殊操作还是提供一些物品。这个问题的答案能为自己指明方向，发现这一关游戏性设计的要点，使玩法层出不穷。</p>
</li>
<li><p>如果在游戏整个进程中，某个游戏玩法只使用了不到三次，那它的价值就没有得到充分利用。</p>
</li>
<li><p>最棒的游戏概念可以用一个或几个动词来描述，如：跳跃、收集、攀爬、设计、探索、透支、潜行。</p>
</li>
<li><p>大多数人都习惯游戏从左往右行进，如果让角色从右往左走会让玩家感觉心神不宁，即使他们说不出来为什么。把这个方法用在玩家进入游戏最后Boss的老窝时效果最有效。</p>
</li>
<li><p>花点时间解构那些同类游戏里最优秀的机制和行为，这会为你的游戏带来不可估量的好处。</p>
</li>
<li><p>玩家想要那些使他们看起来很酷的游戏</p>
</li>
<li><p>困难和挑战之间存在着差别，困难=增加痛苦和损失，挑战=提高技巧和上升空间。这两者之间的平衡称为趣味曲线，趣味曲线上有一个点，越过后挑战性急转而下剩下的只是痛苦和挫败感。不让玩家越过这个点的方法是创造“阶梯式递增的玩点”，设计者应该把一个玩点建立在另一个玩点之上，这些玩法要素随着游戏进程相互结合并缓慢提高难度。</p>
</li>
<li><p>在无聊和困难中间有一个点，位于这个点的玩家全神贯注，丝毫意识不到时间的流逝</p>
<img src="/2020/07/10/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1%E4%BF%A1%E6%9D%A1%EF%BC%88%E4%B8%80%EF%BC%89/%E5%BF%83%E6%B5%81.png" class="" title="心流">
</li>
<li><p>设计精良的强化道具就是一次高度浓缩的行动，它对玩家的影响要是立即生效的。设计强化道具时，设计师应该开动脑筋，琢磨一下以下几个问题：</p>
<ul>
<li>强化道具有什么用</li>
<li>长什么样子，在整个游戏世界里他怎样凸显出来？发光闪光？旋转跳跃？</li>
<li>强化道具可以互相组合吗？还是玩家每次只能使用一种？</li>
<li>它怎样影响玩家的行动？影响移动速度？影响攻击的次数还是类型？影响生命值还是属性？</li>
<li>它产生的影响如何让玩家知晓？通过视觉上的变化还是声音提示？</li>
<li>为了使用强化道具，玩家需要付出什么代价？在某些强化道具的影响下，玩家的速度或移动性会降低，或者可以采取的行动类型会减少</li>
<li>如何强化道具的影响是暂时的，那它快消失时用什么线索提醒玩家？HUD的元素还是视觉效果or声音提示？还是说直到玩家死亡效果才会消失。</li>
</ul>
</li>
<li><p>大型多人在线角色扮演类游戏，有下述内容能使游戏更饱满，更有竞争力</p>
<ul>
<li>允许玩家在游戏中扮演某个特定角色拥有特定技能，如奶妈可以给全体队友回满血</li>
<li>捏脸，可以在游戏里更进一步鼓励定制角色，让玩家购买或赢得适合其个性或职业所处阶级的衣服、如DNF的时装</li>
<li>聊天，游戏内社交的基础</li>
<li>生产技能，玩家的副职业，制作出来的东西最好容易辨识，比如梦幻西游里的装备会有玩家名字。制作过程可以加入带有技巧的小游戏加速生产速度或者成功率</li>
<li>经济系统，最好发展到现实世界</li>
<li>刷怪，升级速度越来越慢就会导致要刷怪，刷怪通常是人为的延长游戏时间，浪费玩家时间让他们感觉受挫。但“刷怪是一项低风险的活动，玩家可以通过简单重复这一活动获取实在的收益，在存在刷怪的游戏里刷怪就是最佳措施，问题不再是对玩家来说什么才是最佳措施，而变成了在玩家忍无可忍之前，能通过刷怪得到多少好处？”，关于刷怪可以做如下尝试：<ul>
<li>如果内容太相似，就会感觉重复和疲惫，增加点变化进去，如DNF的深渊</li>
<li>玩家去刷怪是因为他们缺少力量、能力或者金钱而无法更进一步，让游戏难度平缓一些，而不是陡然徒增</li>
<li>给玩家更多自由，不要总是迫使他们跟随设计者定义好的目标</li>
<li>提供除了金币和经验以外的其他奖励，那刷怪行为就可以变得不那么像“刷怪”</li>
<li>为刷怪做专属奖励，提供成就或特殊点数</li>
<li>做减法，游戏市场或胜利条件缩短，剑与远征的刷怪就做的相当精简</li>
</ul>
</li>
<li>道具收集，做一些在提升战斗能力之外，还能提升游戏内其他能力的道具，特别是那些非常枯燥冗长的东西，比如生产技能挖矿或采集。大部分道具都能按照稀有度分类，设计些只有打败特定敌人才能得到的道具，或者特定季节特定时间段才能得到的道具</li>
<li>PVP，只有玩家与玩家的对抗才能衍生出无穷的乐趣，玩家也需要通过打败其他玩家来验证自己多厉害。但不能让玩家玩兴正浓时被突然干掉。最好只有特定地点才能PVP，比如PVP频道或者限制城镇里不能PK</li>
<li>避免猎杀行为，猎杀是指一个玩家通过反复杀死另一个玩家而对其进行骚扰的行为。但可以像APEX里一样将比较强的玩家做个辨识度，标记出上一局获胜的玩家</li>
<li>工会，玩家是社会性动物，因此会形成群体。需要再游戏中制造一些用于工会举行社交集会的场所，记得设计一些有利于工会行动的工具，如状态追踪、团战日程表以及工会资产分配等</li>
<li>玩家住所，给玩家一个私人空间，供他们炫耀成就，展示纪念品。有一个基地会让玩家感觉同游戏世界的联系更加紧密，也更有“在自己家里”的感觉，这会把游戏世界变成一个他们乐于一次又一次回来的地方</li>
<li>团战，应该创造出能够靠不同职业组合攻克的副本，甚至可以靠玩家出奇招或者单纯靠运气取胜。创造出这些团战的目标，并且精心雕琢他们，让玩家知道他们得提升自己的实力才能打败它们。设计一个便捷的方式，让玩家在团战过后能够轻松的按照个人偏好分配战利品。</li>
<li>交易拍卖，玩家可能会攒下一大堆完全相同的自己用不到的东西，让玩家能够简单公平、互不欺诈的完成交易。在游戏世界里创建利于交易的地点。</li>
</ul>
</li>
<li><p>设计多人关卡步骤：</p>
<ul>
<li>规划关卡<ul>
<li>玩点是什么？玩点的类型决定着关卡的设计，是类似LOL的红蓝双方对抗，还是类似守望先锋那样的推车，又或者是CSGO、吃鸡那种布满掩体的射击场景</li>
<li>剧情是怎样的？玩家为什么在这个地方战斗？了解剧情会帮助你设计出能够讲述故事的关卡物品</li>
<li>什么让你的地图难忘？对于一个新手玩家来说，最难的事情就是一边熟悉关卡一边活下去。有辨识度的几何形状以及关卡物体有助于在玩家刚出生的那一刻找到自己的方向</li>
<li>为各种类型进行设计，很多多人游戏里都有不同角色，确保为每个类型的玩家设计出能让他们发挥出最佳水平的有趣空间</li>
</ul>
</li>
<li>制作关卡地图<ul>
<li>地图要简单，对玩家来说在一个简单的地图中更容易搞明白怎么走，路和开放空间要使用简单的形状，比如方形圆形8字型。更容易引导玩家的同时也方便做出调整</li>
<li>镜像地图，拿出地图设计稿复制反转黏贴，这样做不仅能节约开发时间，而且有助于玩家探索出自己的领域，等他们踏入敌人老巢，也能更好地知道如何前进</li>
<li>设置防御区，可以让玩家大概知道战斗会在哪里发生，进而制定相应的策略</li>
<li>利用好出生点，给玩家一个活着离开出生点的机会，给蹲点虐泉的玩家造成困难</li>
<li>确保玩家能找到该走的路</li>
<li>提供备用的接近路线，允许玩家使用备用路线，会让他们觉得自己很聪明，还能制造一种鬼祟潜入的氛围，同时也给游戏增加一点不可预测性</li>
</ul>
</li>
<li>建造关卡<ul>
<li>检查碰撞，确保关卡中没有会让玩家卡在几何体当中的位置</li>
<li>试玩测试，用热力图，即标记玩家去过的地方以及游戏过程中都做了些什么的图，来决定玩家应该在哪里战斗、哪里躲藏、死在哪里。利用这些数据来调整你的地图。目的是设计出一个到处都可供玩家游戏的空间，而不是让所有人都挤在一起</li>
<li>颜色，颜色能快速让玩家弄明白身处哪个关卡。玩家会把每个关卡和它的配色表关联起来。也可以利用颜色理论来帮助烘托气氛：绿色让人感觉湿冷，红色让人感觉危险，蓝色让人感觉静谧</li>
</ul>
</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/07/10/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1%E4%BF%A1%E6%9D%A1%EF%BC%88%E4%B8%80%EF%BC%89/" data-id="ckdn4jt8x0000i01rgyvtc6mt" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B8%B8%E6%88%8F%E8%AE%BE%E8%AE%A1/" rel="tag">游戏设计</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-UE4GM系统" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/16/UE4GM%E7%B3%BB%E7%BB%9F/" class="article-date">
  <time class="post-time" datetime="2020-01-16T07:04:03.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">16</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/16/UE4GM%E7%B3%BB%E7%BB%9F/">UE4GM系统</a>
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/">游戏开发</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>UE4的GM系统是基于UCheatManager开发的</p>
</blockquote>
<h1 id="1-处理GM的流程图"><a href="#1-处理GM的流程图" class="headerlink" title="1.处理GM的流程图"></a>1.处理GM的流程图</h1><img src="/2020/01/16/UE4GM%E7%B3%BB%E7%BB%9F/%E5%AE%A2%E6%88%B7%E7%AB%AFGM%E6%B5%81%E7%A8%8B.jpg" class="" title="客户端GM流程">

<p>上图是客户端的处理流程，DS服务器上的处理流程大致类似，只不过玩家在DS上是一个NetConnection，所以PlayerController中保存的UPlayer的派生不是ULocalPlayer，而是UNetConnection。</p>
<h1 id="2-启用GM"><a href="#2-启用GM" class="headerlink" title="2. 启用GM"></a>2. 启用GM</h1><p>默认情况下，GM只会在(GetNetMode() == NM_Standalone || GIsEditor)情况下才会启用，也就是说只要不是单机也不是编辑器下运行，默认是不会启动的，需要在PlayerController里手动调用EnableCheats()，此时只要构建类型不是SHIPPING和TEST，就会启用GM。当然也可以根据需要直接调用AddCheats(true)不论何时都启用。</p>
<h1 id="3-如何添加自己的GM指令"><a href="#3-如何添加自己的GM指令" class="headerlink" title="3.如何添加自己的GM指令"></a>3.如何添加自己的GM指令</h1><p>添加自己的指令也比较简单，通过继承UCheatManager来实现自己的派生类即可，自己的GM类里实现具体的GM指令，方式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">UFUNCTION(exec,BlueprintCallable,Category&#x3D;&quot;Cheat Manager&quot;)</span><br><span class="line">void MyGMName(arglist);</span><br></pre></td></tr></table></figure>
<p>其中UFUNCTION中的exec是必须的。</p>
<p>写完自己的GM类后，需要设置PlayerController中的CheatClass为自己的GM类，之后直接在控制台输入 MyGMName args  即可调用到自己的GM实现。</p>
<h1 id="4-一个可在客户端远程调用DS执行GM的方案"><a href="#4-一个可在客户端远程调用DS执行GM的方案" class="headerlink" title="4.一个可在客户端远程调用DS执行GM的方案"></a>4.一个可在客户端远程调用DS执行GM的方案</h1><p>由于有一些GM，比如添加物品，要在DS服务器上调用然后同步结果到客户端，而我们又不能直接在服务器上输入只能在客户端中输入，那就需要一种方式将输入的GM传给服务器。</p>
<p>这里用到的就是两个：UFUNCTION(exec) 和 UFUNCTION(Reliable, Server, WithValidation, BlueprintCallable) 函数，比如<br>UFUNCTION(exec)<br>void ServerGM(const FString&amp; cmd);</p>
<p>UFUNCTION(Reliable, Server, WithValidation, BlueprintCallable)<br>virtual void ServerGM_RPC(const FString&amp; cmd);</p>
<p>其中ServerGM用来供客户端执行，在函数内通过ServerGM_RPC将客户端想要执行的GM命令和参数同步给服务器来执行。</p>
<p>实现如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">void AMyPlayerController::ServerGM(const FString&amp; cmd)</span><br><span class="line">&#123;</span><br><span class="line">	&#x2F;&#x2F;下面是从引擎源码的UPlayer::ConsoleCommand中抄过来的一段代码，大概作用是使用 | 分隔符处理多段的GM指令</span><br><span class="line">	const int32 CmdLen &#x3D; cmd.Len();</span><br><span class="line">	TCHAR* CommandBuffer &#x3D; (TCHAR*)FMemory::Malloc((CmdLen + 1) * sizeof(TCHAR));</span><br><span class="line">	TCHAR* Line &#x3D; (TCHAR*)FMemory::Malloc((CmdLen + 1) * sizeof(TCHAR));</span><br><span class="line"></span><br><span class="line">	const TCHAR* Command &#x3D; CommandBuffer;</span><br><span class="line">	FCString::Strcpy(CommandBuffer, (CmdLen + 1), *cmd.Left(CmdLen));</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; 使用&#39;|&#39;分隔符处理多段GM指令</span><br><span class="line">	while (FParse::Line(&amp;Command, Line, CmdLen + 1))</span><br><span class="line">	&#123;</span><br><span class="line">		ServerGM_RPC(FString(Line));</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	FMemory::Free(CommandBuffer);</span><br><span class="line">	CommandBuffer &#x3D; nullptr;</span><br><span class="line"></span><br><span class="line">	FMemory::Free(Line);</span><br><span class="line">	Line &#x3D; nullptr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void AMyPlayerController::ServerGM_RPC_Implementation(const FString&amp; cmd)</span><br><span class="line">&#123;</span><br><span class="line">	ConsoleCommand(cmd, true);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>此时如果3中的MyGMName需要在服务器上运行，那客户端执行：ServerGM MyGMName args  即可。<br>整个流程大概是：</p>
<ol>
<li>先按文章开头的照流程图在客户端中执行GM，流程图最后一步UPlayer::Exec会在PlayerController上执行ProcessConsoleExec()，从而调用PlayerController的ServerGM函数，并把MyGMName args当做字符串参数传进去。</li>
<li>ServerGM里一顿操作后，通过RPC，在服务器上调用ServerGM_RPC并将字符串MyGMName args 作为参数</li>
<li>ServerGM_RPC直接调用ConsoleCommand，即直接走到了流程图中的这一步<img src="/2020/01/16/UE4GM%E7%B3%BB%E7%BB%9F/ConsoleCommand%E6%AD%A5%E9%AA%A4.jpg" class="" title="ConsoleCommand步骤"></li>
<li>之后就是按照流程图中，调用UPlayer派生类UNetConnection的Exec，然后走到我们自己的CheatManager，调用MyGMName</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/16/UE4GM%E7%B3%BB%E7%BB%9F/" data-id="ckccph4i80000y81rbyugfnqa" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/UE4/" rel="tag">UE4</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/" rel="tag">游戏开发</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-python与深度学习入门一" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/12/python%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/" class="article-date">
  <time class="post-time" datetime="2020-01-11T16:28:01.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">12</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/12/python%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/">python与深度学习入门一</a>
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="深度学习（deep-learning）"><a href="#深度学习（deep-learning）" class="headerlink" title="深度学习（deep learning）"></a>深度学习（deep learning）</h1><p>深度学习（英语：deep learning）是机器学习的分支，是一种以<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener">人工神经网络</a>为架构，对数据进行<a href="https://zh.wikipedia.org/wiki/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0" target="_blank" rel="noopener">表征学习</a>的算法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别[6]）。深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征。<br>深度神经网络，卷积神经网络和递归神经网络已经被应用计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并获取了极好的效果。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/12/python%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/" data-id="ckccph4ig0006y81r4gikb8fm" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-python与机器学习入门二" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/" class="article-date">
  <time class="post-time" datetime="2020-01-11T14:12:44.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">11</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/">python与机器学习入门二</a>
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="写在开头参考"><a href="#写在开头参考" class="headerlink" title="写在开头参考"></a>写在开头参考</h1><p>机器学习算法分类：</p>
<ul>
<li>监督学习（预测）：有<strong>特征值和目标值</strong>，有标准答案<ul>
<li>分类：对应数据类型为<strong>离散型</strong><ul>
<li>k-近邻算法</li>
<li>贝叶斯分类</li>
<li>决策树与随机森林</li>
<li>逻辑回归</li>
<li>神经网络</li>
</ul>
</li>
<li>回归：对应数据类型为<strong>连续型</strong><ul>
<li>线性回归</li>
<li>岭回归</li>
</ul>
</li>
<li>标注<ul>
<li>隐马尔科夫模型</li>
</ul>
</li>
</ul>
</li>
<li>无监督学习：<strong>只有特征值</strong><ul>
<li>聚类<ul>
<li>k-means</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>特征选择的方式：</p>
<ul>
<li>过滤式：<ul>
<li>低方差特征</li>
</ul>
</li>
<li>嵌入式：<ul>
<li>正则化</li>
<li>决策树</li>
<li>神经网络</li>
</ul>
</li>
</ul>
<h4 id="精确率与召回率"><a href="#精确率与召回率" class="headerlink" title="精确率与召回率"></a>精确率与召回率</h4><p>先熟悉一个概念：混淆矩阵<br>在分类任务下，预测结果与正确标记之间存在四种不同的组合，构成混淆矩阵（适用于多分类），构成如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.jpg" class="" title="混淆矩阵">

<p>而精确率与召回率都是通过这个矩阵来算的：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%B2%BE%E7%A1%AE%E7%8E%87%E4%B8%8E%E5%8F%AC%E5%9B%9E%E7%8E%87.jpg" class="" title="精确率与召回率">

<p>其他标准：<br>F1-score，反映了模型的稳健性，是一个综合评判标准，公式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/F1.jpg" class="" title="F1">

<p>分类评估模型API：</p>
<blockquote>
<p>sklearn.metrics.classification_report(y_true,y_pred,target_name=None)</p>
</blockquote>
<ul>
<li>y_true:真实目标值</li>
<li>y_pred:估计器预测的目标值</li>
<li>target_name:目标类别名称</li>
<li>return:每个类别精确率与召回率，F1，support(预测的数量)</li>
</ul>
<h4 id="模型的选择与调优"><a href="#模型的选择与调优" class="headerlink" title="模型的选择与调优"></a>模型的选择与调优</h4><p>1.交叉验证：为了让被评估的模型更加准确可信</p>
<ul>
<li>1.将所有训练集数据分成N等分，其中一份作为验证集，其余的当作训练集，然后可以得出一个准确率</li>
<li>2.把另一份作为验证集，其余的作为训练集，再得出一个准确率</li>
<li>3.一直重复，直到所有数据都作了一次验证集，而此时可以得到N个准确率，将N个准确率求平均值。<br>分为几等分则成为 <strong>几折交叉验证</strong></li>
</ul>
<p>2.网格搜索（超参数搜索）：调参数，如k-近邻超参数K的调整<br> 通常情况下，有很多参数是需要手动指定的，如k-近邻算法中的k值，这种叫超参数。但是手动过程繁琐，所以需要对模型预设几种超参数组合。<strong>每组超参数都采用交叉验证来进行评估</strong>。最后选出最优参数组合建立模型。</p>
<blockquote>
<p>API：sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)</p>
</blockquote>
<ul>
<li>estimator:估计器对象，如实例化的KNN估计器knn = KNeighborsClassifier()</li>
<li>param_grid：估计器的参数(dict),如knn算法中指定的邻近多少个 {“n_neighbors”:{1,3,5}}</li>
<li>cv:指定几折交叉验证</li>
</ul>
<p>返回的对象可以执行</p>
<ul>
<li>fit:输入训练数据</li>
<li>score:准确率</li>
<li>best_score_ : 在交叉验证中的最好结果</li>
<li>best_estimator : 最好的参数模型</li>
<li>cv_results_ :每次交叉验证后的测试集准确率结果和训练集准确率结果</li>
</ul>
<h4 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h4><p>问题：训练数据训练的很好，误差也很小，为什么在测试集上有问题呢？</p>
<p>机器学习中一个重要的话题便是模型的泛化能力，泛化能力强的模型才是好模型，对于训练好的模型，若在训练集表现差，不必说在测试集表现同样会很差，这可能是欠拟合导致；若模型在训练集表现非常好，却在测试集上差强人意，则这便是过拟合导致的，过拟合与欠拟合也可以用 Bias 与 Variance 的角度来解释，欠拟合会导致高 Bias ，过拟合会导致高 Variance ，所以模型需要在 Bias 与 Variance 之间做出一个权衡。</p>
<p>使用简单的模型去拟合复杂数据时，会导致模型很难拟合数据的真实分布，这时模型便欠拟合了，或者说有很大的 Bias，<strong>Bias 即为模型的期望输出与其真实输出之间的差异</strong>；有时为了得到比较精确的模型而过度拟合训练数据，或者模型复杂度过高时，可能连训练数据的噪音也拟合了，导致模型在训练集上效果非常好，但泛化性能却很差，这时模型便过拟合了，或者说有很大的 Variance，这时模型在不同训练集上得到的模型波动比较大，<strong>Variance 刻画了不同训练集得到的模型的输出与这些模型期望输出的差异</strong>。</p>
<p>欠拟合原因及解决办法：</p>
<ul>
<li>原因：学习到的数据特征特少</li>
<li>解决办法：增加数据的特征数量</li>
</ul>
<p>过拟合的原因及解决办法：</p>
<ul>
<li>原因：<ul>
<li>原始特征过多，存在一些嘈杂特征</li>
</ul>
</li>
<li>解决办法：可通过交叉验证检查是否过拟合<ul>
<li>进行特征选择，消除关联性大的特征（很难做）</li>
<li>正则化</li>
</ul>
</li>
</ul>
<p><strong>L2正则化：</strong><br>作用：可以使得回归系数w的每个元素都很小，都接近0<br>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合</p>
<p>更详细的如何分辨是过拟合还是欠拟合以及如何防止，参考文章：<a href="https://zhuanlan.zhihu.com/p/29707029" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29707029</a></p>
<h4 id="模型的保存和加载"><a href="#模型的保存和加载" class="headerlink" title="模型的保存和加载"></a>模型的保存和加载</h4><blockquote>
<p>from sklearn.externals import joblib<br>scikit-learn中的文件保存格式是 pkl<br>保存：<br>joblib.dump(rf,’test.pkl’)</p>
</blockquote>
<ul>
<li>rf为fit训练好的估计器模型实例</li>
</ul>
<p>加载：<br>estimator = joblib.load(‘test.pkl’)</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。</p>
<ul>
<li>log对数损失函数：逻辑回归</li>
<li>：平方损失函数（最小二乘法）：线性回归</li>
<li>指数损失函数：Adaboost</li>
<li>Hinge损失函数：SVM</li>
<li>其它损失函数：0-1损失函数、绝对值损失函数等<br>参考：<a href="http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/" target="_blank" rel="noopener">http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/</a></li>
</ul>
<h1 id="k-近邻算法（KNN）"><a href="#k-近邻算法（KNN）" class="headerlink" title="k-近邻算法（KNN）"></a>k-近邻算法（KNN）</h1><p>####原理与优缺点<br>KNN是通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的<strong>k个最相似</strong>(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别，其中K通常是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。</p>
<p><strong>KNN算法的结果很大程度取决于K的选择</strong>，如下图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/KNN01.jpg" class="" title="KNN01">

<p>公式：在KNN中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/KNN%E5%85%AC%E5%BC%8F.jpg" class="" title="KNN公式">
<blockquote>
<p>k-近邻算法需要做标准化处理</p>
</blockquote>
<p>优点：</p>
<ul>
<li>简单，易于理解，无需参数估计，无需训练只需要一次计算就可以得出结果</li>
<li>对异常值不敏感</li>
<li>适合对稀有事件进行分类</li>
<li>可以处理多分类问题</li>
</ul>
<p>缺点：</p>
<ul>
<li>对测试样本分类时的计算量大，内存开销大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本; </li>
<li>可解释性差，无法告诉你哪个变量更重要，无法给出决策树那样的规则; </li>
<li>K值的选择：k太小，容易受异常点影响，k取值太大，容易受K值数量(类别)波动。可以采用权值的方法（和该样本距离小的邻居权值大）来改进; </li>
<li>KNN是一种消极学习方法、懒惰算法。</li>
</ul>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>样本数据：</p>
<table>
<thead>
<tr>
<th align="left">电影名称</th>
<th align="left">打斗镜头</th>
<th align="left">接吻镜头</th>
<th align="left">电影类型</th>
</tr>
</thead>
<tbody><tr>
<td align="left">California Man</td>
<td align="left">3</td>
<td align="left">104</td>
<td align="left">爱情片</td>
</tr>
<tr>
<td align="left">He’s Not Really into Dudes</td>
<td align="left">2</td>
<td align="left">100</td>
<td align="left">爱情片</td>
</tr>
<tr>
<td align="left">Beautiful woman</td>
<td align="left">1</td>
<td align="left">81</td>
<td align="left">爱情片</td>
</tr>
<tr>
<td align="left">Kevin Longblade</td>
<td align="left">101</td>
<td align="left">10</td>
<td align="left">动作片</td>
</tr>
<tr>
<td align="left">Robo Slayer 3000</td>
<td align="left">99</td>
<td align="left">5</td>
<td align="left">动作片</td>
</tr>
<tr>
<td align="left">Amped II</td>
<td align="left">98</td>
<td align="left">22</td>
<td align="left">动作片</td>
</tr>
<tr>
<td align="left">？</td>
<td align="left">18</td>
<td align="left">90</td>
<td align="left">未知</td>
</tr>
</tbody></table>
<p>如果我们通过公式（如欧氏距离）计算出已知电影与未知电影的距离如下：</p>
<table>
<thead>
<tr>
<th align="left">电影名称</th>
<th align="left">与未知电影的距离</th>
</tr>
</thead>
<tbody><tr>
<td align="left">California Man</td>
<td align="left">20.5</td>
</tr>
<tr>
<td align="left">He’s Not Really into Dudes</td>
<td align="left">18.7</td>
</tr>
<tr>
<td align="left">Beautiful woman</td>
<td align="left">19.2</td>
</tr>
<tr>
<td align="left">Kevin Longblade</td>
<td align="left">115.3</td>
</tr>
<tr>
<td align="left">Robo Slayer 3000</td>
<td align="left">117.4</td>
</tr>
<tr>
<td align="left">Amped II</td>
<td align="left">118.9</td>
</tr>
</tbody></table>
<p>按照距离递增排序，可以找到k个距离最近的电影。假定k=3，则三个最靠近的电影依次是：</p>
<ul>
<li>He’s Not Really into Dudes</li>
<li>Beautiful woman</li>
<li>California Man<br>kNN按照距离最近的三部电影的类型，决定未知电影的类型——爱情片</li>
</ul>
<h4 id="API与实战demo"><a href="#API与实战demo" class="headerlink" title="API与实战demo"></a>API与实战demo</h4><p>API：sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=’auto’)</p>
<ul>
<li>n_neighbors:int，可选，默认为5，设置查询默认使用的邻居数</li>
<li>algorithm：{‘auto’,’ball_tree’,’kd_tree’,’brute’}，可选用计算最近邻居的算法，’ball_tree’将会使用BallTree，’kd_tree’会使用KDTree，auto根据传递给fit的值来自动决定</li>
</ul>
<p>示例：facebook题目:k近邻算法预测入住位置</p>
<p>数据集介绍：<br>本次实验的数据集来自于Kaggle与Facebook合作的机器学习竞赛，旨在通过由facebook提供的2000多万条数据信息预测一个人想要登记入住的地方。<br>数据来源：<a href="https://www.kaggle.com/c/facebook-v-predicting-check-ins/data" target="_blank" rel="noopener">https://www.kaggle.com/c/facebook-v-predicting-check-ins/data</a><br>其中：<br>train.csv</p>
<ul>
<li>row_id：登记事件的ID</li>
<li>xy：坐标</li>
<li>accuracy：定位准确性</li>
<li>time：时间戳</li>
<li>place_id：业务的ID，也是预测的目标值</li>
</ul>
<p>分析思路：</p>
<ul>
<li>数据集预处理<ul>
<li>数据量太 → 缩小坐标范围</li>
<li>时间数据 → 转化为年月日（增加新的时间特征，便于后续计算）</li>
<li>类别太多 → 按照指定条件进行转换</li>
</ul>
</li>
<li>分割数据及标准化<ul>
<li>选定”place_id”为目标值，数据集预处理结果为目标值。</li>
<li>使用StandardScaler类对数据进行<strong>标准化</strong>处理。</li>
</ul>
</li>
<li>KNN分类预测<ul>
<li>在分类预测过程中，使用超参数搜索API对模型进行选择和调优。</li>
</ul>
</li>
</ul>
<p>数据的原始内容为：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/KNN%E6%95%B0%E6%8D%AE%E6%A6%82%E8%A7%88.jpg" class="" title="KNN数据概览">
<br>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knncls</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    k-近邻算法预测用户入住位置</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 读取数据</span></span><br><span class="line">    data = pd.read_csv(<span class="string">"./train.csv"</span>)</span><br><span class="line">    <span class="comment"># print(data.head(10))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征工程</span></span><br><span class="line">    <span class="comment"># 1.通过设定xy范围缩小数据</span></span><br><span class="line">    data = data.query(<span class="string">"x &gt; 1.0 &amp; x &lt; 1.25 &amp; y &gt; 2.5 &amp; y &lt; 2.75"</span>)  <span class="comment"># query相当于一个查询语句，参数为查询条件</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.处理时间戳数据</span></span><br><span class="line">    time_value = pd.to_datetime(data[<span class="string">'time'</span>], unit=<span class="string">'s'</span>)  <span class="comment"># 将时间戳转换为年-月-日 时-分-秒格式</span></span><br><span class="line">    <span class="comment"># print(time_value)</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    timevalue的值为：</span></span><br><span class="line"><span class="string">    0          1970-01-06 10:45:02</span></span><br><span class="line"><span class="string">    1          1970-01-03 03:49:15</span></span><br><span class="line"><span class="string">    2          1970-01-04 17:37:28</span></span><br><span class="line"><span class="string">    3          1970-01-09 03:43:07</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 把日期转换为字典格式</span></span><br><span class="line">    time_value = pd.DatetimeIndex(time_value)</span><br><span class="line">    <span class="comment"># 3.添加构造一些特征</span></span><br><span class="line">    data[<span class="string">'day'</span>] = time_value.day</span><br><span class="line">    data[<span class="string">'hour'</span>] = time_value.hour</span><br><span class="line">    data[<span class="string">'weekday'</span>] = time_value.weekday</span><br><span class="line">    <span class="comment"># 4.把时间戳特征删除,sklearn中axis为1表示列</span></span><br><span class="line">    data = data.drop([<span class="string">'time'</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.只保留入住人数大于3的目标位置,生成新的data</span></span><br><span class="line">    place_count = data.groupby(<span class="string">"place_id"</span>).count()  <span class="comment"># 按照'place_id'进行分组,然后统计次数。groupby使用参考：https://blog.csdn.net/m0_37870649/article/details/80979809</span></span><br><span class="line">    <span class="comment"># print(place_count)</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    此时print的place_count如下：</span></span><br><span class="line"><span class="string">    此时place_id为索引，而原来的特征值都变为了对应索引的place_id出现了多少次</span></span><br><span class="line"><span class="string">    place_id    row_id    x    y  accuracy  day  hour  weekday                                        </span></span><br><span class="line"><span class="string">    1000015801      78   78   78        78   78    78       78</span></span><br><span class="line"><span class="string">    1000017288      95   95   95        95   95    95       95</span></span><br><span class="line"><span class="string">    1000025138     563  563  563       563  563   563      563</span></span><br><span class="line"><span class="string">    1000052096     961  961  961       961  961   961      961</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    place_count_r = place_count[</span><br><span class="line">        place_count.row_id &gt; <span class="number">3</span>].reset_index()  <span class="comment"># 筛选place_count中row_id大于3的，然后通过reset_index()把place_id重新设为特征而不是索引</span></span><br><span class="line">    train_data = data[data[<span class="string">"place_id"</span>].isin(place_count_r[<span class="string">"place_id"</span>])]     <span class="comment">#通过对比筛选</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取特征值和目标值</span></span><br><span class="line">    x = train_data.drop([<span class="string">"place_id"</span>, <span class="string">"row_id"</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    y = train_data[<span class="string">"place_id"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分割数据集</span></span><br><span class="line">    <span class="comment">#参数：特征值，目标值，测试集比例。返回值依次为：训练集特征值、测试集特征值，训练集目标值，测试集目标值</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对训练集和测试集的特征值进行标准化</span></span><br><span class="line">    std = StandardScaler()</span><br><span class="line"></span><br><span class="line">    x_train = std.fit_transform(x_train)</span><br><span class="line">    x_test = std.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化knn估计器</span></span><br><span class="line">    knn = KNeighborsClassifier()    <span class="comment"># n_neighbors，默认5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#knn.fit(x_train, y_train)   #参数：训练集特征值、训练集目标值</span></span><br><span class="line">    <span class="comment"># 预测结果</span></span><br><span class="line">    <span class="comment">#y_predict = knn.predict(x_test) #参数：测试集</span></span><br><span class="line">    <span class="comment"># 打印准确率</span></span><br><span class="line">    <span class="comment">#print("准确率为:", knn.score(x_test, y_test))</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    准确率为: 0.4732860520094563</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进阶思考：</span></span><br><span class="line">    <span class="comment"># 交叉验证与网格搜索对K-近邻算法调优</span></span><br><span class="line">    param = &#123;<span class="string">"n_neighbors"</span>: [i * <span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">5</span>)]&#125;</span><br><span class="line">    gc = GridSearchCV(knn, param_grid=param, cv=<span class="number">5</span>)</span><br><span class="line">    gc.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"在测试集中的准确率："</span>, gc.score(x_test, y_test))</span><br><span class="line">    print(<span class="string">"在交叉验证中验证集的最好结果："</span>, gc.best_score_)</span><br><span class="line">    print(<span class="string">"使用的最好的模型："</span>, gc.best_estimator_)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    在测试集中的准确率： 0.47848699763593383</span></span><br><span class="line"><span class="string">    在交叉验证中验证集的最好结果： 0.47540983606557374</span></span><br><span class="line"><span class="string">    使用的最好的模型： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',</span></span><br><span class="line"><span class="string">           metric_params=None, n_jobs=None, n_neighbors=8, p=2,</span></span><br><span class="line"><span class="string">           weights='uniform')</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    knncls()</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h1><blockquote>
<p>使用前提：特征之间独立，不互相影响</p>
</blockquote>
<p>该算法是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题</p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>条件概率中： P(A|B)表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。其基本求解公式为：P(A|B) = P(AB)/P(B)。<br>通常我们可以很容易直接得出P(A|B)，P(B|A)则很难直接得出，但我们更关心P(B|A)，贝叶斯定理就为我们打通从P(A|B)获得P(B|A)的道路。<br>贝叶斯公式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F.jpg" class="" title="贝叶斯公式">
<p>其中：</p>
<ul>
<li>P(类别)指每个文档类别的概率，计算方式 (某文档类别数 / 总文档数量)</li>
<li>P(特征|类别) = 指给定类别下特征的概率，计算方式：P(F1|C) = Ni/N<ul>
<li>Ni为该F1特征词在C类别所有文档中出现的次数</li>
<li>N为所属类别C下的文档所有词出现的次数和</li>
</ul>
</li>
<li>在比较一个文章属于不同文章类型的概率时，会发现分母P(特征)是相同的，所以可以省略掉</li>
</ul>
<p><strong>拉普拉斯平滑：</strong><br>如果词频列表里面有很多出现次数都为0的词，在计算的时候可能使计算结果都变为0，这是不合理的，解决方式就是增加拉普拉斯修正</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91%E7%B3%BB%E6%95%B0.jpg" class="" title="拉普拉斯平滑系数">
<p>其中，α为指定的系数，一般为1，m为训练文档中统计出的特征词个数。</p>
<p>朴素贝叶斯分类的流程可以由下图表示：</p>


<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>优点：</p>
<ul>
<li>算法逻辑简单,易于实现</li>
<li>分类过程中时空开销小</li>
<li>对缺失数据不太敏感</li>
<li>对小规模的数据表现很好，能处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练。</li>
</ul>
<p>缺点：<br>理论上，<strong>朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好</strong>。</p>
<h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><p>一个简单示例如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%A4%BA%E4%BE%8B.jpg" class="" title="贝叶斯示例">
<p>可以看到由于云计算在娱乐文章里出现的频率是0，所以导致最后结果是0，这里需要进行拉普拉斯平滑处理。<br>即将P(影院，支付宝，云计算|娱乐)计算时的 分子+1，分母+4，</p>
<h4 id="API与实战demo-1"><a href="#API与实战demo-1" class="headerlink" title="API与实战demo"></a>API与实战demo</h4><p>pythonAPI：sklearn.naive_bayes.MultinomialNB(alpha = 1.0)</p>
<ul>
<li>alpha：拉普拉斯平滑系数，默认1</li>
</ul>
<p>此处的实例用的还是之前内置的sklearn20类新闻分类的数据，20个新闻组数据包含了20个主题的18000个新闻组帖子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naviebayes</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    朴素贝叶斯对新闻进行分类</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    news = fetch_20newsgroups(subset=<span class="string">'all'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行分割</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对数据集进行特征抽取,用tf-idf方法</span></span><br><span class="line">    tf = TfidfVectorizer()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 以训练集中的词的列表进行每篇文章重要性统计</span></span><br><span class="line">    x_train = tf.fit_transform(x_train)  <span class="comment"># 此时x_train为sparse矩阵，大概内容就是记录着:(行，列) 对应数值</span></span><br><span class="line">    print(tf.get_feature_names())</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    特征名，也就是每个单词</span></span><br><span class="line"><span class="string">    'davidst', 'davidstern', 'davidw', 'davie', 'daviel', 'davies', 'davinci', 'davis', 'davison',</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    x_test = tf.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行朴素贝叶斯算法的预测</span></span><br><span class="line">    mlt = MultinomialNB(alpha=<span class="number">1.0</span>)</span><br><span class="line">    print(x_train.toarray())</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    大多数是0，因为一篇文章不是所有的词都有，每一行代表一篇文章，列就是上面的特征名，即单词，行列对应值为TF-IDF值</span></span><br><span class="line"><span class="string">    [[0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="string">    [0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="string">    ..................</span></span><br><span class="line"><span class="string">    [0. 0. 0. ... 0. 0. 0.]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    mlt.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">    y_predict = mlt.predict(x_test)</span><br><span class="line">    print(<span class="string">"预测的文章类别为："</span>, y_predict)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    预测的文章类别为： [ 7 16  0 ...  4  8  1]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得出准确率</span></span><br><span class="line">    print(<span class="string">"准确率为："</span>, mlt.score(x_test, y_test))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    准确率为： 0.8293718166383701</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    naviebayes()</span><br></pre></td></tr></table></figure>


<h5 id="番外篇，R语言与朴素贝叶斯"><a href="#番外篇，R语言与朴素贝叶斯" class="headerlink" title="番外篇，R语言与朴素贝叶斯"></a>番外篇，R语言与朴素贝叶斯</h5><blockquote>
<p>R语言中的klaR包提供了朴素贝叶斯算法实现的函数NaiveBayes<br>NaiveBayes(formula, data, …, subset, na.action= na.pass)<br>NaiveBayes(x, grouping, prior, usekernel= FALSE, fL = 0, …)</p>
</blockquote>
<ul>
<li>formula指定参与模型计算的变量，以公式形式给出，类似于y=x1+x2+x3；</li>
<li>data用于指定需要分析的数据对象；</li>
<li>na.action指定缺失值的处理方法，默认情况下不将缺失值纳入模型计算，也不会发生报错信息，当设为“na.omit”时则会删除含有缺失值的样本；</li>
<li>x指定需要处理的数据，可以是数据框形式，也可以是矩阵形式；</li>
<li>grouping为每个观测样本指定所属类别；</li>
<li>prior可为各个类别指定先验概率，默认情况下用各个类别的样本比例作为先验概率；</li>
<li>usekernel指定密度估计的方法（在无法判断数据的分布时，采用密度密度估计方法），默认情况下使用正态分布密度估计，设为TRUE时，则使用核密度估计方法；</li>
<li>fL指定是否进行拉普拉斯修正，默认情况下不对数据进行修正，当数据量较小时，可以设置该参数为1，即进行拉普拉斯修正。</li>
</ul>
<h1 id="决策树-Decision-Tree-和随机森林-Random-Forest"><a href="#决策树-Decision-Tree-和随机森林-Random-Forest" class="headerlink" title="决策树(Decision Tree)和随机森林(Random Forest)"></a>决策树(Decision Tree)和随机森林(Random Forest)</h1><ul>
<li>决策树是用树的结构来构建分类模型，每个节点代表着一个属性，根据这个属性的划分，进入这个节点的儿子节点，直至叶子节点，每个叶子节点都表征着一定的类别，从而达到分类的目的。<ul>
<li>常用的决策树有ID4，C4.5，CART等。在生成树的过程中，需要选择用那个特征进行剖分，一般来说，选取的原则是，分开后能尽可能地提升纯度，可以用信息增益，增益率，以及基尼系数等指标来衡量。如果是一棵树的话，为了避免过拟合，还要进行剪枝（prunning），取消那些可能会导致验证集误差上升的节点。</li>
</ul>
</li>
<li>随机森林实际上是一种特殊的bagging方法，它将决策树用作bagging中的模型。首先，用bootstrap方法生成m个训练集，然后，对于每个训练集，构造一颗决策树，在节点找特征进行分裂的时候，并不是对所有特征找到能使得指标（如信息增益）最大的，而是在特征中随机抽取一部分特征，在抽到的特征中间找到最优解，应用于节点，进行分裂。随机森林的方法由于有了bagging，也就是集成的思想在，实际上相当于对于样本和特征都进行了采样（如果把训练数据看成矩阵，就像实际中常见的那样，那么就是一个行和列都进行采样的过程），所以可以避免过拟合。</li>
</ul>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h4 id="信息论基础"><a href="#信息论基础" class="headerlink" title="信息论基础"></a>信息论基础</h4><p>在信息论中，熵（英语：entropy）是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。<br>熵最好理解为不确定性的量度而不是确定性的量度，因为<strong>越随机的信源的熵越大</strong>。</p>
<p><strong>香农的信息熵理论：</strong><br>如果n个事件发生的概率分别为p1,p2,p3….pn则他们的准确信息量应该是：<br>H = -(p1Logp1 + p2logp2 + p3logp3 + … + pnlogpn)<br>H的专业术语为信息熵，单位为比特，公式：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B5%E5%85%AC%E5%BC%8F.jpg" class="" title="信息熵公式">

<p>例如，当有32支球队比赛时，猜测谁是冠军，在没有任何信息的情况下，每个球队获胜的概率都是 1/32，此时信息熵为<br>H = -(1/32Log1/32 + 1/32log1/32 + 1/32log1/32 + … + 1/32log1/32) = 5<br>而当我们得到一些信息的情况下，比如不同球队的获胜几率为多少，那我们最终得出的信息熵H将会小于5。</p>
<p><strong>信息增益：</strong><br>当得知一个特征条件之后，减少的信息熵的大小。</p>
<h4 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h4><p><strong>决策树划分依据</strong></p>
<ol>
<li>ID3：信息增益最大<br>特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与 特征A给出条件下D的信息条件熵H(D|A) 之差，即公式:<br>g(D,A) = H(D) - H(D|A)<br>此处信息增益表示，得知特征A的信息而使得类D的信息的不确定性减少的程度，计算H(D)和H(D|A)公式：<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B5%E5%85%AC%E5%BC%8F2.jpg" class="" title="信息熵公式2">

</li>
</ol>
<p>上面这公式有点复杂，举个例子：<br>有如下银行贷款的数据，通过前面的特征来决定是否可以贷款。</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E8%B4%B7%E6%AC%BE%E6%95%B0%E6%8D%AE.jpg" class="" title="贷款数据">
<p>对于没有加入特征之前，只看是否贷款数据的信息熵：一共有15条数据，9条可以贷款，6条不可以。所以此时总的信息熵为：0.971，计算方式如下</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B51.jpg" class="" title="信息熵1">
<p>然后我们让A1，A2,A3,A4分别表示年龄，工作，有自己房子和信贷情况四个特征，则分别计算他们的信息增益即可选择下一次决策用什么特征。<br>此处以年龄为例，而年龄中青年、中年、老年各占5个，即 1/3，根据公式，年龄的信息增益计算为：<br>g(D,A1) = H(D) - H(D’|年龄) = 0.971 - [1/3H(青年) + 1/3H(中年)+ 1/3H(老年)]，<br>此时需要计算H(青年)针对类别的信息熵，由于5个青年中，有3个类别是‘否’，2个对应类别是‘是’，所以计算方式如下：<br>H(青年)= -(2/log2/5 + 3/5log3/5)<br>中年和老年的计算方式类似，最终结果为：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B52.jpg" class="" title="信息熵2">
<p>同理，其他的特征信息增益也可以计算出来，g(D,A2)=0.324,g(D,A3)= 0.420,g(D,A4)= 0.363，相比较A3所对应的有自己的房子这个特征的信息增益最大，所以特征A3为最有特征的一个属性。</p>
<ol start="2">
<li>C4.5：信息增益比最大</li>
<li>CART：回归树<ul>
<li>回归树：平方误差最小</li>
<li>分类树：基尼系数（Gini）最小，基尼系数的划分更加细致</li>
</ul>
</li>
</ol>
<h4 id="决策树优缺点"><a href="#决策树优缺点" class="headerlink" title="决策树优缺点"></a>决策树优缺点</h4><p>优点：</p>
<ul>
<li>简单的理解和解释，树木可视化</li>
<li>不需要太多的数据准备操作，其他技术通常需要数据归一化等操作</li>
</ul>
<p>缺点：</p>
<ul>
<li>决策树可能过于复杂，而且训练集中正确率高不代表测试集中正确率高，有些异常点会导致结果有误。过拟合</li>
</ul>
<p>改进：</p>
<ul>
<li>剪枝cart算法</li>
<li>随机森林</li>
</ul>
<h4 id="经典案例：泰坦尼克号乘幸存预测"><a href="#经典案例：泰坦尼克号乘幸存预测" class="headerlink" title="经典案例：泰坦尼克号乘幸存预测"></a>经典案例：泰坦尼克号乘幸存预测</h4><blockquote>
<p>决策树分类器API：sklearn.tree.DecisionTreeClassifier(criterion=’gini’,max_depth=None,random_state=None)</p>
</blockquote>
<ul>
<li>criterion:默认是‘gini’系数，也可以选择信息增益的熵’entropy’</li>
<li>max_depth:树的深度大小</li>
<li>random_state:随机数种子<br>method：</li>
<li>decision_path:返回决策树的路径</li>
</ul>
<blockquote>
<p>决策树的结构，本地保存API：sklearn.tree.export_graphviz(extimator,outfile=”tree.dot”,feature_names=[‘’,’’])</p>
</blockquote>
<ul>
<li>extimator：实例化的估计器对象</li>
<li>outfile：保存路径和文件，必须是后缀dot格式，这格式课方便的转换为pdf，png等格式</li>
<li>feature_names:特征名，可自己指定</li>
</ul>
<ul>
<li>用软件graphviz运行命令：dot -Tpng tree.dot -o tree.png 来查看，tree.png改为tree.pdf则转换为pdf</li>
</ul>
<p>案例数据：<a href="http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt" target="_blank" rel="noopener">http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt</a><br>数据中包含的特征：票的类别，存活结果，乘坐版（pclass），年龄，登陆，home.dest，房间，票，船和性别等。其中乘坐班是指乘客班(1,2,3),是社会经济阶层的代表。其中age数据还存在缺失需要处理。<br>处理流程：</p>
<ul>
<li>pd读取数据</li>
<li>选择有影响的特征，处理缺失值</li>
<li>进行特征工程，pd转换字典，特征抽取x_train.to_dict(orient=”recored”)</li>
<li>决策树估计器流程</li>
</ul>
<p>案例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier,export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decision</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    决策树对泰坦尼克号乘客进行预测生死</span></span><br><span class="line"><span class="string">    :return:None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    titan = pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理数据，找出特征值和目标值,这里以三个为例</span></span><br><span class="line">    x = titan[[<span class="string">'pclass'</span>, <span class="string">'age'</span>, <span class="string">'sex'</span>]]</span><br><span class="line">    print(x)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">         pclass      age     sex</span></span><br><span class="line"><span class="string">    0       1st  29.0000  female</span></span><br><span class="line"><span class="string">    1       1st   2.0000  female</span></span><br><span class="line"><span class="string">    2       1st  30.0000    male</span></span><br><span class="line"><span class="string">    3       1st  25.0000  female</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    y = titan[<span class="string">'survived'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 缺失值处理</span></span><br><span class="line">    x[<span class="string">'age'</span>].fillna(x[<span class="string">'age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分割数据集到训练集和测试集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行特征工程，当特征是类别的时候，要进行ont-hot编码</span></span><br><span class="line">    dict = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    x_train = dict.fit_transform(x_train.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">    print(dict.get_feature_names())</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(x_train)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    刚好与上面对应</span></span><br><span class="line"><span class="string">    [[29.         1.          0.          0.          1.          0.        ]</span></span><br><span class="line"><span class="string">    [ 2.          1.          0.          0.          1.          0.        ]</span></span><br><span class="line"><span class="string">    [30.          1.          0.          0.          0.          1.        ]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x_test = dict.transform(x_test.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用决策树估计器进行预测</span></span><br><span class="line">    dec = DecisionTreeClassifier()  <span class="comment"># max_depth是个超参数，值可以影响结果，也决定了决策树的深度</span></span><br><span class="line">    dec.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#预测准确率</span></span><br><span class="line">    print(<span class="string">"DecisionTreeClassifier无参数时，预测的准确率为："</span>,dec.score(x_test,y_test))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    DecisionTreeClassifier无参数时，预测的准确率为： 0.8206686930091185</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#导出决策树的结构，截图在之后给出</span></span><br><span class="line">    export_graphviz(dec,out_file=<span class="string">"./tree.dot"</span>,feature_names=[<span class="string">'age'</span>, <span class="string">'pclass=1st'</span>, <span class="string">'pclass=2nd'</span>, <span class="string">'pclass=3rd'</span>, <span class="string">'sex=女性'</span>, <span class="string">'sex=男性'</span>])</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    decision()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上述代码保存后的决策树结构图</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%86%B3%E7%AD%96%E6%A0%91%E7%BB%93%E6%9E%9C%E5%9B%BE.jpg" class="" title="决策树结果图">


<h2 id="随机森林-集成学习方法"><a href="#随机森林-集成学习方法" class="headerlink" title="随机森林-集成学习方法"></a>随机森林-集成学习方法</h2><h4 id="集成学习方法"><a href="#集成学习方法" class="headerlink" title="集成学习方法"></a>集成学习方法</h4><p>集成学习（Ensemble learning）通过组合几种模型来提高机器学习的效果。与单一模型相比，该方法可以提供更好的预测结果。<br>集成学习有一个重要的概念叫<strong>Diversity</strong>，也就是说每个分类器要长得尽量不一样。就像一个团队，有的人擅长策划，有人擅长拉赞助，有人擅长执行，这样才是一个牛逼的团队。集成学习中每个分类器也要有一定差异，这样才是一个好的集成。</p>
<p>集成学习算法主要有Boosting和Bagging两种类型：</p>
<ul>
<li>Boosting：通过迭代地训练一系列的分类器，<strong>每个分类器采用的样本的选择方式都和上一轮的学习结果有关</strong>。例如在AdaBoost中，之前分类错误的样本有较高的可能性被选到，在之前分类正确的样本有较小的概率被选到。就像你在背单词，你今天总会选择前面几天背不下来的单词，Boosting也会选择前面轮数没学下来的样本。这个类别下面的主要算法是AdaBoost和GBDT。</li>
<li>Bagging：每个分类器都随机从原样本中做<strong>有放回的采样</strong>，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。随机森林就是这个类别的一个经典算法，仔细学习你会发现它的每棵树和基本的决策树的不同，非常神奇的算法。</li>
</ul>
<p>额外拓展：集成方法是将几种机器学习技术组合成一个预测模型的元算法，以达到减小方差（bagging）、偏差（boosting）或改进预测（stacking）的效果。</p>
<h4 id="随机森林的定义，过程，优势"><a href="#随机森林的定义，过程，优势" class="headerlink" title="随机森林的定义，过程，优势"></a>随机森林的定义，过程，优势</h4><p>定义：在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。</p>
<p>过程：</p>
<ol>
<li>单个树的建立过程：假如有N个样本，M个特征，随机有放回的抽样，即:Bagging:Bootstrap Aggregating<ul>
<li>随机在N个样本中有放回的选择一个样本，重复N次，因为有放回所以可能有重复</li>
<li>随机在M个特征当中选出m个特征<ul>
<li>m的取值：m&lt;&lt;M</li>
</ul>
</li>
</ul>
</li>
<li>重复1过程，建立多颗决策树，他们的样本和特征大多不一样。 </li>
</ol>
<p>优势：</p>
<ul>
<li>当前所有算法中，具有极好的准确率</li>
<li>能够有效地运行在大数据集上</li>
<li>能够处理具有高维特征的输入样本，而且不需要降维</li>
<li>能够评估各个特征在分类问题上的重要性</li>
</ul>
<h4 id="随机森林与泰坦尼克号乘客幸存预测"><a href="#随机森林与泰坦尼克号乘客幸存预测" class="headerlink" title="随机森林与泰坦尼克号乘客幸存预测"></a>随机森林与泰坦尼克号乘客幸存预测</h4><blockquote>
<p>随机森林API:class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)<br>列举几个重要参数，参数实在太多，超参数也有很多了，需要的时候再搜索好了：</p>
</blockquote>
<ul>
<li>n_estimators : integer，可选default=10,森林里（决策）树的数目</li>
<li>criterion : string, 可选(默认值为“gini”)，衡量分裂质量的性能（函数）。 受支持的标准是基尼不纯度的”gini”,和信息增益的”entropy”（熵）</li>
<li>max_features： int, float, string or None,可选default=”auto”，寻找最佳分割时需要考虑的特征数目，不同的值代表不同的考虑方式：这里n_features为总的特征数<ul>
<li>如果是int，就要考虑每一次分割处的max_feature特征</li>
<li>如果是float，那么max_features就是一个百分比，那么（max_feature*n_features）特征整数值是在每个分割处考虑的</li>
<li>如果是auto，那么max_features=sqrt(n_features)，即n_features的平方根值。</li>
<li>如果是log2，那么max_features=log2(n_features)</li>
<li>如果是None,那么max_features=n_features</li>
</ul>
</li>
<li>max_depth : integer or None, 可选的（默认为None），（决策）树的最大深度。如果值为None，那么会扩展节点，直到所有的叶子是纯净的，或者直到所有叶子包含少于min_sample_split的样本。</li>
<li>bootstrap : boolean, 可选的(default=True)，建立决策树时，是否使用有放回抽样。</li>
</ul>
<p>代码示例：处理数据与之前的决策树一样，只是把决策树估计器改成了随机森林，然后进行了网格搜索与交叉验证</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decision</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    决策树对泰坦尼克号乘客进行预测生死</span></span><br><span class="line"><span class="string">    :return:None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    titan = pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理数据，找出特征值和目标值,这里以三个为例</span></span><br><span class="line">    x = titan[[<span class="string">'pclass'</span>, <span class="string">'age'</span>, <span class="string">'sex'</span>]]</span><br><span class="line">    print(x)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">         pclass      age     sex</span></span><br><span class="line"><span class="string">    0       1st  29.0000  female</span></span><br><span class="line"><span class="string">    1       1st   2.0000  female</span></span><br><span class="line"><span class="string">    2       1st  30.0000    male</span></span><br><span class="line"><span class="string">    3       1st  25.0000  female</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    y = titan[<span class="string">'survived'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 缺失值处理</span></span><br><span class="line">    x[<span class="string">'age'</span>].fillna(x[<span class="string">'age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分割数据集到训练集和测试集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行特征工程，当特征是类别的时候，要进行ont-hot编码</span></span><br><span class="line">    dict = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    x_train = dict.fit_transform(x_train.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">    print(dict.get_feature_names())</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(x_train)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    刚好与上面对应</span></span><br><span class="line"><span class="string">    [[29.         1.          0.          0.          1.          0.        ]</span></span><br><span class="line"><span class="string">    [ 2.          1.          0.          0.          1.          0.        ]</span></span><br><span class="line"><span class="string">    [30.          1.          0.          0.          0.          1.        ]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x_test = dict.transform(x_test.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机森林估计器进行预测，超参数调优(这里以两个参数为例）</span></span><br><span class="line">    rf = RandomForestClassifier()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#网格搜索与交叉验证寻找最佳参数</span></span><br><span class="line">    param = &#123;<span class="string">"n_estimators"</span>:[<span class="number">10</span>,<span class="number">20</span>,<span class="number">100</span>,<span class="number">120</span>,<span class="number">200</span>,<span class="number">300</span>,<span class="number">500</span>,<span class="number">800</span>],<span class="string">"max_depth"</span>:[<span class="number">5</span>,<span class="number">8</span>,<span class="number">15</span>,<span class="number">20</span>,<span class="number">30</span>]&#125;</span><br><span class="line">    gc = GridSearchCV(rf,param_grid=param,cv=<span class="number">2</span>)</span><br><span class="line">    gc.fit(x_train,y_train)</span><br><span class="line">    print(<span class="string">"准确率为："</span>,gc.score(x_test,y_test))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    准确率为： 0.8267477203647416</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"选择的参数模型："</span>,gc.best_params_)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    选择的参数模型： &#123;'max_depth': 5, 'n_estimators': 10&#125;</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    decision()</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="线性回归，迭代的算法"><a href="#线性回归，迭代的算法" class="headerlink" title="线性回归，迭代的算法"></a>线性回归，迭代的算法</h1><blockquote>
<p>回归问题：目标值是连续的，如房价，营业额等。</p>
</blockquote>
<h4 id="定义与原理"><a href="#定义与原理" class="headerlink" title="定义与原理"></a>定义与原理</h4><p>线性关系模型：<br>一个通过属性的线性组合来进行预测的函数:</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%9E%8B.jpg" class="" title="线性关系模型">
<p>w为权重，b称为偏置项，可理解为w0*1</p>
<p>线性回归定义：</p>
<blockquote>
<p>在统计学中，线性回归（Linear regression）是利用称为线性回归方程的<a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95" target="_blank" rel="noopener">最小二乘</a> 函数对一个或多个<strong>自变量和因变量之间关系进行建模</strong>的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。</p>
</blockquote>
<p>通用公式：<br>x为特征值，w为要求的值</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%9A%E7%94%A8%E5%85%AC%E5%BC%8F.jpg" class="" title="线性回归通用公式">


<h4 id="线性回归的损失函数：平方损失函数（最小二乘法-Ordinary-Least-Squares-）"><a href="#线性回归的损失函数：平方损失函数（最小二乘法-Ordinary-Least-Squares-）" class="headerlink" title="线性回归的损失函数：平方损失函数（最小二乘法, Ordinary Least Squares ）"></a>线性回归的损失函数：平方损失函数（最小二乘法, Ordinary Least Squares ）</h4><p>最小二乘法是线性回归的一种，OLS将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从<a href="https://zh.wikipedia.org/wiki/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83" target="_blank" rel="noopener">高斯分布</a>（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：<strong>最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小</strong>。换言之，OLS是基于距离的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：</p>
<ul>
<li>简单，计算方便</li>
<li>欧氏距离是一种很好的相似性度量标准</li>
<li>在不同的表示域变换后特征性质不变</li>
</ul>
<p>下面的式子，f(x)：模型的预测值。Y：真实值<br>平方损失（Square loss）的标准形式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1%E7%9A%84%E6%A0%87%E5%87%86%E5%BD%A2%E5%BC%8F.jpg" class="" title="平方损失的标准形式">

<p>当样本个数为n时，此时的损失函数变为：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%A0%B7%E6%9C%AC%E6%95%B0%E4%B8%BAn%E6%97%B6%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.jpg" class="" title="样本数为n时的损失函数">

<p>Y-f(X)表示的是残差，整个式子表示的是残差的平方和，而我们的目的就是<strong>最小化这个目标函数值</strong>（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。</p>
<p><strong>而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标</strong>，公式如下:假设有n个数据，后面的减法为 预测值-真实值</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%9D%87%E6%96%B9%E5%B7%AE.jpg" class="" title="均方差">

<p>我们通常说的线性有两种情况，一种是因变量y是自变量x的线性函数，一种是因变量y是参数α的线性函数。在机器学习中，通常指的都是后一种情况。</p>
<h4 id="两种求解方法"><a href="#两种求解方法" class="headerlink" title="两种求解方法"></a>两种求解方法</h4><p>从前面得知，线性回归要求每个特征对应的权重w值，使得线性回归的损失函数值最小，有两种方法：</p>
<ol>
<li>最小二乘法之正规方程（不常用）<br>X为特征值矩阵，y为目标值矩阵，求解方程如下：</li>
</ol>
<p>缺点：当特征过于复杂，求解速度太慢</p>
<ol start="2">
<li>最小二乘法之梯度下降SGD（理解过程）<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.jpg" class="" title="梯度下降">
线性回归中一直在迭代，刚开始是原始值减去后面的学习速率*方向，得到一个新的值，下次迭代用这个新的值继续减去 学习速率*方向。<br>使用：面对训练数据规模十分庞大的任务</li>
</ol>
<p>正规方程与梯度下降对比：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%AF%B9%E6%AF%94.jpg" class="" title="对比">


<h4 id="API与案例"><a href="#API与案例" class="headerlink" title="API与案例"></a>API与案例</h4><blockquote>
<p>sklearn.linear_model.LinearRegression()</p>
</blockquote>
<ul>
<li>普通最小二乘线性回归</li>
<li>coef_：求出的回归系数</li>
</ul>
<blockquote>
<p>sklearn.linear_model.SGDRegressor()</p>
</blockquote>
<ul>
<li>通过使用SGD最小化线性模型</li>
<li>coef_:求出的回归系数</li>
</ul>
<p>均方差算回归损失的API：</p>
<blockquote>
<p>sklearn.metrics.mean_squared_error(y_true,y_pred)</p>
</blockquote>
<ul>
<li>y_true:真实值</li>
<li>y_pred:预测值</li>
<li>return：浮点数结果</li>
<li>真实值和预测值为标准化之前的值</li>
</ul>
<p>案例：波士顿房价预测<br>流程：</p>
<ol>
<li>数据获取</li>
<li>数据分割</li>
<li>训练与测试数据<strong>标准化处理</strong></li>
<li>使用最简单的线性回归模型LinearRegression和SGDRegressor对房价进行预测</li>
</ol>
<p>代码案例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, SGDRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mylinear</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    线性回归预测房子价格</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    lb = load_boston()</span><br><span class="line">    <span class="comment"># 分割数据集到训练集和测试集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(lb.data, lb.target, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行标准化</span></span><br><span class="line">    <span class="comment"># 这里特征值和目标值都要进行</span></span><br><span class="line">    std_x = StandardScaler()  <span class="comment"># 对特征值进行标准化</span></span><br><span class="line">    x_train = std_x.fit_transform(x_train)</span><br><span class="line">    x_test = std_x.transform(x_test)</span><br><span class="line"></span><br><span class="line">    std_y = StandardScaler()  <span class="comment"># 对目标值进行标准化</span></span><br><span class="line">    y_train = std_y.fit_transform(y_train.reshape(<span class="number">-1</span>, <span class="number">1</span>))  <span class="comment"># 要求是二维</span></span><br><span class="line">    y_test = std_y.fit_transform(y_test.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># estimator预测</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 正规方程求解方式预测</span></span><br><span class="line">    lr = LinearRegression()</span><br><span class="line">    lr.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"回归系数为："</span>, lr.coef_)</span><br><span class="line">    <span class="string">''''</span></span><br><span class="line"><span class="string">    回归系数为： [[-0.13482789  0.14684301  0.01549439  0.03522313 -0.21053828  0.32880086</span></span><br><span class="line"><span class="string">    -0.00599198 -0.39363529  0.25896495 -0.2026098  -0.21658332  0.09938895</span></span><br><span class="line"><span class="string">    -0.38005182]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 预测测试集的房子价格</span></span><br><span class="line">    y_lr_predict = std_y.inverse_transform(lr.predict(x_test))  <span class="comment"># 此时的值还是标准化后的,所以要inverse</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"测试集里每个房子的预测价格："</span>, y_lr_predict)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    测试集里每个房子的预测价格： [[-5.23859608]</span></span><br><span class="line"><span class="string">    [39.61950876]</span></span><br><span class="line"><span class="string">    [35.86414606]</span></span><br><span class="line"><span class="string">    ......</span></span><br><span class="line"><span class="string">    [20.31808398]</span></span><br><span class="line"><span class="string">    [17.36664138]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"正规方程的均方差："</span>, mean_squared_error(std_y.inverse_transform(y_test), y_lr_predict))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    正规方程的均方差： 24.203056713653485</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 梯度下降进行房价预测</span></span><br><span class="line">    sgd = SGDRegressor()  <span class="comment"># 这里可以指定学习率</span></span><br><span class="line">    sgd.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"SGD预测回归系数为："</span>, sgd.coef_)</span><br><span class="line">    <span class="string">''''</span></span><br><span class="line"><span class="string">    SGD预测回归系数为： [-0.08189344  0.07032661 -0.03413881  0.10527339 -0.09947185  0.33781714</span></span><br><span class="line"><span class="string">    -0.04008106 -0.21279579  0.06362954 -0.05064323 -0.19061834  0.10570017</span></span><br><span class="line"><span class="string">    -0.36071628]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 预测测试集的房子价格</span></span><br><span class="line">    y_sgd_predict = std_y.inverse_transform(sgd.predict(x_test))  <span class="comment"># 此时的值还是标准化后的,所以要inverse</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"测试集里每个房子的预测价格："</span>, y_sgd_predict)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    测试集里每个房子的预测价格：[22.25491159 30.38773853 12.72306638 35.68389966 29.69842243 21.70338892</span></span><br><span class="line"><span class="string">    15.23925266 27.85422604 24.72436984 31.05806439 16.91816538 28.40787834</span></span><br><span class="line"><span class="string">    ......</span></span><br><span class="line"><span class="string">    21.3051416  23.92421059 20.83558394 23.80267794 28.17431605 32.1886237</span></span><br><span class="line"><span class="string">    25.89847536]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"梯度下降的均方差："</span>, mean_squared_error(std_y.inverse_transform(y_test), y_sgd_predict))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    梯度下降的均方差： 24.303529166136546</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    mylinear()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="岭回归解决过拟合"><a href="#岭回归解决过拟合" class="headerlink" title="岭回归解决过拟合"></a>岭回归解决过拟合</h2><p>线性回归LinearRegression 容易出现过拟合，为了把训练集数据表现更好<br>而岭回归是带有正则化的线性回归</p>
<p>线性回归与岭回归Ridge对比：<br>岭回归得到的<strong>回归系数更符合实际，更可靠</strong>。另外，能让估计参数的波动范围变小，变的稳定。在存在病态数据偏多的研究中有较大的实用价值。</p>
<blockquote>
<p>具有L2正则化的线性最小二乘法：sklearn.linear_model.Ridge(alpha=1.0)</p>
</blockquote>
<ul>
<li>alpha：正则化力度</li>
<li>coef_:求解的回归系数w</li>
</ul>
<p>上面的案例用岭回归解决：</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 岭回归进行房价预测</span><br><span class="line">rd &#x3D; Ridge(alpha&#x3D;1.0)  # 这里可以指定学习率</span><br><span class="line">rd.fit(x_train, y_train)</span><br><span class="line">print(&quot;岭回归预测回归系数为：&quot;, rd.coef_)</span><br><span class="line">&#39;&#39;&#39;&#39;</span><br><span class="line">岭回归预测回归系数为： [[-0.07357443  0.11581584  0.0248411   0.06555144 -0.17895055  0.27716807</span><br><span class="line">-0.01529063 -0.32589572  0.24111193 -0.19075882 -0.21855738  0.09673899</span><br><span class="line">-0.44673051]]</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"># 预测测试集的房子价格</span><br><span class="line">y_rd_predict &#x3D; std_y.inverse_transform(rd.predict(x_test))  # 此时的值还是标准化后的,所以要inverse</span><br><span class="line"></span><br><span class="line">print(&quot;测试集里每个房子的预测价格：&quot;, y_rd_predict)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">测试集里每个房子的预测价格： [[33.05501377]</span><br><span class="line">[29.49658894]</span><br><span class="line">......</span><br><span class="line">[19.62373227]</span><br><span class="line">[12.41679022]]</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">print(&quot;岭回归的均方差：&quot;, mean_squared_error(std_y.inverse_transform(y_test), y_sgd_predict))</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">岭回归的均方差： 27.097530207872403</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>逻辑回归的输入就是线性回归的式子，而且也是自我学习的过程，要迭代回归<br>逻辑回归可以做什么，适应场景：二分类问题</p>
<ul>
<li>广告是否会被点击</li>
<li>是否为垃圾邮件</li>
<li>是否患病</li>
<li>金融诈骗</li>
</ul>
<p>逻辑回归的输入：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%BE%93%E5%85%A5.jpg" class="" title="逻辑回归输入">

<h4 id="过程：如何从线性回归的输入变为逻辑回归的分类"><a href="#过程：如何从线性回归的输入变为逻辑回归的分类" class="headerlink" title="过程：如何从线性回归的输入变为逻辑回归的分类"></a>过程：如何从线性回归的输入变为逻辑回归的分类</h4><p>先了解一下什么事sigmoid函数：将输入转换为0~1，与y轴交叉点默认为0.5</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/sigmoid%E5%87%BD%E6%95%B0.jpg" class="" title="sigmoid函数">
<ul>
<li>横坐标是一个具体的值</li>
<li>输出纵坐标是0~1之间的值，可看做概率值</li>
</ul>
<p>而sigmoid的公式和逻辑回归的公式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%85%AC%E5%BC%8F.jpg" class="" title="逻辑回归公式">
<ul>
<li>e:常数2.71</li>
<li>Z：回归的结果</li>
</ul>
<h4 id="逻辑回归的损失函数：log对数似然损失函数"><a href="#逻辑回归的损失函数：log对数似然损失函数" class="headerlink" title="逻辑回归的损失函数：log对数似然损失函数"></a>逻辑回归的损失函数：log对数似然损失函数</h4><p>有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即max F(y, f(x)) —&gt; min -F(y, f(x)))。从损失函数的视角来看，它就成了log损失函数了。</p>
<p>损失函数公式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%AF%B9%E6%95%B0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.jpg" class="" title="对数损失函数">

<p>当目标值是1，此时损失值与预测结果为1的概率（x轴）的关系如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%9B%AE%E6%A0%87%E5%80%BC%E4%B8%BA1.jpg" class="" title="目标值为1">

<p>当目标值是0，此时损失值与预测结果为0的概率（x轴）的关系如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%9B%AE%E6%A0%87%E5%80%BC%E6%98%AF0.jpg" class="" title="目标值是0">


<h4 id="API与案例："><a href="#API与案例：" class="headerlink" title="API与案例："></a>API与案例：</h4><blockquote>
<p>逻辑回归API：sklearn.linear_model.LogisticRegression(penalty=’l2’,C=1.0)</p>
</blockquote>
<ul>
<li>penalty:正则化方式</li>
<li>c:正则化粒度</li>
</ul>
<p>案例：预测癌症肿瘤（乳腺癌）<br>数据地址：<a href="http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data" target="_blank" rel="noopener">http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data</a><br>数据内容：</p>
<ul>
<li>每个案例11列数据，第一列是检索Id，后9列是与病情相关的医学特征，最后1列表示肿瘤类型的数值</li>
<li>肿瘤类型数值：2为良性，占65.5%，4为恶性，占34.5%<ul>
<li>注：逻辑回归中，因为针对的是2分类型，所以只有两个结果，只用计算一个概率值来表示属于某一种类型的概率，另一个为(1-概率),而这个概率值一般是数据中占比比较少的那一个类型，比如这里的恶性</li>
</ul>
</li>
<li>其中包含几个缺失值，用“？”标出</li>
</ul>
<p>流程：</p>
<ul>
<li>从网上获取数据</li>
<li>数据缺失值处理、标准化</li>
<li>估计器流程</li>
</ul>
<p>代码案例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    逻辑回归做二分类进行癌症肿瘤是否是恶性的预测（根据属性特征）</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 构造列标签名字,注：如果数据中没有标签，则pandas会默认把第一行数据作为特征名，这里数据是没有标签名的，所以这里手动添加</span></span><br><span class="line">    column = [<span class="string">'Sample code number'</span>,<span class="string">'Clump Thickness'</span>,<span class="string">'Uniformity of Cell Size'</span>,<span class="string">'Uniformity of Cell Shape'</span>,<span class="string">'Marginal Adhesion'</span>,<span class="string">'Single Epithelial Cell Size'</span></span><br><span class="line">              ,<span class="string">'Bare Nuclei'</span>,<span class="string">'Bland Chromatin'</span>,<span class="string">'Normal Nucleoli'</span>,<span class="string">'Mitoses'</span>,<span class="string">'Class'</span>]</span><br><span class="line">    data = pd.read_csv(<span class="string">"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"</span></span><br><span class="line">                , names=column)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#缺失值进行处理</span></span><br><span class="line">    data = data.replace(to_replace=<span class="string">'?'</span>,value=np.nan)</span><br><span class="line">    data = data.dropna()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据分割</span></span><br><span class="line">    x_train,x_test,y_train,y_test = train_test_split(data[column[<span class="number">1</span>:<span class="number">10</span>]],data[column[<span class="number">10</span>]],test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#进行标准化</span></span><br><span class="line">    std = StandardScaler()</span><br><span class="line">    x_train = std.fit_transform(x_train)</span><br><span class="line">    x_test = std.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#逻辑回归预测</span></span><br><span class="line">    lg = LogisticRegression(penalty=<span class="string">'l2'</span>,C=<span class="number">1.0</span>)</span><br><span class="line">    lg.fit(x_train,y_train)</span><br><span class="line">    y_predict = lg.predict(x_test)</span><br><span class="line">    print(<span class="string">"计算的回归系数为："</span>,lg.coef_)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算的回归系数为： [[1.07139763 0.920439   1.03352718 0.88427896 0.07783286 1.15006497</span></span><br><span class="line"><span class="string">    0.94210002 0.59530658 0.81971895]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"准确率为："</span>,lg.score(x_test,y_test))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    准确率为： 0.9590643274853801</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"召回率："</span>,classification_report(y_test,y_predict,labels=[<span class="number">2</span>,<span class="number">4</span>],target_names=[<span class="string">"良性"</span>,<span class="string">"恶性"</span>]))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    召回率：       precision    recall  f1-score   support</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">          良性       0.96      0.97      0.97       110</span></span><br><span class="line"><span class="string">          恶性       0.95      0.93      0.94        61</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     micro avg       0.96      0.96      0.96       171</span></span><br><span class="line"><span class="string">     macro avg       0.96      0.95      0.96       171</span></span><br><span class="line"><span class="string">  weighted avg       0.96      0.96      0.96       171</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    logistic()</span><br></pre></td></tr></table></figure>


<h1 id="非监督学习：K-均值聚类（k-means）算法"><a href="#非监督学习：K-均值聚类（k-means）算法" class="headerlink" title="非监督学习：K-均值聚类（k-means）算法"></a>非监督学习：K-均值聚类（k-means）算法</h1><blockquote>
<p>K-均值聚类属于无监督学习。那么监督学习和无监督学习的区别在哪儿呢？监督学习知道从对象（数据）中学习什么，而无监督学习无需知道所要搜寻的目标，它是根据算法得到数据的共同特征。比如用分类和聚类来说，分类事先就知道所要得到的类别，而聚类则不一样，只是以相似度为基础，将对象分得不同的簇。</p>
</blockquote>
<p>K-Means算法有大量的变体，包括初始化优化K-Means++, 距离计算优化elkan K-Means算法和大数据情况下的优化Mini Batch K-Means算法。</p>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><p>K-means是一个反复迭代的过程，算法分为四个步骤：</p>
<ol>
<li>选取数据空间中的K个对象作为初始中心，每个对象代表一个聚类中心；<ul>
<li>我们会根据对数据的先验经验选择一个合适的k值，如果没有什么先验知识，则可以通过交叉验证选择一个合适的k值。</li>
</ul>
</li>
<li>对于样本中的数据对象，根据它们与这些聚类中心的欧氏距离，按距离最近的准则将它们分到距离它们最近的聚类中心（最相似）所对应的类；</li>
<li>更新聚类中心：将每个类别中所有对象所对应的均值作为该类别的聚类中心，计算目标函数的值；</li>
<li>判断聚类中心和目标函数的值是否发生改变，若不变，则输出结果，若改变，则返回2）</li>
</ol>
<p>这里发现了一个python2.7版本的简易实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span>      <span class="comment">#general function to parse tab -delimited floats</span></span><br><span class="line">    dataMat = []                <span class="comment">#assume last column is target value</span></span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        fltLine = map(float,curLine) <span class="comment">#map all elements to float()</span></span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br><span class="line">\<span class="comment">#计算欧氏距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum(power(vecA - vecB, <span class="number">2</span>))) <span class="comment">#la.norm(vecA-vecB)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">    n = shape(dataSet)[<span class="number">1</span>]</span><br><span class="line">    centroids = mat(zeros((k,n)))<span class="comment">#create centroid mat</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):<span class="comment">#create random cluster centers, within bounds of each dimension</span></span><br><span class="line">        minJ = min(dataSet[:,j]) </span><br><span class="line">        rangeJ = float(max(dataSet[:,j]) - minJ)</span><br><span class="line">        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line">\<span class="comment">#dataSet样本点,k 簇的个数</span></span><br><span class="line">\<span class="comment">#disMeas距离量度，默认为欧几里得距离</span></span><br><span class="line">\<span class="comment">#createCent,初始点的选取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>] <span class="comment">#样本数</span></span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="number">2</span>))) <span class="comment">#m*2的矩阵                   </span></span><br><span class="line">    centroids = createCent(dataSet, k) <span class="comment">#初始化k个中心</span></span><br><span class="line">    clusterChanged = <span class="literal">True</span>             </span><br><span class="line">    <span class="keyword">while</span> clusterChanged:      <span class="comment">#当聚类不再变化</span></span><br><span class="line">        clusterChanged = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            minDist = inf; minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k): <span class="comment">#找到最近的质心</span></span><br><span class="line">                distJI = distMeas(centroids[j,:],dataSet[i,:])</span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:</span><br><span class="line">                    minDist = distJI; minIndex = j</span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i,<span class="number">0</span>] != minIndex: clusterChanged = <span class="literal">True</span></span><br><span class="line">            <span class="comment"># 第1列为所属质心，第2列为距离</span></span><br><span class="line">            clusterAssment[i,:] = minIndex,minDist**<span class="number">2</span></span><br><span class="line">        <span class="keyword">print</span> centroids</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更改质心位置</span></span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):</span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[:,<span class="number">0</span>].A==cent)[<span class="number">0</span>]]</span><br><span class="line">            centroids[cent,:] = mean(ptsInClust, axis=<span class="number">0</span>) </span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>伪代码如下：<br>function K-Means(输入数据，中心点个数K)<br>    获取输入数据的维度Dim和个数N<br>    随机生成K个Dim维的点<br>    while(算法未收敛)<br>        对N个点：计算每个点属于哪一类。<br>        对于K个中心点：<br>            1，找出所有属于自己这一类的所有数据点<br>            2，把自己的坐标修改为这些数据点的中心点坐标<br>    end<br>    输出结果：<br>end</p>
<blockquote>
<p>sklearn.cluster.KMeans(n_clusters=8,init=’k-means++’) </p>
</blockquote>
<ul>
<li>n_clusters:开始的聚类中心数量</li>
<li>init:初始化方法，默认为’k-means++’</li>
<li>labels_:默认标记的类型，可以和真实值比较（不是值比较）</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/" data-id="ckccph4ih0007y81rb1hr9e8x" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

    </footer>
  </div>
  
</article>




  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">next &amp;raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <h1 class="blog-title">Pokemonlei的博客</h1>
    <h2 class="blog-subtitle"></h2>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://avatars0.githubusercontent.com/u/20333903?v=3&amp;s=460">
    <h2 class="author">pokemonlei</h2>
    <h3 class="description">陈磊的博客 | pokemonlei</h3>
    <div class="count-box">
      <a href="/archives"><div><strong>14</strong><br>文章</div></a>
      <a href="/categories"><div><strong>7</strong><br>分类</div></a>
      <a href="/tags"><div><strong>13</strong><br>标签</div></a>
    </div>



    <div class="social-link">
      
        <a class="hvr-bounce-in" href="https://user.qzone.qq.com/1176014533/infocenter" target="_blank" title="QQZone">
          QQZone
        </a>
      
    </div>

    <div class="friend-link">
      <h2>友情链接</h2>
      
        <a class="hvr-bounce-in" href="http://blog.shanamaid.top/" target="_blank" title="ShanaMaid">
          ShanaMaid
        </a>
      
    </div>
  </div>
</div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy;2020 - 2021 pokemonlei<br>
      由<a href="http://hexo.io/" target="_blank">Hexo</a>强力驱动 | 
      主题-<a href="https://github.com/ShanaMaid/hexo-theme-shana" target="_blank" rel="noopener">Shana</a>
      
    </div>
    
  </div>
</footer>
    </div>
    

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="//apps.bdimg.com/libs/wow/0.1.6/wow.min.js"></script>
<script>
new WOW().init();
</script>   


  
<link rel="stylesheet" href="/plugin/fancybox/jquery.fancybox.css">

  
<script src="/plugin/fancybox/jquery.fancybox.pack.js"></script>




  
<link rel="stylesheet" href="/plugin/galmenu/GalMenu.css">

  
<script src="/plugin/galmenu/GalMenu.js"></script>

  <div class="GalMenu GalDropDown">
      <div class="circle" id="gal">
        <div class="ring">
          
            <a href="/" title="" class="menuItem">首页</a>
          
            <a href="/tags" title="" class="menuItem">标签</a>
          
            <a href="/categories" title="" class="menuItem">分类</a>
          
            <a href="/archives" title="" class="menuItem">归档</a>
          
            <a href="/xxxxxxxxx" title="" class="menuItem">占位1</a>
          
            <a href="/xxxxxxx" title="" class="menuItem">占位2</a>
          
        </div>
        
          <audio id="audio" src="#"></audio>
        
      </div> 
</div>
<div id="overlay" style="opacity: 1; cursor: pointer;"></div>
  <script type="text/javascript">var items = document.querySelectorAll('.menuItem');
    for (var i = 0,
    l = items.length; i < l; i++) {
      items[i].style.left = (50 - 35 * Math.cos( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%";
      items[i].style.top = (50 + 35 * Math.sin( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%"
    }</script>
<script type="text/javascript">
  $(document).ready(function() {
    $('body').GalMenu({
      'menu': 'GalDropDown'
    })
  });
</script>

  <section class="hidden-xs"> 
  <ul class="cb-slideshow"> 
    <li><span>苟利</span></li> 
    <li><span>国家</span></li> 
    <li><span>生死以</span></li> 
    <li><span>岂能</span></li> 
    <li><span>祸福</span></li> 
    <li><span>趋避之</span></li> 
  </ul>
</section>

<script src="/js/script.js"></script>




  </div>
</body>
</html>
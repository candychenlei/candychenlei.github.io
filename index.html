<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Pokemonlei的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="陈磊的博客 | pokemonlei">
<meta property="og:type" content="website">
<meta property="og:title" content="Pokemonlei的博客">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Pokemonlei的博客">
<meta property="og:description" content="陈磊的博客 | pokemonlei">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="pokemonlei">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Pokemonlei的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<link rel="stylesheet" href="/plugin/bganimation/bg.css">

  

  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div class="outer">
        <div class="widget-wrap mobile-header">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://avatars0.githubusercontent.com/u/20333903?v=3&amp;s=460">
    <h2 class="author">pokemonlei</h2>
    <h3 class="description">陈磊的博客 | pokemonlei</h3>
    <div class="count-box">
      <a href="/archives"><div><strong>11</strong><br>文章</div></a>
      <a href="/categories"><div><strong>6</strong><br>分类</div></a>
      <a href="/tags"><div><strong>11</strong><br>标签</div></a>
    </div>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

        <section id="main">
  
    <article id="post-UE4GM系统" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/16/UE4GM%E7%B3%BB%E7%BB%9F/" class="article-date">
  <time class="post-time" datetime="2020-01-16T07:04:03.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">16</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/16/UE4GM%E7%B3%BB%E7%BB%9F/">UE4GM系统</a>
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/">游戏开发</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>UE4的GM系统是基于UCheatManager开发的</p>
</blockquote>
<h1 id="1-处理GM的流程图"><a href="#1-处理GM的流程图" class="headerlink" title="1.处理GM的流程图"></a>1.处理GM的流程图</h1><img src="/2020/01/16/UE4GM%E7%B3%BB%E7%BB%9F/%E5%AE%A2%E6%88%B7%E7%AB%AFGM%E6%B5%81%E7%A8%8B.jpg" class="" title="客户端GM流程">

<p>上图是客户端的处理流程，DS服务器上的处理流程大致类似，只不过玩家在DS上是一个NetConnection，所以PlayerController中保存的UPlayer的派生不是ULocalPlayer，而是UNetConnection。</p>
<h1 id="2-启用GM"><a href="#2-启用GM" class="headerlink" title="2. 启用GM"></a>2. 启用GM</h1><p>默认情况下，GM只会在(GetNetMode() == NM_Standalone || GIsEditor)情况下才会启用，也就是说只要不是单机也不是编辑器下运行，默认是不会启动的，需要在PlayerController里手动调用EnableCheats()，此时只要构建类型不是SHIPPING和TEST，就会启用GM。当然也可以根据需要直接调用AddCheats(true)不论何时都启用。</p>
<h1 id="3-如何添加自己的GM指令"><a href="#3-如何添加自己的GM指令" class="headerlink" title="3.如何添加自己的GM指令"></a>3.如何添加自己的GM指令</h1><p>添加自己的指令也比较简单，通过继承UCheatManager来实现自己的派生类即可，自己的GM类里实现具体的GM指令，方式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">UFUNCTION(exec,BlueprintCallable,Category&#x3D;&quot;Cheat Manager&quot;)</span><br><span class="line">void MyGMName(arglist);</span><br></pre></td></tr></table></figure>
<p>其中UFUNCTION中的exec是必须的。</p>
<p>写完自己的GM类后，需要设置PlayerController中的CheatClass为自己的GM类，之后直接在控制台输入 MyGMName args  即可调用到自己的GM实现。</p>
<h1 id="4-一个可在客户端远程调用DS执行GM的方案"><a href="#4-一个可在客户端远程调用DS执行GM的方案" class="headerlink" title="4.一个可在客户端远程调用DS执行GM的方案"></a>4.一个可在客户端远程调用DS执行GM的方案</h1><p>由于有一些GM，比如添加物品，要在DS服务器上调用然后同步结果到客户端，而我们又不能直接在服务器上输入只能在客户端中输入，那就需要一种方式将输入的GM传给服务器。</p>
<p>这里用到的就是两个：UFUNCTION(exec) 和 UFUNCTION(Reliable, Server, WithValidation, BlueprintCallable) 函数，比如<br>UFUNCTION(exec)<br>void ServerGM(const FString&amp; cmd);</p>
<p>UFUNCTION(Reliable, Server, WithValidation, BlueprintCallable)<br>virtual void ServerGM_RPC(const FString&amp; cmd);</p>
<p>其中ServerGM用来供客户端执行，在函数内通过ServerGM_RPC将客户端想要执行的GM命令和参数同步给服务器来执行。</p>
<p>实现如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">void AMyPlayerController::ServerGM(const FString&amp; cmd)</span><br><span class="line">&#123;</span><br><span class="line">	&#x2F;&#x2F;下面是从引擎源码的UPlayer::ConsoleCommand中抄过来的一段代码，大概作用是使用 | 分隔符处理多段的GM指令</span><br><span class="line">	const int32 CmdLen &#x3D; cmd.Len();</span><br><span class="line">	TCHAR* CommandBuffer &#x3D; (TCHAR*)FMemory::Malloc((CmdLen + 1) * sizeof(TCHAR));</span><br><span class="line">	TCHAR* Line &#x3D; (TCHAR*)FMemory::Malloc((CmdLen + 1) * sizeof(TCHAR));</span><br><span class="line"></span><br><span class="line">	const TCHAR* Command &#x3D; CommandBuffer;</span><br><span class="line">	FCString::Strcpy(CommandBuffer, (CmdLen + 1), *cmd.Left(CmdLen));</span><br><span class="line"></span><br><span class="line">	&#x2F;&#x2F; 使用&#39;|&#39;分隔符处理多段GM指令</span><br><span class="line">	while (FParse::Line(&amp;Command, Line, CmdLen + 1))</span><br><span class="line">	&#123;</span><br><span class="line">		ServerGM_RPC(FString(Line));</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	FMemory::Free(CommandBuffer);</span><br><span class="line">	CommandBuffer &#x3D; nullptr;</span><br><span class="line"></span><br><span class="line">	FMemory::Free(Line);</span><br><span class="line">	Line &#x3D; nullptr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void AMyPlayerController::ServerGM_RPC_Implementation(const FString&amp; cmd)</span><br><span class="line">&#123;</span><br><span class="line">	ConsoleCommand(cmd, true);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>此时如果3中的MyGMName需要在服务器上运行，那客户端执行：ServerGM MyGMName args  即可。<br>整个流程大概是：</p>
<ol>
<li>先按文章开头的照流程图在客户端中执行GM，流程图最后一步UPlayer::Exec会在PlayerController上执行ProcessConsoleExec()，从而调用PlayerController的ServerGM函数，并把MyGMName args当做字符串参数传进去。</li>
<li>ServerGM里一顿操作后，通过RPC，在服务器上调用ServerGM_RPC并将字符串MyGMName args 作为参数</li>
<li>ServerGM_RPC直接调用ConsoleCommand，即直接走到了流程图中的这一步<img src="/2020/01/16/UE4GM%E7%B3%BB%E7%BB%9F/ConsoleCommand%E6%AD%A5%E9%AA%A4.jpg" class="" title="ConsoleCommand步骤"></li>
<li>之后就是按照流程图中，调用UPlayer派生类UNetConnection的Exec，然后走到我们自己的CheatManager，调用MyGMName</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/16/UE4GM%E7%B3%BB%E7%BB%9F/" data-id="ck9sdq07v000170vb6b7ih4k2" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/UE4/" rel="tag">UE4</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/" rel="tag">游戏开发</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-python与深度学习入门一" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/12/python%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/" class="article-date">
  <time class="post-time" datetime="2020-01-11T16:28:01.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">12</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/12/python%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/">python与深度学习入门一</a>
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="深度学习（deep-learning）"><a href="#深度学习（deep-learning）" class="headerlink" title="深度学习（deep learning）"></a>深度学习（deep learning）</h1><p>深度学习（英语：deep learning）是机器学习的分支，是一种以<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener">人工神经网络</a>为架构，对数据进行<a href="https://zh.wikipedia.org/wiki/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0" target="_blank" rel="noopener">表征学习</a>的算法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别[6]）。深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征。<br>深度神经网络，卷积神经网络和递归神经网络已经被应用计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并获取了极好的效果。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/12/python%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/" data-id="ck9sdq07x000370vb21rrd0xi" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-python与机器学习入门二" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/" class="article-date">
  <time class="post-time" datetime="2020-01-11T14:12:44.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">11</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/">python与机器学习入门二</a>
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="写在开头参考"><a href="#写在开头参考" class="headerlink" title="写在开头参考"></a>写在开头参考</h1><p>机器学习算法分类：</p>
<ul>
<li>监督学习（预测）：有<strong>特征值和目标值</strong>，有标准答案<ul>
<li>分类：对应数据类型为<strong>离散型</strong><ul>
<li>k-近邻算法</li>
<li>贝叶斯分类</li>
<li>决策树与随机森林</li>
<li>逻辑回归</li>
<li>神经网络</li>
</ul>
</li>
<li>回归：对应数据类型为<strong>连续型</strong><ul>
<li>线性回归</li>
<li>岭回归</li>
</ul>
</li>
<li>标注<ul>
<li>隐马尔科夫模型</li>
</ul>
</li>
</ul>
</li>
<li>无监督学习：<strong>只有特征值</strong><ul>
<li>聚类<ul>
<li>k-means</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>特征选择的方式：</p>
<ul>
<li>过滤式：<ul>
<li>低方差特征</li>
</ul>
</li>
<li>嵌入式：<ul>
<li>正则化</li>
<li>决策树</li>
<li>神经网络</li>
</ul>
</li>
</ul>
<h4 id="精确率与召回率"><a href="#精确率与召回率" class="headerlink" title="精确率与召回率"></a>精确率与召回率</h4><p>先熟悉一个概念：混淆矩阵<br>在分类任务下，预测结果与正确标记之间存在四种不同的组合，构成混淆矩阵（适用于多分类），构成如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.jpg" class="" title="混淆矩阵">

<p>而精确率与召回率都是通过这个矩阵来算的：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%B2%BE%E7%A1%AE%E7%8E%87%E4%B8%8E%E5%8F%AC%E5%9B%9E%E7%8E%87.jpg" class="" title="精确率与召回率">

<p>其他标准：<br>F1-score，反映了模型的稳健性，是一个综合评判标准，公式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/F1.jpg" class="" title="F1">

<p>分类评估模型API：</p>
<blockquote>
<p>sklearn.metrics.classification_report(y_true,y_pred,target_name=None)</p>
</blockquote>
<ul>
<li>y_true:真实目标值</li>
<li>y_pred:估计器预测的目标值</li>
<li>target_name:目标类别名称</li>
<li>return:每个类别精确率与召回率，F1，support(预测的数量)</li>
</ul>
<h4 id="模型的选择与调优"><a href="#模型的选择与调优" class="headerlink" title="模型的选择与调优"></a>模型的选择与调优</h4><p>1.交叉验证：为了让被评估的模型更加准确可信</p>
<ul>
<li>1.将所有训练集数据分成N等分，其中一份作为验证集，其余的当作训练集，然后可以得出一个准确率</li>
<li>2.把另一份作为验证集，其余的作为训练集，再得出一个准确率</li>
<li>3.一直重复，直到所有数据都作了一次验证集，而此时可以得到N个准确率，将N个准确率求平均值。<br>分为几等分则成为 <strong>几折交叉验证</strong></li>
</ul>
<p>2.网格搜索（超参数搜索）：调参数，如k-近邻超参数K的调整<br> 通常情况下，有很多参数是需要手动指定的，如k-近邻算法中的k值，这种叫超参数。但是手动过程繁琐，所以需要对模型预设几种超参数组合。<strong>每组超参数都采用交叉验证来进行评估</strong>。最后选出最优参数组合建立模型。</p>
<blockquote>
<p>API：sklearn.model_selection.GridSearchCV(estimator,param_grid=None,cv=None)</p>
</blockquote>
<ul>
<li>estimator:估计器对象，如实例化的KNN估计器knn = KNeighborsClassifier()</li>
<li>param_grid：估计器的参数(dict),如knn算法中指定的邻近多少个 {“n_neighbors”:{1,3,5}}</li>
<li>cv:指定几折交叉验证</li>
</ul>
<p>返回的对象可以执行</p>
<ul>
<li>fit:输入训练数据</li>
<li>score:准确率</li>
<li>best_score_ : 在交叉验证中的最好结果</li>
<li>best_estimator : 最好的参数模型</li>
<li>cv_results_ :每次交叉验证后的测试集准确率结果和训练集准确率结果</li>
</ul>
<h4 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h4><p>问题：训练数据训练的很好，误差也很小，为什么在测试集上有问题呢？</p>
<p>机器学习中一个重要的话题便是模型的泛化能力，泛化能力强的模型才是好模型，对于训练好的模型，若在训练集表现差，不必说在测试集表现同样会很差，这可能是欠拟合导致；若模型在训练集表现非常好，却在测试集上差强人意，则这便是过拟合导致的，过拟合与欠拟合也可以用 Bias 与 Variance 的角度来解释，欠拟合会导致高 Bias ，过拟合会导致高 Variance ，所以模型需要在 Bias 与 Variance 之间做出一个权衡。</p>
<p>使用简单的模型去拟合复杂数据时，会导致模型很难拟合数据的真实分布，这时模型便欠拟合了，或者说有很大的 Bias，<strong>Bias 即为模型的期望输出与其真实输出之间的差异</strong>；有时为了得到比较精确的模型而过度拟合训练数据，或者模型复杂度过高时，可能连训练数据的噪音也拟合了，导致模型在训练集上效果非常好，但泛化性能却很差，这时模型便过拟合了，或者说有很大的 Variance，这时模型在不同训练集上得到的模型波动比较大，<strong>Variance 刻画了不同训练集得到的模型的输出与这些模型期望输出的差异</strong>。</p>
<p>欠拟合原因及解决办法：</p>
<ul>
<li>原因：学习到的数据特征特少</li>
<li>解决办法：增加数据的特征数量</li>
</ul>
<p>过拟合的原因及解决办法：</p>
<ul>
<li>原因：<ul>
<li>原始特征过多，存在一些嘈杂特征</li>
</ul>
</li>
<li>解决办法：可通过交叉验证检查是否过拟合<ul>
<li>进行特征选择，消除关联性大的特征（很难做）</li>
<li>正则化</li>
</ul>
</li>
</ul>
<p><strong>L2正则化：</strong><br>作用：可以使得回归系数w的每个元素都很小，都接近0<br>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合</p>
<p>更详细的如何分辨是过拟合还是欠拟合以及如何防止，参考文章：<a href="https://zhuanlan.zhihu.com/p/29707029" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29707029</a></p>
<h4 id="模型的保存和加载"><a href="#模型的保存和加载" class="headerlink" title="模型的保存和加载"></a>模型的保存和加载</h4><blockquote>
<p>from sklearn.externals import joblib<br>scikit-learn中的文件保存格式是 pkl<br>保存：<br>joblib.dump(rf,’test.pkl’)</p>
</blockquote>
<ul>
<li>rf为fit训练好的估计器模型实例</li>
</ul>
<p>加载：<br>estimator = joblib.load(‘test.pkl’)</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。</p>
<ul>
<li>log对数损失函数：逻辑回归</li>
<li>：平方损失函数（最小二乘法）：线性回归</li>
<li>指数损失函数：Adaboost</li>
<li>Hinge损失函数：SVM</li>
<li>其它损失函数：0-1损失函数、绝对值损失函数等<br>参考：<a href="http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/" target="_blank" rel="noopener">http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/</a></li>
</ul>
<h1 id="k-近邻算法（KNN）"><a href="#k-近邻算法（KNN）" class="headerlink" title="k-近邻算法（KNN）"></a>k-近邻算法（KNN）</h1><p>####原理与优缺点<br>KNN是通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的<strong>k个最相似</strong>(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别，其中K通常是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。</p>
<p><strong>KNN算法的结果很大程度取决于K的选择</strong>，如下图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/KNN01.jpg" class="" title="KNN01">

<p>公式：在KNN中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/KNN%E5%85%AC%E5%BC%8F.jpg" class="" title="KNN公式">
<blockquote>
<p>k-近邻算法需要做标准化处理</p>
</blockquote>
<p>优点：</p>
<ul>
<li>简单，易于理解，无需参数估计，无需训练只需要一次计算就可以得出结果</li>
<li>对异常值不敏感</li>
<li>适合对稀有事件进行分类</li>
<li>可以处理多分类问题</li>
</ul>
<p>缺点：</p>
<ul>
<li>对测试样本分类时的计算量大，内存开销大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本; </li>
<li>可解释性差，无法告诉你哪个变量更重要，无法给出决策树那样的规则; </li>
<li>K值的选择：k太小，容易受异常点影响，k取值太大，容易受K值数量(类别)波动。可以采用权值的方法（和该样本距离小的邻居权值大）来改进; </li>
<li>KNN是一种消极学习方法、懒惰算法。</li>
</ul>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p>样本数据：</p>
<table>
<thead>
<tr>
<th align="left">电影名称</th>
<th align="left">打斗镜头</th>
<th align="left">接吻镜头</th>
<th align="left">电影类型</th>
</tr>
</thead>
<tbody><tr>
<td align="left">California Man</td>
<td align="left">3</td>
<td align="left">104</td>
<td align="left">爱情片</td>
</tr>
<tr>
<td align="left">He’s Not Really into Dudes</td>
<td align="left">2</td>
<td align="left">100</td>
<td align="left">爱情片</td>
</tr>
<tr>
<td align="left">Beautiful woman</td>
<td align="left">1</td>
<td align="left">81</td>
<td align="left">爱情片</td>
</tr>
<tr>
<td align="left">Kevin Longblade</td>
<td align="left">101</td>
<td align="left">10</td>
<td align="left">动作片</td>
</tr>
<tr>
<td align="left">Robo Slayer 3000</td>
<td align="left">99</td>
<td align="left">5</td>
<td align="left">动作片</td>
</tr>
<tr>
<td align="left">Amped II</td>
<td align="left">98</td>
<td align="left">22</td>
<td align="left">动作片</td>
</tr>
<tr>
<td align="left">？</td>
<td align="left">18</td>
<td align="left">90</td>
<td align="left">未知</td>
</tr>
</tbody></table>
<p>如果我们通过公式（如欧氏距离）计算出已知电影与未知电影的距离如下：</p>
<table>
<thead>
<tr>
<th align="left">电影名称</th>
<th align="left">与未知电影的距离</th>
</tr>
</thead>
<tbody><tr>
<td align="left">California Man</td>
<td align="left">20.5</td>
</tr>
<tr>
<td align="left">He’s Not Really into Dudes</td>
<td align="left">18.7</td>
</tr>
<tr>
<td align="left">Beautiful woman</td>
<td align="left">19.2</td>
</tr>
<tr>
<td align="left">Kevin Longblade</td>
<td align="left">115.3</td>
</tr>
<tr>
<td align="left">Robo Slayer 3000</td>
<td align="left">117.4</td>
</tr>
<tr>
<td align="left">Amped II</td>
<td align="left">118.9</td>
</tr>
</tbody></table>
<p>按照距离递增排序，可以找到k个距离最近的电影。假定k=3，则三个最靠近的电影依次是：</p>
<ul>
<li>He’s Not Really into Dudes</li>
<li>Beautiful woman</li>
<li>California Man<br>kNN按照距离最近的三部电影的类型，决定未知电影的类型——爱情片</li>
</ul>
<h4 id="API与实战demo"><a href="#API与实战demo" class="headerlink" title="API与实战demo"></a>API与实战demo</h4><p>API：sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=’auto’)</p>
<ul>
<li>n_neighbors:int，可选，默认为5，设置查询默认使用的邻居数</li>
<li>algorithm：{‘auto’,’ball_tree’,’kd_tree’,’brute’}，可选用计算最近邻居的算法，’ball_tree’将会使用BallTree，’kd_tree’会使用KDTree，auto根据传递给fit的值来自动决定</li>
</ul>
<p>示例：facebook题目:k近邻算法预测入住位置</p>
<p>数据集介绍：<br>本次实验的数据集来自于Kaggle与Facebook合作的机器学习竞赛，旨在通过由facebook提供的2000多万条数据信息预测一个人想要登记入住的地方。<br>数据来源：<a href="https://www.kaggle.com/c/facebook-v-predicting-check-ins/data" target="_blank" rel="noopener">https://www.kaggle.com/c/facebook-v-predicting-check-ins/data</a><br>其中：<br>train.csv</p>
<ul>
<li>row_id：登记事件的ID</li>
<li>xy：坐标</li>
<li>accuracy：定位准确性</li>
<li>time：时间戳</li>
<li>place_id：业务的ID，也是预测的目标值</li>
</ul>
<p>分析思路：</p>
<ul>
<li>数据集预处理<ul>
<li>数据量太 → 缩小坐标范围</li>
<li>时间数据 → 转化为年月日（增加新的时间特征，便于后续计算）</li>
<li>类别太多 → 按照指定条件进行转换</li>
</ul>
</li>
<li>分割数据及标准化<ul>
<li>选定”place_id”为目标值，数据集预处理结果为目标值。</li>
<li>使用StandardScaler类对数据进行<strong>标准化</strong>处理。</li>
</ul>
</li>
<li>KNN分类预测<ul>
<li>在分类预测过程中，使用超参数搜索API对模型进行选择和调优。</li>
</ul>
</li>
</ul>
<p>数据的原始内容为：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/KNN%E6%95%B0%E6%8D%AE%E6%A6%82%E8%A7%88.jpg" class="" title="KNN数据概览">
<br>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">knncls</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    k-近邻算法预测用户入住位置</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 读取数据</span></span><br><span class="line">    data = pd.read_csv(<span class="string">"./train.csv"</span>)</span><br><span class="line">    <span class="comment"># print(data.head(10))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 特征工程</span></span><br><span class="line">    <span class="comment"># 1.通过设定xy范围缩小数据</span></span><br><span class="line">    data = data.query(<span class="string">"x &gt; 1.0 &amp; x &lt; 1.25 &amp; y &gt; 2.5 &amp; y &lt; 2.75"</span>)  <span class="comment"># query相当于一个查询语句，参数为查询条件</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.处理时间戳数据</span></span><br><span class="line">    time_value = pd.to_datetime(data[<span class="string">'time'</span>], unit=<span class="string">'s'</span>)  <span class="comment"># 将时间戳转换为年-月-日 时-分-秒格式</span></span><br><span class="line">    <span class="comment"># print(time_value)</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    timevalue的值为：</span></span><br><span class="line"><span class="string">    0          1970-01-06 10:45:02</span></span><br><span class="line"><span class="string">    1          1970-01-03 03:49:15</span></span><br><span class="line"><span class="string">    2          1970-01-04 17:37:28</span></span><br><span class="line"><span class="string">    3          1970-01-09 03:43:07</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 把日期转换为字典格式</span></span><br><span class="line">    time_value = pd.DatetimeIndex(time_value)</span><br><span class="line">    <span class="comment"># 3.添加构造一些特征</span></span><br><span class="line">    data[<span class="string">'day'</span>] = time_value.day</span><br><span class="line">    data[<span class="string">'hour'</span>] = time_value.hour</span><br><span class="line">    data[<span class="string">'weekday'</span>] = time_value.weekday</span><br><span class="line">    <span class="comment"># 4.把时间戳特征删除,sklearn中axis为1表示列</span></span><br><span class="line">    data = data.drop([<span class="string">'time'</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.只保留入住人数大于3的目标位置,生成新的data</span></span><br><span class="line">    place_count = data.groupby(<span class="string">"place_id"</span>).count()  <span class="comment"># 按照'place_id'进行分组,然后统计次数。groupby使用参考：https://blog.csdn.net/m0_37870649/article/details/80979809</span></span><br><span class="line">    <span class="comment"># print(place_count)</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    此时print的place_count如下：</span></span><br><span class="line"><span class="string">    此时place_id为索引，而原来的特征值都变为了对应索引的place_id出现了多少次</span></span><br><span class="line"><span class="string">    place_id    row_id    x    y  accuracy  day  hour  weekday                                        </span></span><br><span class="line"><span class="string">    1000015801      78   78   78        78   78    78       78</span></span><br><span class="line"><span class="string">    1000017288      95   95   95        95   95    95       95</span></span><br><span class="line"><span class="string">    1000025138     563  563  563       563  563   563      563</span></span><br><span class="line"><span class="string">    1000052096     961  961  961       961  961   961      961</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    place_count_r = place_count[</span><br><span class="line">        place_count.row_id &gt; <span class="number">3</span>].reset_index()  <span class="comment"># 筛选place_count中row_id大于3的，然后通过reset_index()把place_id重新设为特征而不是索引</span></span><br><span class="line">    train_data = data[data[<span class="string">"place_id"</span>].isin(place_count_r[<span class="string">"place_id"</span>])]     <span class="comment">#通过对比筛选</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取特征值和目标值</span></span><br><span class="line">    x = train_data.drop([<span class="string">"place_id"</span>, <span class="string">"row_id"</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    y = train_data[<span class="string">"place_id"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分割数据集</span></span><br><span class="line">    <span class="comment">#参数：特征值，目标值，测试集比例。返回值依次为：训练集特征值、测试集特征值，训练集目标值，测试集目标值</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对训练集和测试集的特征值进行标准化</span></span><br><span class="line">    std = StandardScaler()</span><br><span class="line"></span><br><span class="line">    x_train = std.fit_transform(x_train)</span><br><span class="line">    x_test = std.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实例化knn估计器</span></span><br><span class="line">    knn = KNeighborsClassifier()    <span class="comment"># n_neighbors，默认5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#knn.fit(x_train, y_train)   #参数：训练集特征值、训练集目标值</span></span><br><span class="line">    <span class="comment"># 预测结果</span></span><br><span class="line">    <span class="comment">#y_predict = knn.predict(x_test) #参数：测试集</span></span><br><span class="line">    <span class="comment"># 打印准确率</span></span><br><span class="line">    <span class="comment">#print("准确率为:", knn.score(x_test, y_test))</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    准确率为: 0.4732860520094563</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进阶思考：</span></span><br><span class="line">    <span class="comment"># 交叉验证与网格搜索对K-近邻算法调优</span></span><br><span class="line">    param = &#123;<span class="string">"n_neighbors"</span>: [i * <span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">5</span>)]&#125;</span><br><span class="line">    gc = GridSearchCV(knn, param_grid=param, cv=<span class="number">5</span>)</span><br><span class="line">    gc.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"在测试集中的准确率："</span>, gc.score(x_test, y_test))</span><br><span class="line">    print(<span class="string">"在交叉验证中验证集的最好结果："</span>, gc.best_score_)</span><br><span class="line">    print(<span class="string">"使用的最好的模型："</span>, gc.best_estimator_)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    在测试集中的准确率： 0.47848699763593383</span></span><br><span class="line"><span class="string">    在交叉验证中验证集的最好结果： 0.47540983606557374</span></span><br><span class="line"><span class="string">    使用的最好的模型： KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',</span></span><br><span class="line"><span class="string">           metric_params=None, n_jobs=None, n_neighbors=8, p=2,</span></span><br><span class="line"><span class="string">           weights='uniform')</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    knncls()</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h1><blockquote>
<p>使用前提：特征之间独立，不互相影响</p>
</blockquote>
<p>该算法是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题</p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>条件概率中： P(A|B)表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。其基本求解公式为：P(A|B) = P(AB)/P(B)。<br>通常我们可以很容易直接得出P(A|B)，P(B|A)则很难直接得出，但我们更关心P(B|A)，贝叶斯定理就为我们打通从P(A|B)获得P(B|A)的道路。<br>贝叶斯公式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F.jpg" class="" title="贝叶斯公式">
<p>其中：</p>
<ul>
<li>P(类别)指每个文档类别的概率，计算方式 (某文档类别数 / 总文档数量)</li>
<li>P(特征|类别) = 指给定类别下特征的概率，计算方式：P(F1|C) = Ni/N<ul>
<li>Ni为该F1特征词在C类别所有文档中出现的次数</li>
<li>N为所属类别C下的文档所有词出现的次数和</li>
</ul>
</li>
<li>在比较一个文章属于不同文章类型的概率时，会发现分母P(特征)是相同的，所以可以省略掉</li>
</ul>
<p><strong>拉普拉斯平滑：</strong><br>如果词频列表里面有很多出现次数都为0的词，在计算的时候可能使计算结果都变为0，这是不合理的，解决方式就是增加拉普拉斯修正</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91%E7%B3%BB%E6%95%B0.jpg" class="" title="拉普拉斯平滑系数">
<p>其中，α为指定的系数，一般为1，m为训练文档中统计出的特征词个数。</p>
<p>朴素贝叶斯分类的流程可以由下图表示：</p>


<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>优点：</p>
<ul>
<li>算法逻辑简单,易于实现</li>
<li>分类过程中时空开销小</li>
<li>对缺失数据不太敏感</li>
<li>对小规模的数据表现很好，能处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练。</li>
</ul>
<p>缺点：<br>理论上，<strong>朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好</strong>。</p>
<h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><p>一个简单示例如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%A4%BA%E4%BE%8B.jpg" class="" title="贝叶斯示例">
<p>可以看到由于云计算在娱乐文章里出现的频率是0，所以导致最后结果是0，这里需要进行拉普拉斯平滑处理。<br>即将P(影院，支付宝，云计算|娱乐)计算时的 分子+1，分母+4，</p>
<h4 id="API与实战demo-1"><a href="#API与实战demo-1" class="headerlink" title="API与实战demo"></a>API与实战demo</h4><p>pythonAPI：sklearn.naive_bayes.MultinomialNB(alpha = 1.0)</p>
<ul>
<li>alpha：拉普拉斯平滑系数，默认1</li>
</ul>
<p>此处的实例用的还是之前内置的sklearn20类新闻分类的数据，20个新闻组数据包含了20个主题的18000个新闻组帖子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naviebayes</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    朴素贝叶斯对新闻进行分类</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    news = fetch_20newsgroups(subset=<span class="string">'all'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行分割</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对数据集进行特征抽取,用tf-idf方法</span></span><br><span class="line">    tf = TfidfVectorizer()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 以训练集中的词的列表进行每篇文章重要性统计</span></span><br><span class="line">    x_train = tf.fit_transform(x_train)  <span class="comment"># 此时x_train为sparse矩阵，大概内容就是记录着:(行，列) 对应数值</span></span><br><span class="line">    print(tf.get_feature_names())</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    特征名，也就是每个单词</span></span><br><span class="line"><span class="string">    'davidst', 'davidstern', 'davidw', 'davie', 'daviel', 'davies', 'davinci', 'davis', 'davison',</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    x_test = tf.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行朴素贝叶斯算法的预测</span></span><br><span class="line">    mlt = MultinomialNB(alpha=<span class="number">1.0</span>)</span><br><span class="line">    print(x_train.toarray())</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    大多数是0，因为一篇文章不是所有的词都有，每一行代表一篇文章，列就是上面的特征名，即单词，行列对应值为TF-IDF值</span></span><br><span class="line"><span class="string">    [[0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="string">    [0. 0. 0. ... 0. 0. 0.]</span></span><br><span class="line"><span class="string">    ..................</span></span><br><span class="line"><span class="string">    [0. 0. 0. ... 0. 0. 0.]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    mlt.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">    y_predict = mlt.predict(x_test)</span><br><span class="line">    print(<span class="string">"预测的文章类别为："</span>, y_predict)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    预测的文章类别为： [ 7 16  0 ...  4  8  1]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得出准确率</span></span><br><span class="line">    print(<span class="string">"准确率为："</span>, mlt.score(x_test, y_test))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    准确率为： 0.8293718166383701</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    naviebayes()</span><br></pre></td></tr></table></figure>


<h5 id="番外篇，R语言与朴素贝叶斯"><a href="#番外篇，R语言与朴素贝叶斯" class="headerlink" title="番外篇，R语言与朴素贝叶斯"></a>番外篇，R语言与朴素贝叶斯</h5><blockquote>
<p>R语言中的klaR包提供了朴素贝叶斯算法实现的函数NaiveBayes<br>NaiveBayes(formula, data, …, subset, na.action= na.pass)<br>NaiveBayes(x, grouping, prior, usekernel= FALSE, fL = 0, …)</p>
</blockquote>
<ul>
<li>formula指定参与模型计算的变量，以公式形式给出，类似于y=x1+x2+x3；</li>
<li>data用于指定需要分析的数据对象；</li>
<li>na.action指定缺失值的处理方法，默认情况下不将缺失值纳入模型计算，也不会发生报错信息，当设为“na.omit”时则会删除含有缺失值的样本；</li>
<li>x指定需要处理的数据，可以是数据框形式，也可以是矩阵形式；</li>
<li>grouping为每个观测样本指定所属类别；</li>
<li>prior可为各个类别指定先验概率，默认情况下用各个类别的样本比例作为先验概率；</li>
<li>usekernel指定密度估计的方法（在无法判断数据的分布时，采用密度密度估计方法），默认情况下使用正态分布密度估计，设为TRUE时，则使用核密度估计方法；</li>
<li>fL指定是否进行拉普拉斯修正，默认情况下不对数据进行修正，当数据量较小时，可以设置该参数为1，即进行拉普拉斯修正。</li>
</ul>
<h1 id="决策树-Decision-Tree-和随机森林-Random-Forest"><a href="#决策树-Decision-Tree-和随机森林-Random-Forest" class="headerlink" title="决策树(Decision Tree)和随机森林(Random Forest)"></a>决策树(Decision Tree)和随机森林(Random Forest)</h1><ul>
<li>决策树是用树的结构来构建分类模型，每个节点代表着一个属性，根据这个属性的划分，进入这个节点的儿子节点，直至叶子节点，每个叶子节点都表征着一定的类别，从而达到分类的目的。<ul>
<li>常用的决策树有ID4，C4.5，CART等。在生成树的过程中，需要选择用那个特征进行剖分，一般来说，选取的原则是，分开后能尽可能地提升纯度，可以用信息增益，增益率，以及基尼系数等指标来衡量。如果是一棵树的话，为了避免过拟合，还要进行剪枝（prunning），取消那些可能会导致验证集误差上升的节点。</li>
</ul>
</li>
<li>随机森林实际上是一种特殊的bagging方法，它将决策树用作bagging中的模型。首先，用bootstrap方法生成m个训练集，然后，对于每个训练集，构造一颗决策树，在节点找特征进行分裂的时候，并不是对所有特征找到能使得指标（如信息增益）最大的，而是在特征中随机抽取一部分特征，在抽到的特征中间找到最优解，应用于节点，进行分裂。随机森林的方法由于有了bagging，也就是集成的思想在，实际上相当于对于样本和特征都进行了采样（如果把训练数据看成矩阵，就像实际中常见的那样，那么就是一个行和列都进行采样的过程），所以可以避免过拟合。</li>
</ul>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h4 id="信息论基础"><a href="#信息论基础" class="headerlink" title="信息论基础"></a>信息论基础</h4><p>在信息论中，熵（英语：entropy）是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。<br>熵最好理解为不确定性的量度而不是确定性的量度，因为<strong>越随机的信源的熵越大</strong>。</p>
<p><strong>香农的信息熵理论：</strong><br>如果n个事件发生的概率分别为p1,p2,p3….pn则他们的准确信息量应该是：<br>H = -(p1Logp1 + p2logp2 + p3logp3 + … + pnlogpn)<br>H的专业术语为信息熵，单位为比特，公式：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B5%E5%85%AC%E5%BC%8F.jpg" class="" title="信息熵公式">

<p>例如，当有32支球队比赛时，猜测谁是冠军，在没有任何信息的情况下，每个球队获胜的概率都是 1/32，此时信息熵为<br>H = -(1/32Log1/32 + 1/32log1/32 + 1/32log1/32 + … + 1/32log1/32) = 5<br>而当我们得到一些信息的情况下，比如不同球队的获胜几率为多少，那我们最终得出的信息熵H将会小于5。</p>
<p><strong>信息增益：</strong><br>当得知一个特征条件之后，减少的信息熵的大小。</p>
<h4 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h4><p><strong>决策树划分依据</strong></p>
<ol>
<li>ID3：信息增益最大<br>特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与 特征A给出条件下D的信息条件熵H(D|A) 之差，即公式:<br>g(D,A) = H(D) - H(D|A)<br>此处信息增益表示，得知特征A的信息而使得类D的信息的不确定性减少的程度，计算H(D)和H(D|A)公式：<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B5%E5%85%AC%E5%BC%8F2.jpg" class="" title="信息熵公式2">

</li>
</ol>
<p>上面这公式有点复杂，举个例子：<br>有如下银行贷款的数据，通过前面的特征来决定是否可以贷款。</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E8%B4%B7%E6%AC%BE%E6%95%B0%E6%8D%AE.jpg" class="" title="贷款数据">
<p>对于没有加入特征之前，只看是否贷款数据的信息熵：一共有15条数据，9条可以贷款，6条不可以。所以此时总的信息熵为：0.971，计算方式如下</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B51.jpg" class="" title="信息熵1">
<p>然后我们让A1，A2,A3,A4分别表示年龄，工作，有自己房子和信贷情况四个特征，则分别计算他们的信息增益即可选择下一次决策用什么特征。<br>此处以年龄为例，而年龄中青年、中年、老年各占5个，即 1/3，根据公式，年龄的信息增益计算为：<br>g(D,A1) = H(D) - H(D’|年龄) = 0.971 - [1/3H(青年) + 1/3H(中年)+ 1/3H(老年)]，<br>此时需要计算H(青年)针对类别的信息熵，由于5个青年中，有3个类别是‘否’，2个对应类别是‘是’，所以计算方式如下：<br>H(青年)= -(2/log2/5 + 3/5log3/5)<br>中年和老年的计算方式类似，最终结果为：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E4%BF%A1%E6%81%AF%E7%86%B52.jpg" class="" title="信息熵2">
<p>同理，其他的特征信息增益也可以计算出来，g(D,A2)=0.324,g(D,A3)= 0.420,g(D,A4)= 0.363，相比较A3所对应的有自己的房子这个特征的信息增益最大，所以特征A3为最有特征的一个属性。</p>
<ol start="2">
<li>C4.5：信息增益比最大</li>
<li>CART：回归树<ul>
<li>回归树：平方误差最小</li>
<li>分类树：基尼系数（Gini）最小，基尼系数的划分更加细致</li>
</ul>
</li>
</ol>
<h4 id="决策树优缺点"><a href="#决策树优缺点" class="headerlink" title="决策树优缺点"></a>决策树优缺点</h4><p>优点：</p>
<ul>
<li>简单的理解和解释，树木可视化</li>
<li>不需要太多的数据准备操作，其他技术通常需要数据归一化等操作</li>
</ul>
<p>缺点：</p>
<ul>
<li>决策树可能过于复杂，而且训练集中正确率高不代表测试集中正确率高，有些异常点会导致结果有误。过拟合</li>
</ul>
<p>改进：</p>
<ul>
<li>剪枝cart算法</li>
<li>随机森林</li>
</ul>
<h4 id="经典案例：泰坦尼克号乘幸存预测"><a href="#经典案例：泰坦尼克号乘幸存预测" class="headerlink" title="经典案例：泰坦尼克号乘幸存预测"></a>经典案例：泰坦尼克号乘幸存预测</h4><blockquote>
<p>决策树分类器API：sklearn.tree.DecisionTreeClassifier(criterion=’gini’,max_depth=None,random_state=None)</p>
</blockquote>
<ul>
<li>criterion:默认是‘gini’系数，也可以选择信息增益的熵’entropy’</li>
<li>max_depth:树的深度大小</li>
<li>random_state:随机数种子<br>method：</li>
<li>decision_path:返回决策树的路径</li>
</ul>
<blockquote>
<p>决策树的结构，本地保存API：sklearn.tree.export_graphviz(extimator,outfile=”tree.dot”,feature_names=[‘’,’’])</p>
</blockquote>
<ul>
<li>extimator：实例化的估计器对象</li>
<li>outfile：保存路径和文件，必须是后缀dot格式，这格式课方便的转换为pdf，png等格式</li>
<li>feature_names:特征名，可自己指定</li>
</ul>
<ul>
<li>用软件graphviz运行命令：dot -Tpng tree.dot -o tree.png 来查看，tree.png改为tree.pdf则转换为pdf</li>
</ul>
<p>案例数据：<a href="http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt" target="_blank" rel="noopener">http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt</a><br>数据中包含的特征：票的类别，存活结果，乘坐版（pclass），年龄，登陆，home.dest，房间，票，船和性别等。其中乘坐班是指乘客班(1,2,3),是社会经济阶层的代表。其中age数据还存在缺失需要处理。<br>处理流程：</p>
<ul>
<li>pd读取数据</li>
<li>选择有影响的特征，处理缺失值</li>
<li>进行特征工程，pd转换字典，特征抽取x_train.to_dict(orient=”recored”)</li>
<li>决策树估计器流程</li>
</ul>
<p>案例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier,export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decision</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    决策树对泰坦尼克号乘客进行预测生死</span></span><br><span class="line"><span class="string">    :return:None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    titan = pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理数据，找出特征值和目标值,这里以三个为例</span></span><br><span class="line">    x = titan[[<span class="string">'pclass'</span>, <span class="string">'age'</span>, <span class="string">'sex'</span>]]</span><br><span class="line">    print(x)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">         pclass      age     sex</span></span><br><span class="line"><span class="string">    0       1st  29.0000  female</span></span><br><span class="line"><span class="string">    1       1st   2.0000  female</span></span><br><span class="line"><span class="string">    2       1st  30.0000    male</span></span><br><span class="line"><span class="string">    3       1st  25.0000  female</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    y = titan[<span class="string">'survived'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 缺失值处理</span></span><br><span class="line">    x[<span class="string">'age'</span>].fillna(x[<span class="string">'age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分割数据集到训练集和测试集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行特征工程，当特征是类别的时候，要进行ont-hot编码</span></span><br><span class="line">    dict = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    x_train = dict.fit_transform(x_train.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">    print(dict.get_feature_names())</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(x_train)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    刚好与上面对应</span></span><br><span class="line"><span class="string">    [[29.         1.          0.          0.          1.          0.        ]</span></span><br><span class="line"><span class="string">    [ 2.          1.          0.          0.          1.          0.        ]</span></span><br><span class="line"><span class="string">    [30.          1.          0.          0.          0.          1.        ]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x_test = dict.transform(x_test.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用决策树估计器进行预测</span></span><br><span class="line">    dec = DecisionTreeClassifier()  <span class="comment"># max_depth是个超参数，值可以影响结果，也决定了决策树的深度</span></span><br><span class="line">    dec.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#预测准确率</span></span><br><span class="line">    print(<span class="string">"DecisionTreeClassifier无参数时，预测的准确率为："</span>,dec.score(x_test,y_test))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    DecisionTreeClassifier无参数时，预测的准确率为： 0.8206686930091185</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#导出决策树的结构，截图在之后给出</span></span><br><span class="line">    export_graphviz(dec,out_file=<span class="string">"./tree.dot"</span>,feature_names=[<span class="string">'age'</span>, <span class="string">'pclass=1st'</span>, <span class="string">'pclass=2nd'</span>, <span class="string">'pclass=3rd'</span>, <span class="string">'sex=女性'</span>, <span class="string">'sex=男性'</span>])</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    decision()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上述代码保存后的决策树结构图</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%86%B3%E7%AD%96%E6%A0%91%E7%BB%93%E6%9E%9C%E5%9B%BE.jpg" class="" title="决策树结果图">


<h2 id="随机森林-集成学习方法"><a href="#随机森林-集成学习方法" class="headerlink" title="随机森林-集成学习方法"></a>随机森林-集成学习方法</h2><h4 id="集成学习方法"><a href="#集成学习方法" class="headerlink" title="集成学习方法"></a>集成学习方法</h4><p>集成学习（Ensemble learning）通过组合几种模型来提高机器学习的效果。与单一模型相比，该方法可以提供更好的预测结果。<br>集成学习有一个重要的概念叫<strong>Diversity</strong>，也就是说每个分类器要长得尽量不一样。就像一个团队，有的人擅长策划，有人擅长拉赞助，有人擅长执行，这样才是一个牛逼的团队。集成学习中每个分类器也要有一定差异，这样才是一个好的集成。</p>
<p>集成学习算法主要有Boosting和Bagging两种类型：</p>
<ul>
<li>Boosting：通过迭代地训练一系列的分类器，<strong>每个分类器采用的样本的选择方式都和上一轮的学习结果有关</strong>。例如在AdaBoost中，之前分类错误的样本有较高的可能性被选到，在之前分类正确的样本有较小的概率被选到。就像你在背单词，你今天总会选择前面几天背不下来的单词，Boosting也会选择前面轮数没学下来的样本。这个类别下面的主要算法是AdaBoost和GBDT。</li>
<li>Bagging：每个分类器都随机从原样本中做<strong>有放回的采样</strong>，然后分别在这些采样后的样本上训练分类器，然后再把这些分类器组合起来。简单的多数投票一般就可以。随机森林就是这个类别的一个经典算法，仔细学习你会发现它的每棵树和基本的决策树的不同，非常神奇的算法。</li>
</ul>
<p>额外拓展：集成方法是将几种机器学习技术组合成一个预测模型的元算法，以达到减小方差（bagging）、偏差（boosting）或改进预测（stacking）的效果。</p>
<h4 id="随机森林的定义，过程，优势"><a href="#随机森林的定义，过程，优势" class="headerlink" title="随机森林的定义，过程，优势"></a>随机森林的定义，过程，优势</h4><p>定义：在机器学习中，随机森林是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。</p>
<p>过程：</p>
<ol>
<li>单个树的建立过程：假如有N个样本，M个特征，随机有放回的抽样，即:Bagging:Bootstrap Aggregating<ul>
<li>随机在N个样本中有放回的选择一个样本，重复N次，因为有放回所以可能有重复</li>
<li>随机在M个特征当中选出m个特征<ul>
<li>m的取值：m&lt;&lt;M</li>
</ul>
</li>
</ul>
</li>
<li>重复1过程，建立多颗决策树，他们的样本和特征大多不一样。 </li>
</ol>
<p>优势：</p>
<ul>
<li>当前所有算法中，具有极好的准确率</li>
<li>能够有效地运行在大数据集上</li>
<li>能够处理具有高维特征的输入样本，而且不需要降维</li>
<li>能够评估各个特征在分类问题上的重要性</li>
</ul>
<h4 id="随机森林与泰坦尼克号乘客幸存预测"><a href="#随机森林与泰坦尼克号乘客幸存预测" class="headerlink" title="随机森林与泰坦尼克号乘客幸存预测"></a>随机森林与泰坦尼克号乘客幸存预测</h4><blockquote>
<p>随机森林API:class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)<br>列举几个重要参数，参数实在太多，超参数也有很多了，需要的时候再搜索好了：</p>
</blockquote>
<ul>
<li>n_estimators : integer，可选default=10,森林里（决策）树的数目</li>
<li>criterion : string, 可选(默认值为“gini”)，衡量分裂质量的性能（函数）。 受支持的标准是基尼不纯度的”gini”,和信息增益的”entropy”（熵）</li>
<li>max_features： int, float, string or None,可选default=”auto”，寻找最佳分割时需要考虑的特征数目，不同的值代表不同的考虑方式：这里n_features为总的特征数<ul>
<li>如果是int，就要考虑每一次分割处的max_feature特征</li>
<li>如果是float，那么max_features就是一个百分比，那么（max_feature*n_features）特征整数值是在每个分割处考虑的</li>
<li>如果是auto，那么max_features=sqrt(n_features)，即n_features的平方根值。</li>
<li>如果是log2，那么max_features=log2(n_features)</li>
<li>如果是None,那么max_features=n_features</li>
</ul>
</li>
<li>max_depth : integer or None, 可选的（默认为None），（决策）树的最大深度。如果值为None，那么会扩展节点，直到所有的叶子是纯净的，或者直到所有叶子包含少于min_sample_split的样本。</li>
<li>bootstrap : boolean, 可选的(default=True)，建立决策树时，是否使用有放回抽样。</li>
</ul>
<p>代码示例：处理数据与之前的决策树一样，只是把决策树估计器改成了随机森林，然后进行了网格搜索与交叉验证</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decision</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    决策树对泰坦尼克号乘客进行预测生死</span></span><br><span class="line"><span class="string">    :return:None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    titan = pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理数据，找出特征值和目标值,这里以三个为例</span></span><br><span class="line">    x = titan[[<span class="string">'pclass'</span>, <span class="string">'age'</span>, <span class="string">'sex'</span>]]</span><br><span class="line">    print(x)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">         pclass      age     sex</span></span><br><span class="line"><span class="string">    0       1st  29.0000  female</span></span><br><span class="line"><span class="string">    1       1st   2.0000  female</span></span><br><span class="line"><span class="string">    2       1st  30.0000    male</span></span><br><span class="line"><span class="string">    3       1st  25.0000  female</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    y = titan[<span class="string">'survived'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 缺失值处理</span></span><br><span class="line">    x[<span class="string">'age'</span>].fillna(x[<span class="string">'age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分割数据集到训练集和测试集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行特征工程，当特征是类别的时候，要进行ont-hot编码</span></span><br><span class="line">    dict = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    x_train = dict.fit_transform(x_train.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">    print(dict.get_feature_names())</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    ['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(x_train)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    刚好与上面对应</span></span><br><span class="line"><span class="string">    [[29.         1.          0.          0.          1.          0.        ]</span></span><br><span class="line"><span class="string">    [ 2.          1.          0.          0.          1.          0.        ]</span></span><br><span class="line"><span class="string">    [30.          1.          0.          0.          0.          1.        ]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    x_test = dict.transform(x_test.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机森林估计器进行预测，超参数调优(这里以两个参数为例）</span></span><br><span class="line">    rf = RandomForestClassifier()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#网格搜索与交叉验证寻找最佳参数</span></span><br><span class="line">    param = &#123;<span class="string">"n_estimators"</span>:[<span class="number">10</span>,<span class="number">20</span>,<span class="number">100</span>,<span class="number">120</span>,<span class="number">200</span>,<span class="number">300</span>,<span class="number">500</span>,<span class="number">800</span>],<span class="string">"max_depth"</span>:[<span class="number">5</span>,<span class="number">8</span>,<span class="number">15</span>,<span class="number">20</span>,<span class="number">30</span>]&#125;</span><br><span class="line">    gc = GridSearchCV(rf,param_grid=param,cv=<span class="number">2</span>)</span><br><span class="line">    gc.fit(x_train,y_train)</span><br><span class="line">    print(<span class="string">"准确率为："</span>,gc.score(x_test,y_test))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    准确率为： 0.8267477203647416</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"选择的参数模型："</span>,gc.best_params_)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    选择的参数模型： &#123;'max_depth': 5, 'n_estimators': 10&#125;</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    decision()</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="线性回归，迭代的算法"><a href="#线性回归，迭代的算法" class="headerlink" title="线性回归，迭代的算法"></a>线性回归，迭代的算法</h1><blockquote>
<p>回归问题：目标值是连续的，如房价，营业额等。</p>
</blockquote>
<h4 id="定义与原理"><a href="#定义与原理" class="headerlink" title="定义与原理"></a>定义与原理</h4><p>线性关系模型：<br>一个通过属性的线性组合来进行预测的函数:</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB%E6%A8%A1%E5%9E%8B.jpg" class="" title="线性关系模型">
<p>w为权重，b称为偏置项，可理解为w0*1</p>
<p>线性回归定义：</p>
<blockquote>
<p>在统计学中，线性回归（Linear regression）是利用称为线性回归方程的<a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95" target="_blank" rel="noopener">最小二乘</a> 函数对一个或多个<strong>自变量和因变量之间关系进行建模</strong>的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。</p>
</blockquote>
<p>通用公式：<br>x为特征值，w为要求的值</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%80%9A%E7%94%A8%E5%85%AC%E5%BC%8F.jpg" class="" title="线性回归通用公式">


<h4 id="线性回归的损失函数：平方损失函数（最小二乘法-Ordinary-Least-Squares-）"><a href="#线性回归的损失函数：平方损失函数（最小二乘法-Ordinary-Least-Squares-）" class="headerlink" title="线性回归的损失函数：平方损失函数（最小二乘法, Ordinary Least Squares ）"></a>线性回归的损失函数：平方损失函数（最小二乘法, Ordinary Least Squares ）</h4><p>最小二乘法是线性回归的一种，OLS将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从<a href="https://zh.wikipedia.org/wiki/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83" target="_blank" rel="noopener">高斯分布</a>（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：<strong>最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小</strong>。换言之，OLS是基于距离的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：</p>
<ul>
<li>简单，计算方便</li>
<li>欧氏距离是一种很好的相似性度量标准</li>
<li>在不同的表示域变换后特征性质不变</li>
</ul>
<p>下面的式子，f(x)：模型的预测值。Y：真实值<br>平方损失（Square loss）的标准形式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1%E7%9A%84%E6%A0%87%E5%87%86%E5%BD%A2%E5%BC%8F.jpg" class="" title="平方损失的标准形式">

<p>当样本个数为n时，此时的损失函数变为：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%A0%B7%E6%9C%AC%E6%95%B0%E4%B8%BAn%E6%97%B6%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.jpg" class="" title="样本数为n时的损失函数">

<p>Y-f(X)表示的是残差，整个式子表示的是残差的平方和，而我们的目的就是<strong>最小化这个目标函数值</strong>（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。</p>
<p><strong>而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标</strong>，公式如下:假设有n个数据，后面的减法为 预测值-真实值</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%9D%87%E6%96%B9%E5%B7%AE.jpg" class="" title="均方差">

<p>我们通常说的线性有两种情况，一种是因变量y是自变量x的线性函数，一种是因变量y是参数α的线性函数。在机器学习中，通常指的都是后一种情况。</p>
<h4 id="两种求解方法"><a href="#两种求解方法" class="headerlink" title="两种求解方法"></a>两种求解方法</h4><p>从前面得知，线性回归要求每个特征对应的权重w值，使得线性回归的损失函数值最小，有两种方法：</p>
<ol>
<li>最小二乘法之正规方程（不常用）<br>X为特征值矩阵，y为目标值矩阵，求解方程如下：</li>
</ol>
<p>缺点：当特征过于复杂，求解速度太慢</p>
<ol start="2">
<li>最小二乘法之梯度下降SGD（理解过程）<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.jpg" class="" title="梯度下降">
线性回归中一直在迭代，刚开始是原始值减去后面的学习速率*方向，得到一个新的值，下次迭代用这个新的值继续减去 学习速率*方向。<br>使用：面对训练数据规模十分庞大的任务</li>
</ol>
<p>正规方程与梯度下降对比：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%AF%B9%E6%AF%94.jpg" class="" title="对比">


<h4 id="API与案例"><a href="#API与案例" class="headerlink" title="API与案例"></a>API与案例</h4><blockquote>
<p>sklearn.linear_model.LinearRegression()</p>
</blockquote>
<ul>
<li>普通最小二乘线性回归</li>
<li>coef_：求出的回归系数</li>
</ul>
<blockquote>
<p>sklearn.linear_model.SGDRegressor()</p>
</blockquote>
<ul>
<li>通过使用SGD最小化线性模型</li>
<li>coef_:求出的回归系数</li>
</ul>
<p>均方差算回归损失的API：</p>
<blockquote>
<p>sklearn.metrics.mean_squared_error(y_true,y_pred)</p>
</blockquote>
<ul>
<li>y_true:真实值</li>
<li>y_pred:预测值</li>
<li>return：浮点数结果</li>
<li>真实值和预测值为标准化之前的值</li>
</ul>
<p>案例：波士顿房价预测<br>流程：</p>
<ol>
<li>数据获取</li>
<li>数据分割</li>
<li>训练与测试数据<strong>标准化处理</strong></li>
<li>使用最简单的线性回归模型LinearRegression和SGDRegressor对房价进行预测</li>
</ol>
<p>代码案例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, SGDRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mylinear</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    线性回归预测房子价格</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    lb = load_boston()</span><br><span class="line">    <span class="comment"># 分割数据集到训练集和测试集</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(lb.data, lb.target, test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行标准化</span></span><br><span class="line">    <span class="comment"># 这里特征值和目标值都要进行</span></span><br><span class="line">    std_x = StandardScaler()  <span class="comment"># 对特征值进行标准化</span></span><br><span class="line">    x_train = std_x.fit_transform(x_train)</span><br><span class="line">    x_test = std_x.transform(x_test)</span><br><span class="line"></span><br><span class="line">    std_y = StandardScaler()  <span class="comment"># 对目标值进行标准化</span></span><br><span class="line">    y_train = std_y.fit_transform(y_train.reshape(<span class="number">-1</span>, <span class="number">1</span>))  <span class="comment"># 要求是二维</span></span><br><span class="line">    y_test = std_y.fit_transform(y_test.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># estimator预测</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 正规方程求解方式预测</span></span><br><span class="line">    lr = LinearRegression()</span><br><span class="line">    lr.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"回归系数为："</span>, lr.coef_)</span><br><span class="line">    <span class="string">''''</span></span><br><span class="line"><span class="string">    回归系数为： [[-0.13482789  0.14684301  0.01549439  0.03522313 -0.21053828  0.32880086</span></span><br><span class="line"><span class="string">    -0.00599198 -0.39363529  0.25896495 -0.2026098  -0.21658332  0.09938895</span></span><br><span class="line"><span class="string">    -0.38005182]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 预测测试集的房子价格</span></span><br><span class="line">    y_lr_predict = std_y.inverse_transform(lr.predict(x_test))  <span class="comment"># 此时的值还是标准化后的,所以要inverse</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"测试集里每个房子的预测价格："</span>, y_lr_predict)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    测试集里每个房子的预测价格： [[-5.23859608]</span></span><br><span class="line"><span class="string">    [39.61950876]</span></span><br><span class="line"><span class="string">    [35.86414606]</span></span><br><span class="line"><span class="string">    ......</span></span><br><span class="line"><span class="string">    [20.31808398]</span></span><br><span class="line"><span class="string">    [17.36664138]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"正规方程的均方差："</span>, mean_squared_error(std_y.inverse_transform(y_test), y_lr_predict))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    正规方程的均方差： 24.203056713653485</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 梯度下降进行房价预测</span></span><br><span class="line">    sgd = SGDRegressor()  <span class="comment"># 这里可以指定学习率</span></span><br><span class="line">    sgd.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"SGD预测回归系数为："</span>, sgd.coef_)</span><br><span class="line">    <span class="string">''''</span></span><br><span class="line"><span class="string">    SGD预测回归系数为： [-0.08189344  0.07032661 -0.03413881  0.10527339 -0.09947185  0.33781714</span></span><br><span class="line"><span class="string">    -0.04008106 -0.21279579  0.06362954 -0.05064323 -0.19061834  0.10570017</span></span><br><span class="line"><span class="string">    -0.36071628]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 预测测试集的房子价格</span></span><br><span class="line">    y_sgd_predict = std_y.inverse_transform(sgd.predict(x_test))  <span class="comment"># 此时的值还是标准化后的,所以要inverse</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"测试集里每个房子的预测价格："</span>, y_sgd_predict)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    测试集里每个房子的预测价格：[22.25491159 30.38773853 12.72306638 35.68389966 29.69842243 21.70338892</span></span><br><span class="line"><span class="string">    15.23925266 27.85422604 24.72436984 31.05806439 16.91816538 28.40787834</span></span><br><span class="line"><span class="string">    ......</span></span><br><span class="line"><span class="string">    21.3051416  23.92421059 20.83558394 23.80267794 28.17431605 32.1886237</span></span><br><span class="line"><span class="string">    25.89847536]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"梯度下降的均方差："</span>, mean_squared_error(std_y.inverse_transform(y_test), y_sgd_predict))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    梯度下降的均方差： 24.303529166136546</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    mylinear()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="岭回归解决过拟合"><a href="#岭回归解决过拟合" class="headerlink" title="岭回归解决过拟合"></a>岭回归解决过拟合</h2><p>线性回归LinearRegression 容易出现过拟合，为了把训练集数据表现更好<br>而岭回归是带有正则化的线性回归</p>
<p>线性回归与岭回归Ridge对比：<br>岭回归得到的<strong>回归系数更符合实际，更可靠</strong>。另外，能让估计参数的波动范围变小，变的稳定。在存在病态数据偏多的研究中有较大的实用价值。</p>
<blockquote>
<p>具有L2正则化的线性最小二乘法：sklearn.linear_model.Ridge(alpha=1.0)</p>
</blockquote>
<ul>
<li>alpha：正则化力度</li>
<li>coef_:求解的回归系数w</li>
</ul>
<p>上面的案例用岭回归解决：</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 岭回归进行房价预测</span><br><span class="line">rd &#x3D; Ridge(alpha&#x3D;1.0)  # 这里可以指定学习率</span><br><span class="line">rd.fit(x_train, y_train)</span><br><span class="line">print(&quot;岭回归预测回归系数为：&quot;, rd.coef_)</span><br><span class="line">&#39;&#39;&#39;&#39;</span><br><span class="line">岭回归预测回归系数为： [[-0.07357443  0.11581584  0.0248411   0.06555144 -0.17895055  0.27716807</span><br><span class="line">-0.01529063 -0.32589572  0.24111193 -0.19075882 -0.21855738  0.09673899</span><br><span class="line">-0.44673051]]</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"># 预测测试集的房子价格</span><br><span class="line">y_rd_predict &#x3D; std_y.inverse_transform(rd.predict(x_test))  # 此时的值还是标准化后的,所以要inverse</span><br><span class="line"></span><br><span class="line">print(&quot;测试集里每个房子的预测价格：&quot;, y_rd_predict)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">测试集里每个房子的预测价格： [[33.05501377]</span><br><span class="line">[29.49658894]</span><br><span class="line">......</span><br><span class="line">[19.62373227]</span><br><span class="line">[12.41679022]]</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">print(&quot;岭回归的均方差：&quot;, mean_squared_error(std_y.inverse_transform(y_test), y_sgd_predict))</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">岭回归的均方差： 27.097530207872403</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>逻辑回归的输入就是线性回归的式子，而且也是自我学习的过程，要迭代回归<br>逻辑回归可以做什么，适应场景：二分类问题</p>
<ul>
<li>广告是否会被点击</li>
<li>是否为垃圾邮件</li>
<li>是否患病</li>
<li>金融诈骗</li>
</ul>
<p>逻辑回归的输入：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%BE%93%E5%85%A5.jpg" class="" title="逻辑回归输入">

<h4 id="过程：如何从线性回归的输入变为逻辑回归的分类"><a href="#过程：如何从线性回归的输入变为逻辑回归的分类" class="headerlink" title="过程：如何从线性回归的输入变为逻辑回归的分类"></a>过程：如何从线性回归的输入变为逻辑回归的分类</h4><p>先了解一下什么事sigmoid函数：将输入转换为0~1，与y轴交叉点默认为0.5</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/sigmoid%E5%87%BD%E6%95%B0.jpg" class="" title="sigmoid函数">
<ul>
<li>横坐标是一个具体的值</li>
<li>输出纵坐标是0~1之间的值，可看做概率值</li>
</ul>
<p>而sigmoid的公式和逻辑回归的公式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%85%AC%E5%BC%8F.jpg" class="" title="逻辑回归公式">
<ul>
<li>e:常数2.71</li>
<li>Z：回归的结果</li>
</ul>
<h4 id="逻辑回归的损失函数：log对数似然损失函数"><a href="#逻辑回归的损失函数：log对数似然损失函数" class="headerlink" title="逻辑回归的损失函数：log对数似然损失函数"></a>逻辑回归的损失函数：log对数似然损失函数</h4><p>有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即max F(y, f(x)) —&gt; min -F(y, f(x)))。从损失函数的视角来看，它就成了log损失函数了。</p>
<p>损失函数公式如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E5%AF%B9%E6%95%B0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.jpg" class="" title="对数损失函数">

<p>当目标值是1，此时损失值与预测结果为1的概率（x轴）的关系如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%9B%AE%E6%A0%87%E5%80%BC%E4%B8%BA1.jpg" class="" title="目标值为1">

<p>当目标值是0，此时损失值与预测结果为0的概率（x轴）的关系如下：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/%E7%9B%AE%E6%A0%87%E5%80%BC%E6%98%AF0.jpg" class="" title="目标值是0">


<h4 id="API与案例："><a href="#API与案例：" class="headerlink" title="API与案例："></a>API与案例：</h4><blockquote>
<p>逻辑回归API：sklearn.linear_model.LogisticRegression(penalty=’l2’,C=1.0)</p>
</blockquote>
<ul>
<li>penalty:正则化方式</li>
<li>c:正则化粒度</li>
</ul>
<p>案例：预测癌症肿瘤（乳腺癌）<br>数据地址：<a href="http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data" target="_blank" rel="noopener">http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data</a><br>数据内容：</p>
<ul>
<li>每个案例11列数据，第一列是检索Id，后9列是与病情相关的医学特征，最后1列表示肿瘤类型的数值</li>
<li>肿瘤类型数值：2为良性，占65.5%，4为恶性，占34.5%<ul>
<li>注：逻辑回归中，因为针对的是2分类型，所以只有两个结果，只用计算一个概率值来表示属于某一种类型的概率，另一个为(1-概率),而这个概率值一般是数据中占比比较少的那一个类型，比如这里的恶性</li>
</ul>
</li>
<li>其中包含几个缺失值，用“？”标出</li>
</ul>
<p>流程：</p>
<ul>
<li>从网上获取数据</li>
<li>数据缺失值处理、标准化</li>
<li>估计器流程</li>
</ul>
<p>代码案例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    逻辑回归做二分类进行癌症肿瘤是否是恶性的预测（根据属性特征）</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 构造列标签名字,注：如果数据中没有标签，则pandas会默认把第一行数据作为特征名，这里数据是没有标签名的，所以这里手动添加</span></span><br><span class="line">    column = [<span class="string">'Sample code number'</span>,<span class="string">'Clump Thickness'</span>,<span class="string">'Uniformity of Cell Size'</span>,<span class="string">'Uniformity of Cell Shape'</span>,<span class="string">'Marginal Adhesion'</span>,<span class="string">'Single Epithelial Cell Size'</span></span><br><span class="line">              ,<span class="string">'Bare Nuclei'</span>,<span class="string">'Bland Chromatin'</span>,<span class="string">'Normal Nucleoli'</span>,<span class="string">'Mitoses'</span>,<span class="string">'Class'</span>]</span><br><span class="line">    data = pd.read_csv(<span class="string">"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"</span></span><br><span class="line">                , names=column)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#缺失值进行处理</span></span><br><span class="line">    data = data.replace(to_replace=<span class="string">'?'</span>,value=np.nan)</span><br><span class="line">    data = data.dropna()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据分割</span></span><br><span class="line">    x_train,x_test,y_train,y_test = train_test_split(data[column[<span class="number">1</span>:<span class="number">10</span>]],data[column[<span class="number">10</span>]],test_size=<span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#进行标准化</span></span><br><span class="line">    std = StandardScaler()</span><br><span class="line">    x_train = std.fit_transform(x_train)</span><br><span class="line">    x_test = std.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#逻辑回归预测</span></span><br><span class="line">    lg = LogisticRegression(penalty=<span class="string">'l2'</span>,C=<span class="number">1.0</span>)</span><br><span class="line">    lg.fit(x_train,y_train)</span><br><span class="line">    y_predict = lg.predict(x_test)</span><br><span class="line">    print(<span class="string">"计算的回归系数为："</span>,lg.coef_)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算的回归系数为： [[1.07139763 0.920439   1.03352718 0.88427896 0.07783286 1.15006497</span></span><br><span class="line"><span class="string">    0.94210002 0.59530658 0.81971895]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"准确率为："</span>,lg.score(x_test,y_test))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    准确率为： 0.9590643274853801</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"召回率："</span>,classification_report(y_test,y_predict,labels=[<span class="number">2</span>,<span class="number">4</span>],target_names=[<span class="string">"良性"</span>,<span class="string">"恶性"</span>]))</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    召回率：       precision    recall  f1-score   support</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">          良性       0.96      0.97      0.97       110</span></span><br><span class="line"><span class="string">          恶性       0.95      0.93      0.94        61</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     micro avg       0.96      0.96      0.96       171</span></span><br><span class="line"><span class="string">     macro avg       0.96      0.95      0.96       171</span></span><br><span class="line"><span class="string">  weighted avg       0.96      0.96      0.96       171</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    logistic()</span><br></pre></td></tr></table></figure>


<h1 id="非监督学习：K-均值聚类（k-means）算法"><a href="#非监督学习：K-均值聚类（k-means）算法" class="headerlink" title="非监督学习：K-均值聚类（k-means）算法"></a>非监督学习：K-均值聚类（k-means）算法</h1><blockquote>
<p>K-均值聚类属于无监督学习。那么监督学习和无监督学习的区别在哪儿呢？监督学习知道从对象（数据）中学习什么，而无监督学习无需知道所要搜寻的目标，它是根据算法得到数据的共同特征。比如用分类和聚类来说，分类事先就知道所要得到的类别，而聚类则不一样，只是以相似度为基础，将对象分得不同的簇。</p>
</blockquote>
<p>K-Means算法有大量的变体，包括初始化优化K-Means++, 距离计算优化elkan K-Means算法和大数据情况下的优化Mini Batch K-Means算法。</p>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><p>K-means是一个反复迭代的过程，算法分为四个步骤：</p>
<ol>
<li>选取数据空间中的K个对象作为初始中心，每个对象代表一个聚类中心；<ul>
<li>我们会根据对数据的先验经验选择一个合适的k值，如果没有什么先验知识，则可以通过交叉验证选择一个合适的k值。</li>
</ul>
</li>
<li>对于样本中的数据对象，根据它们与这些聚类中心的欧氏距离，按距离最近的准则将它们分到距离它们最近的聚类中心（最相似）所对应的类；</li>
<li>更新聚类中心：将每个类别中所有对象所对应的均值作为该类别的聚类中心，计算目标函数的值；</li>
<li>判断聚类中心和目标函数的值是否发生改变，若不变，则输出结果，若改变，则返回2）</li>
</ol>
<p>这里发现了一个python2.7版本的简易实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span>      <span class="comment">#general function to parse tab -delimited floats</span></span><br><span class="line">    dataMat = []                <span class="comment">#assume last column is target value</span></span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        fltLine = map(float,curLine) <span class="comment">#map all elements to float()</span></span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br><span class="line">\<span class="comment">#计算欧氏距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum(power(vecA - vecB, <span class="number">2</span>))) <span class="comment">#la.norm(vecA-vecB)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">    n = shape(dataSet)[<span class="number">1</span>]</span><br><span class="line">    centroids = mat(zeros((k,n)))<span class="comment">#create centroid mat</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):<span class="comment">#create random cluster centers, within bounds of each dimension</span></span><br><span class="line">        minJ = min(dataSet[:,j]) </span><br><span class="line">        rangeJ = float(max(dataSet[:,j]) - minJ)</span><br><span class="line">        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line">\<span class="comment">#dataSet样本点,k 簇的个数</span></span><br><span class="line">\<span class="comment">#disMeas距离量度，默认为欧几里得距离</span></span><br><span class="line">\<span class="comment">#createCent,初始点的选取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>] <span class="comment">#样本数</span></span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="number">2</span>))) <span class="comment">#m*2的矩阵                   </span></span><br><span class="line">    centroids = createCent(dataSet, k) <span class="comment">#初始化k个中心</span></span><br><span class="line">    clusterChanged = <span class="literal">True</span>             </span><br><span class="line">    <span class="keyword">while</span> clusterChanged:      <span class="comment">#当聚类不再变化</span></span><br><span class="line">        clusterChanged = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            minDist = inf; minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k): <span class="comment">#找到最近的质心</span></span><br><span class="line">                distJI = distMeas(centroids[j,:],dataSet[i,:])</span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:</span><br><span class="line">                    minDist = distJI; minIndex = j</span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i,<span class="number">0</span>] != minIndex: clusterChanged = <span class="literal">True</span></span><br><span class="line">            <span class="comment"># 第1列为所属质心，第2列为距离</span></span><br><span class="line">            clusterAssment[i,:] = minIndex,minDist**<span class="number">2</span></span><br><span class="line">        <span class="keyword">print</span> centroids</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更改质心位置</span></span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):</span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[:,<span class="number">0</span>].A==cent)[<span class="number">0</span>]]</span><br><span class="line">            centroids[cent,:] = mean(ptsInClust, axis=<span class="number">0</span>) </span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>伪代码如下：<br>function K-Means(输入数据，中心点个数K)<br>    获取输入数据的维度Dim和个数N<br>    随机生成K个Dim维的点<br>    while(算法未收敛)<br>        对N个点：计算每个点属于哪一类。<br>        对于K个中心点：<br>            1，找出所有属于自己这一类的所有数据点<br>            2，把自己的坐标修改为这些数据点的中心点坐标<br>    end<br>    输出结果：<br>end</p>
<blockquote>
<p>sklearn.cluster.KMeans(n_clusters=8,init=’k-means++’) </p>
</blockquote>
<ul>
<li>n_clusters:开始的聚类中心数量</li>
<li>init:初始化方法，默认为’k-means++’</li>
<li>labels_:默认标记的类型，可以和真实值比较（不是值比较）</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BA%8C/" data-id="ck9sdq083000a70vbbradcyi5" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-python与机器学习入门一" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/" class="article-date">
  <time class="post-time" datetime="2020-01-11T09:35:46.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">11</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/">python与机器学习入门一</a>
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="为什么要写本文"><a href="#为什么要写本文" class="headerlink" title="为什么要写本文"></a>为什么要写本文</h1><p>机器学习涉及到很多数学相关基础知识，概率论，高数算法等，而直接去看这些课本或书籍，简直就是晦涩难懂，哪怕是西瓜书这种通俗易懂的书籍，对于数学不好的人来讲也有点像天书。为了让小白可以较轻松的掌握一些机器学习相关知识，还是要从案例出发，通过案例来学习用到的数学知识，熟悉各种算法的原理，顺便将用到的库和框架进行简单介绍，而且可以结合场景解决实际问题，提高初学者幸福感。<br>不过在此还是要列举一些书籍可供参考：</p>
<ul>
<li>概率论和高数，直接大学课本就好</li>
<li>《机器学习》：俗称西瓜书，学习机器学习的经典书籍</li>
<li>《Python数据分析与挖掘实战》：从数据挖掘的应用出发，以电力、航空、医疗、互联网、生产制造以及公共服务等行业真实案例为主线，深入浅出介绍Python数据挖掘建模过程，实践性极强。 </li>
<li>《TensorFlow技术解析与实战》：包揽TensorFlow1.1的新特性 人脸识别 语音识别 图像和语音相结合等热点一应俱全</li>
</ul>
<h1 id="机器学习是什么"><a href="#机器学习是什么" class="headerlink" title="机器学习是什么"></a>机器学习是什么</h1><blockquote>
<p>机器学习是人工智能的一个分支。机器学习算法是一类从<strong>数据</strong>中自动分析获得<strong>规律</strong>，并利用规律对未知数据进行<strong>预测</strong>的算法。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。 它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。因为学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为统计学习理论。</p>
</blockquote>
<p>机器学习应用在哪些方面？</p>
<blockquote>
<p>维基百科：机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。<br>新闻等应用上的推荐类功能,人脸识别相关功能：直播时的美颜、加装饰等等；<br>这里有一个集合，最具价值的50个机器学习应用[2017年]：<a href="https://bigquant.com/community/t/topic/6909" target="_blank" rel="noopener">https://bigquant.com/community/t/topic/6909</a></p>
</blockquote>
<h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><p>python深度学习中，读取数据一般是通过pandas读取文件，之所以不读取数据库，是因为数据库读取速度会成为性能瓶颈。</p>
<p>可用数据集：</p>
<ul>
<li>Kaggle：1、大数据竞赛平台，真实数据，数据量巨大</li>
<li>UCI：1，覆盖科学、生活、经济等领域</li>
<li>scikit-learn：数据量小，方便学习</li>
</ul>
<p>常用数据集结构：特征值+目标值<br>注：有些数据可以没有目标值<br>数据中对于特征的处理：</p>
<ul>
<li>pandas：一个数据读取非常方便以及基本的处理格式的工具</li>
<li>对于<strong>特征的处理</strong>提供了强大的接口</li>
</ul>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>什么是特征工程（目的）</p>
<blockquote>
<p>一句话：特征工程是将<strong>原始数据转换为更好的代表预测模型的潜在问题的特征</strong>的过程，从而提高了对未知数据的预测准确性。<br> “数据决定了机器学习的上限，而算法只是尽可能逼近这个上限”，这里的数据指的就是经过特征工程得到的数据。特征工程指的是把原始数据转变为模型的训练数据的过程，它的目的就是获取更好的训练数据特征，使得机器学习模型逼近这个上限。特征工程能使得模型的性能得到提升，有时甚至在简单的模型上也能取得不错的效果。特征工程在机器学习中占有非常重要的作用，一般认为括特征构建、特征提取、特征选择三个部分。特征构建比较麻烦，需要一定的经验。 特征提取与特征选择都是为了从原始特征中找出最有效的特征。它们之间的区别是特征提取强调通过特征转换的方式得到一组具有明显物理或统计意义的特征；而特征选择是从特征集合中挑选一组具有明显物理或统计意义的特征子集。两者都能帮助减少特征的维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。<br>特征工程的目的是筛选出更好的特征，获取更好的训练数据.</p>
</blockquote>
<p>特征工程流程：</p>
<ul>
<li><ol>
<li>数据采集 / 清洗 / 采样<ul>
<li>1.1:数据采集：数据采集前需要明确采集哪些数据，一般的思路为：哪些数据对最后的结果预测有帮助？数据我们能够采集到吗？线上实时计算的时候获取是否快捷？ </li>
<li>1.2:数据清洗： 数据清洗也是很重要的一步，机器学习算法大多数时候就是一个加工机器，至于最后的产品如何，取决于原材料的好坏。数据清洗就是要去除脏数据，比如某些商品的刷单数据。 </li>
<li>1.3:数据采样：采集、清洗过数据以后，正负样本是不均衡的，要进行数据采样。采样的方法有随机采样和分层抽样。但是随机采样会有隐患，因为可能某次随机采样得到的数据很不均匀，更多的是根据特征采用分层抽样。　　</li>
</ul>
</li>
</ol>
</li>
<li>2.特征处理<ul>
<li>2.1:数值型<ul>
<li>幅度调整/归一化：python中会有一些函数比如preprocessing.MinMaxScaler()将幅度调整到 [0,1] 区间。</li>
<li>统计值：包括max, min, mean, std等。python中用pandas库序列化数据后，可以得到数据的统计值。 </li>
</ul>
</li>
<li>2.2:类别型,类别型一般是文本信息，比如颜色是红色、黄色还是蓝色，我们存储数据的时候就需要先处理数据<ul>
<li>one-hot编码，编码后得到哑变量。统计这个特征上有多少类，就设置几维的向量，pd.get_dummies()可以进行one-hot编码。</li>
<li>Hash编码成词向量</li>
<li>Histogram映射：把每一列的特征拿出来，根据target内容做统计，把target中的每个内容对应的百分比填到对应的向量的位置。优点是把两个特征联系起来。 </li>
</ul>
</li>
<li>2.3:时间型<ul>
<li>连续值</li>
<li>离散值</li>
</ul>
</li>
<li>2.4:文本型<ul>
<li>词袋：文本数据预处理后，去掉停用词，剩下的词组成的list，在词库中的映射稀疏向量。Python中用CountVectorizer处理词袋． </li>
<li>把词袋中的词扩充到n-gram：n-gram代表n个词的组合。</li>
</ul>
</li>
<li>2.5:统计型<ul>
<li>加减平均,商品价格高于平均价格多少，用户在某个品类下消费超过平均用户多少，用户连续登录天数超过平均多少…</li>
<li>分位线,商品属于售出商品价格的多少分位线处</li>
<li>次序型</li>
<li>比例类</li>
</ul>
</li>
<li>2.6:组合特征<ul>
<li>拼接型</li>
<li>模型特征组合</li>
</ul>
</li>
</ul>
</li>
<li>3.特征选择:特征选择，就是从多个特征中，挑选出一些对结果预测最有用的特征。特征选择和降维有什么区别呢？前者只踢掉原本特征里和结果预测关系不大的， 后者做特征的计算组合构成新特征。<br>  3.1:过滤型,评估单个特征和结果值之间的相关程度， 排序留下Top相关的特征部分。<br>  3.2:包裹型,把特征选择看做一个特征子集搜索问题， 筛选各种特征子集， 用模型评估效果。 典型算法：“递归特征删除算法”。<br>  3.3:嵌入型,根据模型来分析特征的重要性，最常见的方式为用正则化方式来做特征选择。</li>
</ul>
<h3 id="scikit-learn工具"><a href="#scikit-learn工具" class="headerlink" title="scikit-learn工具"></a>scikit-learn工具</h3><ul>
<li>Python语言的机器学习工具基于Numpy和Scipy</li>
<li>提供了大量用于数据挖掘和分析的工具，包括数据预处理、交叉验证、算法与可视化算法等一系列接口。</li>
<li>文档完善，容易上手</li>
</ul>
<h3 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h3><p>API：sklearn.feature_extraction</p>
<p>字典特征数据抽取：把字典中一些<strong>类别</strong>数据，转换为特征值，如One-hot编码形势。</p>
<ul>
<li>DictVectorizer</li>
</ul>
<p>文本特征抽取：对文本数据进行特征值化<br>文本抽取默认不会对中文进行分词，而是按照空格逗号等进行分。所以在fit_transform之前要进行分词。</p>
<ul>
<li>CountVectorizer:简单的频率统计</li>
<li>TfidfVectorizer：tf-idf算法<ul>
<li>在将文本分词并向量化后，我们可以得到词汇表中每个词在各个文本中形成的词向量，但是这时候的词向量里，有一些类似”to”,”my”,”is”的词，出现的频率可能会高于文章的中心词汇，如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。</li>
<li>TF-IDF是Term Frequency -  Inverse Document Frequency的缩写，即“词频-逆文本频率”。它由两部分组成，TF和IDF。<ul>
<li>TF也就是词频，我们做的向量化也就是做了文本中各个词的出现频率统计。<blockquote>
<p>词频（TF） = 某个词在文章中的出现次数<br>词频（TF） = 某个词在文章中的出现次数 / 文章总词数。这里是因为文章有长短之分，为了便于不同文章的比较,做”词频”标准化.</p>
</blockquote>
</li>
<li>IDF，即“逆文本频率”，IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词<blockquote>
<p>逆文档频率（IDF） = log（语料库的文档总数/包含该词的文档总数+1）</p>
</blockquote>
</li>
</ul>
</li>
<li>TF-IDF = 词频（TF) * 逆文档频率（IDF）,TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。</li>
</ul>
</li>
</ul>
<p>特征抽取简单demo及API如下：</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction import DictVectorizer</span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line"></span><br><span class="line">def dictvec():</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    字典数据抽取</span><br><span class="line">    :return: None</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    # 实例化</span><br><span class="line">    dict &#x3D; DictVectorizer(sparse&#x3D;False)  # sparse指定fit_transform的结果是否转换为sparse矩阵,False的话则不是sparse矩阵而是数组形势</span><br><span class="line">    # 调用fit_transform</span><br><span class="line">    data &#x3D; dict.fit_transform(</span><br><span class="line">        [&#123;&#39;city&#39;: &#39;北京&#39;, &#39;temperature&#39;: 100&#125;, &#123;&#39;city&#39;: &#39;上海&#39;, &#39;temperature&#39;: 60&#125;, &#123;&#39;city&#39;: &#39;深圳&#39;, &#39;temperature&#39;: 30&#125;])     #参数是列表</span><br><span class="line">    print(data)</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    sparse &#x3D; False情况下,这也叫 One-hot编码：</span><br><span class="line">    [[  0.   1.   0. 100.]</span><br><span class="line">    [  1.   0.   0.  60.]</span><br><span class="line">    [  0.   0.   1.  30.]]</span><br><span class="line">    sparse默认为true情况下：</span><br><span class="line">    (0, 1)	1.0</span><br><span class="line">    (0, 3)	100.0</span><br><span class="line">    (1, 0)	1.0</span><br><span class="line">    (1, 3)	60.0</span><br><span class="line">    (2, 2)	1.0</span><br><span class="line">    (2, 3)	30.0</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    print(dict.get_feature_names())  # 返回类别名称，如这里：[&#39;city&#x3D;上海&#39;,&#39;city&#x3D;北京&#39;,&#39;city&#x3D;深圳&#39;,&#39;temperature&#39;]</span><br><span class="line">    return None</span><br><span class="line"></span><br><span class="line">def countvec():</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    对文本进行特征值化</span><br><span class="line">    :return: None</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    cv &#x3D; CountVectorizer()  #这里没有sparse参数，后续如果要转换为数组，则要用fit_transform产生的sparse矩阵调用toarray()函数</span><br><span class="line">    data &#x3D; cv.fit_transform([&quot;To the world \\u9752\\u9752 may be just one person&quot;,&quot; To me \\u9752\\u9752 may be the world&quot;])     # 参数是列表</span><br><span class="line"></span><br><span class="line">    print(cv.get_feature_names())</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    统计所有词，单个的英文字母不会统计，比如‘i’，结果为：</span><br><span class="line">    [&#39;be&#39;, &#39;just&#39;, &#39;may&#39;, &#39;me&#39;, &#39;one&#39;, &#39;person&#39;, &#39;the&#39;, &#39;to&#39;, &#39;u9752&#39;, &#39;world&#39;]</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    print(data.toarray())</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    对应词出现次数</span><br><span class="line">    [[1 1 1 0 1 1 1 1 2 1]</span><br><span class="line">    [1 0 1 1 0 0 1 1 2 1]]</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    return None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    # dictvec()</span><br><span class="line">    countvec()</span><br></pre></td></tr></table></figure>


<h3 id="特征预处理"><a href="#特征预处理" class="headerlink" title="特征预处理"></a>特征预处理</h3><p>通过特定的统计方法讲数据转换为算法要求的数据。</p>
<blockquote>
<p>sklearn.preprocessing</p>
</blockquote>
<ul>
<li>归一化，防止某个数据对结果影响太大 sklearn.preprocessing.MinMaxScaler<ul>
<li>通过将原始数据进行变换，把原始数据映射到[0,1]之间</li>
<li>原理：作用于每一列，max为一列的最大值，min为一列的最小值<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/sklearn%E5%BD%92%E4%B8%80%E5%8C%96%E5%85%AC%E5%BC%8F.jpg" class="" title="sklearn归一化公式"> </li>
<li>缺点：异常点对此影响较大，这种情况下要使用标准化来解决</li>
</ul>
</li>
<li>标准化，异常值影响较小 sklearn.preprocessing.StandardScaler<ul>
<li>通过对原始数据进行变换，把数据变换到<strong>均值为0，标准差为1</strong>的范围内</li>
<li>在已有样本足够多的情况下比较稳定，适合现代嘈杂的大数据场景</li>
<li>原理：<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/sklearn%E6%A0%87%E5%87%86%E5%8C%96%E5%85%AC%E5%BC%8F.jpg" class="" title="sklearn标准化公式"></li>
</ul>
</li>
</ul>
<p>归一化与标准化示例如下：</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import MinMaxScaler,StandardScaler</span><br><span class="line"></span><br><span class="line">def mm():</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    归一化处理</span><br><span class="line">    :return: None</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    mm &#x3D; MinMaxScaler(feature_range&#x3D;(0,1))  #参数可以指定归一化后的范围，默认为[0,1]</span><br><span class="line">    data &#x3D; mm.fit_transform([[10,2,5,40],[20,4,5,8],[30,6,5,5]])</span><br><span class="line">    print(data)</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    归一化是按照列来归一的</span><br><span class="line">    [[0.         0.         0.         1.        ]</span><br><span class="line">    [0.5        0.5        0.         0.08571429]</span><br><span class="line">    [1.         1.         0.         0.        ]]</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    return None</span><br><span class="line"></span><br><span class="line">def stand():</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    标准化缩放</span><br><span class="line">    :return: None</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    std &#x3D; StandardScaler()</span><br><span class="line">    data &#x3D; std.fit_transform([[1,-1,3],[2,4,2],[4,6,-1]])</span><br><span class="line">    print(data)</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    转换后的值，每一列均值为0，标准差为1</span><br><span class="line">    [[-1.06904497 -1.35873244  0.98058068]</span><br><span class="line">    [-0.26726124  0.33968311  0.39223227]</span><br><span class="line">    [ 1.33630621  1.01904933 -1.37281295]]</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    return None</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    #mm()</span><br><span class="line">    stand()</span><br></pre></td></tr></table></figure>

<h3 id="数据降维"><a href="#数据降维" class="headerlink" title="数据降维"></a>数据降维</h3><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>特征选择原因：如果训练数据包含许多<strong>冗余</strong>或<strong>无关</strong>特征，因而移除这些特征并不会导致丢失信息</p>
<ul>
<li>数据冗余：部分特征相关度高，容易消耗计算机性能</li>
<li>缩短训练时间</li>
<li>改善通用性、降低过拟合</li>
</ul>
<p>特征选择是什么？</p>
<blockquote>
<p>特征选择就是单纯的从提取到的所有特征中选择部分特征作为训练集特征，特征在选择前和选择后可以改变值、也可以不改变值，但是选择后的特征维数肯定比选择前小，因为只是选择了其中一部分特征。</p>
</blockquote>
<p>主要方法：</p>
<ul>
<li>Filter(过滤式):<ul>
<li>VarianceThreshold(方差过滤):由于方差为0的时候，一列的数据是一样的，所以是对结果没用的数据，由此推论，当方差小于某个值的时候，这个特征是不具有参考性的，可以剔除。<ul>
<li>sklearn.feature_selection.VarianceThreshold</li>
</ul>
</li>
</ul>
</li>
<li>Embedded(嵌入式):正则化、决策树</li>
<li>Wrapper(包裹式) </li>
<li>神经网络</li>
</ul>
<h4 id="主成分分析（PCA，principal-Component-Analysis）"><a href="#主成分分析（PCA，principal-Component-Analysis）" class="headerlink" title="主成分分析（PCA，principal Component Analysis）"></a>主成分分析（PCA，principal Component Analysis）</h4><p>背景：</p>
<blockquote>
<p>在许多领域的研究与应用中，通常需要对含有多个变量的数据进行观测，收集大量数据后进行分析寻找规律。多变量大数据集无疑会为研究和应用提供丰富的信息，但是也在一定程度上增加了数据采集的工作量。更重要的是在很多情形下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性。如果分别对每个指标进行分析，分析往往是孤立的，不能完全利用数据中的信息，因此盲目减少特征会损失很多有用的信息，从而产生错误的结论。因此需要找到一种合理的方法，在减少需要分析的特征同时，尽量减少信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量之间存在一定的相关关系，因此可以考虑将关系紧密的变量变成尽可能少的新变量，使这些新变量是两两不相关的，那么就可以用较少的综合指标分别代表存在于各个变量中的各类信息。主成分分析与因子分析就属于这类降维算法。<br>说白了就是特征太多情况下，如果很多特征之间是有联系的，这时直接通过特征选择等方式来剔除掉某些特征，会导致失去一些有用信息。所以需要一种方法，在减少特征数量的同时，尽可能少的减少信息的损失。</p>
</blockquote>
<p>PCA是什么？</p>
<blockquote>
<p>是一种使用最广泛的数据压缩算法。在PCA中，数据从原来的坐标系转换到新的坐标系，由数据本身决定。转换坐标系时，以方差最大的方向作为坐标轴方向，因为数据的最大方差给出了数据的最重要的信息。第一个新坐标轴选择的是原始数据中方差最大的方法，第二个新坐标轴选择的是与第一个新坐标轴正交且方差次大的方向。重复该过程，重复次数为原始数据的特征维数。<br>通过这种方式获得的新的坐标系，我们发现，大部分方差都包含在前面几个坐标轴中，后面的坐标轴所含的方差几乎为0,于是，我们可以忽略余下的坐标轴，只保留前面的方差较大的坐标轴。事实上，这样也就相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，也就实现了对数据特征的降维处理。<br>PCA里，特征数量会减少，但数据也会改变。一般特征数量达到上百才会用</p>
</blockquote>
<p>作用：</p>
<blockquote>
<p>可以削减回归分析或者聚类分析中特征的数量</p>
</blockquote>
<p>原理：<br>一张结果图：</p>
<img src="/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/%E9%99%8D%E7%BB%B4%E6%8A%95%E5%BD%B1.jpg" class="" title="降维投影">
<blockquote>
<p>详细过程参考这里的‘3.5PCA算法两种实现方法’：<a href="https://blog.csdn.net/program_developer/article/details/80632779" target="_blank" rel="noopener">https://blog.csdn.net/program_developer/article/details/80632779</a></p>
</blockquote>
<p>数据降维的两个方法demo：</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_selection import VarianceThreshold</span><br><span class="line">from sklearn.decomposition import PCA</span><br><span class="line">def var():</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    特征选择-删除低方差的特征</span><br><span class="line">    :return: None</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    var &#x3D; VarianceThreshold(threshold&#x3D;0.0)  # threshold指定方差阈值</span><br><span class="line">    data &#x3D; var.fit_transform([[0,2,0,3],[0,1,4,3],[0,1,1,3]])  #数据的第一列和最后一列方差为0</span><br><span class="line">    print(data)</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    过滤掉了第一列和最后一列</span><br><span class="line">    [[2 0]</span><br><span class="line">    [1 4]</span><br><span class="line">    [1 1]]</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    return None</span><br><span class="line"></span><br><span class="line">def pca():</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    主成分分析进行特征降维</span><br><span class="line">    :return: None</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    pca &#x3D; PCA(n_components&#x3D;0.9)   # n_components可以选择保留的数据信息比例，一般为90%~95%</span><br><span class="line">    data &#x3D; pca.fit_transform([[2,4,6,8],[5,4,8,6],[1,5,6,3]])</span><br><span class="line">    print(data)</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    [[-1.32007718  2.16837282]</span><br><span class="line">    [-1.95237119 -1.90596201]</span><br><span class="line">    [ 3.27244837 -0.26241081]]</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    return None</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    pca()</span><br></pre></td></tr></table></figure>


<h1 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h1><blockquote>
<p>算法是核心，数据和计算是基础。</p>
</blockquote>
<p>机器学习算法判别依据：数据类型</p>
<ul>
<li>离散型数据：由记录不同类别个体的数目所得到的数据，又称计数数据，所有这些数据全部都是整数，而且不能再细分，也不能进一步提高他们的精确度。<strong>区间内不可分</strong></li>
<li>连续型数据：变量可以在某个范围内取任一数，即变量的取值可以是连续的，如长度、时间、质量等，这类数值通常含有小数部分。<strong>区间内可分</strong></li>
</ul>
<p>机器学习算法分类：</p>
<ul>
<li>监督学习（预测）：有<strong>特征值和目标值</strong>，有标准答案<ul>
<li>分类：对应数据类型为<strong>离散型</strong><ul>
<li>k-近邻算法</li>
<li>贝叶斯分类</li>
<li>决策树与随机森林</li>
<li>逻辑回归</li>
<li>神经网络</li>
</ul>
</li>
<li>回归：对应数据类型为<strong>连续型</strong><ul>
<li>线性回归</li>
<li>岭回归</li>
</ul>
</li>
<li>标注<ul>
<li>隐马尔科夫模型</li>
</ul>
</li>
</ul>
</li>
<li>无监督学习：<strong>只有特征值</strong><ul>
<li>聚类<ul>
<li>k-means</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="机器学习开发流程"><a href="#机器学习开发流程" class="headerlink" title="机器学习开发流程"></a>机器学习开发流程</h3><ul>
<li>获取数据<blockquote>
<p>包括获取原始数据以及从原始数据中经过特征工程从原始数据中提取训练、测试数据</p>
</blockquote>
</li>
<li>特征工程<blockquote>
<p>包括从原始数据中特征构建、特征提取、特征选择。特征工程做的好能发挥原始数据的最大效力，往往能够使得算法的效果和性能得到显著的提升，有时能使简单的模型的效果比复杂的模型效果好。数据挖掘的大部分时间就花在特征工程上面，是机器学习非常基础而又必备的步骤。数据预处理、数据清洗、筛选显著特征、摒弃非显著特征等等都非常重要。</p>
</blockquote>
</li>
<li>训练模型、诊断、调优<blockquote>
<p>诊断中至关重要的是判断过拟合、欠拟合，常见的方法是绘制学习曲线，交叉验证。通过增加训练的数据量、降低模型复杂度来降低过拟合的风险，提高特征的数量和质量、增加模型复杂来防止欠拟合。诊断后的模型需要进行进一步调优，调优后的新模型需要重新诊断</p>
</blockquote>
</li>
<li>模型验证、误差分析<blockquote>
<p>主要是分析出误差来源与数据、特征、算法。</p>
</blockquote>
</li>
<li>模型融合<blockquote>
<p>提升算法的准确度主要方法是模型的前端（特征工程、清洗、预处理、采样）和后端的模型融合</p>
</blockquote>
</li>
<li>上线运行<blockquote>
<p>通过提供API等形式来上线</p>
</blockquote>
</li>
</ul>
<h1 id="sklearn数据集"><a href="#sklearn数据集" class="headerlink" title="sklearn数据集"></a>sklearn数据集</h1><h4 id="1-数据集划分"><a href="#1-数据集划分" class="headerlink" title="1.数据集划分"></a>1.数据集划分</h4><p>机器学习一般的数据集会划分为两个部分：</p>
<ul>
<li>训练数据：用户训练、构建模型</li>
<li>测试数据：在模型检验时使用，用于评估模型是否有效</li>
</ul>
<h4 id="2-sklearn数据集API介绍"><a href="#2-sklearn数据集API介绍" class="headerlink" title="2.sklearn数据集API介绍"></a>2.sklearn数据集API介绍</h4><ul>
<li>sklearn.model_selection.train_test_split</li>
<li>sklearn.datasets<ul>
<li>datasets.load_*();    获取小规模数据集，数据包含在datasets里</li>
<li>datasets.fetch_*(data_home=None);  获取大规模数据集，需要从网络上下载，第一个参数是data_home表示数据集下载的目录，默认 ‘~/scikit_learn_data/‘</li>
</ul>
<ul>
<li>load*和fetch*返回的数据类型都是datasets.base.Bunch(<strong>字典格式</strong>)<ul>
<li>data:特征数据数组，是[n_samples * m_features]的二维<strong>numpy.ndarry数组</strong></li>
<li>target:标签数组，是n_samples的<strong>一维numpy.ndarray数组</strong></li>
<li>DESCR:数据描述</li>
<li>feature_names:特征名，<strong>新闻数据、手写数字、回归数据集没有</strong></li>
<li>target_names:标签名</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="3-sklearn分类数据集，对应分类算法"><a href="#3-sklearn分类数据集，对应分类算法" class="headerlink" title="3.sklearn分类数据集，对应分类算法"></a>3.sklearn分类数据集，对应分类算法</h4><blockquote>
<p>此处以内置数据集 load_iris和20newsgroups 为例</p>
</blockquote>
<p>1.load_iris()获取内置普通分类用的数据集<br>2.获取用于分类测试的大数据集，sklearn.datasets.fetch_20newgroups(),数据集收集了大约20,000左右的新闻组文档，均匀分为20个不同主题的新闻组集合。</p>
<blockquote>
<p>datasets.clear</p>
</blockquote>
<p>数据集进行分割：<br>sklearn.model_selection.train_test_split(<em>arrays,*</em>options)</p>
<ul>
<li>x:数据集的特征值</li>
<li>y:数据集的标签值</li>
<li>test_size:测试集大小，一般为float</li>
<li>random_state:随机数种子</li>
<li>return:训练集特征值，测试集特征值，训练标签，测试标签</li>
</ul>
<p>下面为两个数据集的获取代码，目前仅为数据采集，至于特征提取、模型训练等后续再写</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">from sklearn.datasets import load_iris, fetch_20newsgroups</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">内置的数据集，加载并返回鸢尾花数据集</span><br><span class="line">类别：3</span><br><span class="line">特征：4</span><br><span class="line">样本数量：150</span><br><span class="line">每个类别数量：50</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">li &#x3D; load_iris()</span><br><span class="line"></span><br><span class="line">print(li.data)  # 获取特征数据数组</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">[[5.1 3.5 1.4 0.2]</span><br><span class="line"> [4.9 3.  1.4 0.2]</span><br><span class="line"> [4.7 3.2 1.3 0.2]</span><br><span class="line"> ......</span><br><span class="line"> [5.9 3.  5.1 1.8]]</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">print(li.target)  # 获取目标值</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">目标值是离散型，0，1，2</span><br><span class="line">[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</span><br><span class="line"> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</span><br><span class="line"> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2</span><br><span class="line"> 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</span><br><span class="line"> 2 2]</span><br><span class="line">.. _iris_dataset:</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">    # print(li.DESCR)     #打印出一些描述信息</span><br><span class="line"></span><br><span class="line">train_feature, test_feature, train_target, test_target &#x3D; train_test_split(li.data, li.target,</span><br><span class="line">                                                                          test_size&#x3D;0.25)  # 参数：特征值，目标值，测试集大小</span><br><span class="line">    # 返回值依次为：训练集的特征值、测试集特征值、训练集目标值、测试集目标值</span><br><span class="line"></span><br><span class="line">print(&quot;训练集特征值和目标值：&quot;, train_feature, train_target)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">训练集特征值和目标值： [[6.4 3.2 5.3 2.3]</span><br><span class="line"> [4.6 3.6 1.  0.2]</span><br><span class="line"> [5.5 2.5 4.  1.3]</span><br><span class="line"> ......</span><br><span class="line"> [5.7 2.8 4.1 1.3]</span><br><span class="line"> [5.5 2.4 3.7 1. ]]</span><br><span class="line"> [2 0 1 1 2 2 0 1 1 0 2 2 2 0 1 1 2 2 1 1 1 0 0 2 1 2 1 0 1 1 0 0 1 0 0 0 2</span><br><span class="line"> 2 0 0 1 1 1 1 1 0 1 2 0 0 0 2 1 0 0 1 1 0 1 1 1 0 0 2 0 1 2 2 2 0 2 0 2 2</span><br><span class="line"> 0 1 2 0 1 0 2 2 2 0 2 0 1 0 1 0 1 0 2 1 1 1 0 1 1 1 2 2 2 0 0 2 2 2 0 2 1</span><br><span class="line"> 1]</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">print(&quot;测试集特征值和目标值：&quot;, test_feature, test_target)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">测试集特征值和目标值： [[6.3 2.9 5.6 1.8]</span><br><span class="line"> [6.3 2.5 4.9 1.5]</span><br><span class="line"> [7.3 2.9 6.3 1.8]</span><br><span class="line"> ......</span><br><span class="line"> [4.4 2.9 1.4 0.2]</span><br><span class="line"> [5.1 3.4 1.5 0.2]]</span><br><span class="line"> [2 1 2 2 2 1 2 0 2 2 2 0 0 2 0 0 1 1 1 1 2 2 0 2 2 1 1 2 2 0 0 1 1 0 0 2 0 0]</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">也是一个内置的数据集，不过是是大数据集，会从网络下载</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">news &#x3D; fetch_20newsgroups(data_home&#x3D;r&quot;G:\pyMechineLearn&quot;, subset&#x3D;&#39;all&#39;)</span><br><span class="line">print(news.data)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">貌似是一个list类型，每一个元素是str类型，也就是一篇文章。</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">print(news.target)</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">不同的数对应不同的文章类型</span><br><span class="line">[10  3 17 ...  3  1  7]</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="4-sklearn回归数据集，对应回归算法"><a href="#4-sklearn回归数据集，对应回归算法" class="headerlink" title="4.sklearn回归数据集，对应回归算法"></a>4.sklearn回归数据集，对应回归算法</h4><blockquote>
<p>此处以内置数据集 波士顿房价数据集为例<br>sklearn.datasets.load_boston()</p>
</blockquote>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">from sklearn.datasets import load_boston</span><br><span class="line"></span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">波士顿房价数据</span><br><span class="line">目标类别：5-50</span><br><span class="line">特征：13</span><br><span class="line">样本数量：506</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">lb &#x3D; load_boston()</span><br><span class="line"></span><br><span class="line">print(lb.data)  #获取特征值</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">特征值就是收集的属性，房子的一些大小等信息</span><br><span class="line">[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]</span><br><span class="line"> [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]</span><br><span class="line"> [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]</span><br><span class="line"> ...</span><br><span class="line"> [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]</span><br><span class="line"> [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]</span><br><span class="line"> [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">print(lb.target)    #获取目标值</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4</span><br><span class="line"> 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8</span><br><span class="line">.......</span><br><span class="line">  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9</span><br><span class="line"> 22.  11.9]</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">print(lb.DESCR) #描述信息</span><br><span class="line">&#39;&#39;&#39;</span><br><span class="line">\*\*Data Set Characteristics:\*\* </span><br><span class="line">:Number of Instances: 506</span><br><span class="line">:Attribute Information (in order):</span><br><span class="line">        - CRIM     per capita crime rate by town</span><br><span class="line">        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.</span><br><span class="line">        - INDUS    proportion of non-retail business acres per town</span><br><span class="line">        .........</span><br><span class="line">        - MEDV     Median value of owner-occupied homes in $1000&#39;s</span><br><span class="line">&#39;&#39;&#39;</span><br></pre></td></tr></table></figure>

<h1 id="转换器（Transformer）与估计器（estimator）"><a href="#转换器（Transformer）与估计器（estimator）" class="headerlink" title="转换器（Transformer）与估计器（estimator）"></a>转换器（Transformer）与估计器（estimator）</h1><p>特征工程中，实例化出来的可调用fit_transform API的对象（如 DictVectorizer等）就是一个Transformer</p>
<ul>
<li>fit_transform():输入数据直接转换</li>
<li>fit()+transform():fit是单纯的输入数据，然后进行一些预先处理，如计算平均值，方差等。transform是进行数据的转换</li>
</ul>
<p>在sklearn中，估计器（estimator）是一个重要的角色，是一类实现了算法的API，这玩意不是知道接口是啥就可以，而是传入的算法参数- -。。</p>
<ul>
<li>用于分类的估计器：<ul>
<li>sklearn.neighbors k-近邻算法</li>
<li>sklearn.naive_bayes 贝叶斯</li>
<li>sklearn.linear_model.logisticRegression 逻辑回归</li>
<li>sklearn.tree 决策树与随机森林</li>
</ul>
</li>
<li>用于分类的估计器：<ul>
<li>sklearn.linear_model.linearRegression 线性回归</li>
<li>sklearn.linear_model.Ridge 岭回归</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/11/python%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%80/" data-id="ck9sdq07z000570vbhyyn9wyr" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/" rel="tag">特征工程</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-python数据分析基础" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/11/python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80/" class="article-date">
  <time class="post-time" datetime="2020-01-11T09:25:24.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">11</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/11/python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80/">python数据分析基础</a>
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/">数据分析</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><ul>
<li>Python VS R 语言(原文：<a href="https://mp.weixin.qq.com/s/CAsVeEJlgru85NUHcUKv6A)：" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/CAsVeEJlgru85NUHcUKv6A)：</a><blockquote>
<ul>
<li>什么是R语言？<br>R语言，一种自由软件编程语言与操作环境，主要用于统计分析、绘图、数据挖掘。R基于S语言的一个GNU计划项目，所以也可以当作S语言的一种实现。</li>
<li>Python与R语言共同特点</li>
<li>Python和R在数据分析和数据挖掘方面都有比较专业和全面的模块，很多常用的功能，比如矩阵运算、向量运算等都有比较高级的用法</li>
<li>Python和R两门语言有多平台适应性，linux、window都可以使用，并且代码可移植性强</li>
<li>Python和R比较贴近MATLAB以及minitab等常用的数学工具</li>
<li>Python与R语言区别</li>
<li>数据结构方面，由于是从科学计算的角度出发，R中的数据结构非常的简单，主要包括向量(一维)、多维数组(二维时为矩阵)、列表(非结构化数据)、数据框(结构化数据)。而 Python 则包含更丰富的数据结构来实现数据更精准的访问和内存控制，多维数组(可读写、有序)、元组(只读、有序)、集合(唯一、无序)、字典(Key-Value)等等。</li>
<li>Python与R相比速度要快。Python可以直接处理上G的数据;R不行，R分析数据时需要先通过数据库把大数据转化为小数据(通过groupby)才能交给R做分析，因此R不可能直接分析行为详单，只能分析统计结果。</li>
<li>Python是一套比较平衡的语言，各方面都可以，无论是对其他语言的调用，和数据源的连接、读取，对系统的操作，还是正则表达和文字处理，Python都有着明显优势。 而R是在统计方面比较突出。</li>
<li>总结<br>总的来说，Python 的 pandas 借鉴了R的dataframes，R 中的 rvest 则参考了 Python的BeautifulSoup，两种语言在一定程度上存在互补性，通常，我们认为 Python 比 R 在计算机编程、网络爬虫上更有优势，而 R 在统计分析上是一种更高效的独立数据分析工具。所以说，同时学会Python和R这两把刷子才是数据科学的王道。</li>
</ul>
</blockquote>
</li>
</ul>
<p>python其实是可以直接<strong>调用R语言的函数</strong>的，只需要安装rpy2模块即可。</p>
<h2 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h2><p>提到数据分析，就不得不考虑到数据分析后的展示，Matplotlib是一个Python 2D绘图库，它可以在各种平台上以各种硬拷贝格式和交互式环境生成出具有出版品质的图形。<br>至于matplotlib的安装，可以通过 Anaconda来安装，相关操作不多赘述。</p>
<h4 id="折线图示例"><a href="#折线图示例" class="headerlink" title="折线图示例"></a>折线图示例</h4><img src="/2020/01/11/python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80/%E6%8A%98%E7%BA%BF%E5%9B%BE.jpg" class="" title="折线图">
<p>相关api在代码中都有注释<br>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="comment"># 折线图</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注：matplotlib默认时不支持中文的，可通过rc修改</span></span><br><span class="line">font = &#123;<span class="string">'family'</span>: <span class="string">'SimHei'</span>,</span><br><span class="line">        <span class="string">'weight'</span>: <span class="string">'bold'</span>,</span><br><span class="line">        <span class="string">'size'</span>: <span class="string">'16'</span>&#125;</span><br><span class="line">matplotlib.rc(<span class="string">'font'</span>, **font)</span><br><span class="line"></span><br><span class="line">x = range(<span class="number">2</span>, <span class="number">26</span>, <span class="number">2</span>)</span><br><span class="line">y = [<span class="number">15</span>, <span class="number">13</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">15</span>, <span class="number">3</span>, <span class="number">20</span>, <span class="number">22</span>, <span class="number">14.5</span>, <span class="number">6.3</span>, <span class="number">21.2</span>, <span class="number">10</span>]</span><br><span class="line"><span class="comment"># 要在一个图形上绘制多个曲线，可直接设置另一个列表然后通过plot来绘制</span></span><br><span class="line">another_y = [<span class="number">10</span>, <span class="number">10</span>, <span class="number">22</span>, <span class="number">15</span>, <span class="number">13</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">11.5</span>, <span class="number">20</span>, <span class="number">21.2</span>, <span class="number">20</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置图片大小</span></span><br><span class="line"><span class="comment"># plt.figure(figsize=(20, 8), dpi=80)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图，label为折线名字，需要在后面添加legend方法才能显示在图形上，还可设置线条颜色和style等</span></span><br><span class="line">plt.plot(x, y, label=<span class="string">"第一条"</span>)</span><br><span class="line"><span class="comment"># 绘制另一条曲线</span></span><br><span class="line">plt.plot(x, another_y, label=<span class="string">"第二条"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制x轴的刻度,根据列表参数的值展示刻度</span></span><br><span class="line"><span class="comment"># plt.xticks(x)</span></span><br><span class="line"><span class="comment"># plt.xticks(range(2,25))</span></span><br><span class="line"><span class="comment"># x轴刻度显示字符串, 需要构造一个和xticks第一个参数一一对应的字符串列表，这也可以解决xy轴刻度对不准问题，自定义在哪里显示刻度和显示什么</span></span><br><span class="line">_xstring_labels = [<span class="string">"string&#123;&#125;"</span>.format(i) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">plt.xticks(x, _xstring_labels, rotation=<span class="number">90</span>)  <span class="comment"># rotation可以让x刻度旋转一定角度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示y轴刻度</span></span><br><span class="line">plt.yticks(range(min(y), max(y)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加图形和坐标轴的描述信息</span></span><br><span class="line">plt.xlabel(<span class="string">"x轴信息"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y轴信息"</span>)</span><br><span class="line">plt.title(<span class="string">"图形名字"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制网格,参数alpha可以设置透明度</span></span><br><span class="line">plt.grid()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加图例，loc参数可设置图例位置</span></span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示图形</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存图形,后缀为svg可以保存为矢量图</span></span><br><span class="line"><span class="comment"># plt.savefig("./testone.png")</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="其他图形示例"><a href="#其他图形示例" class="headerlink" title="其他图形示例"></a>其他图形示例</h4><p>绘制其他图形的api和折线图几乎相同，程序参考上面就可以，下面列举一下api以供参考<br>其中 x,y为x和y轴的列表</p>
<ul>
<li>散点图<ul>
<li>plt.scatter(x,y)</li>
</ul>
</li>
<li>条形图<ul>
<li>plt.bar(x,y,width=’0.3’) #width为设置线条的粗细</li>
<li>plt.hbar(x,y,height=’0.3’)   #绘制横向的条形图,height为横向线条的粗细</li>
<li>通过设置 xticks可以实现数字和字符串的相对应</li>
</ul>
</li>
<li>直方图<ul>
<li>plt.hist(a,num_bins,normed = True)  #a为数据列表，num_bins为数据分为多少组，normed用来设置是否是频率直方图。<ul>
<li>组数 = 极差/组距 = (max(a) - min(a))/自己设置的组距，注：最大值-最小值的结果一定要能被组距整除，不然图像会出现偏差</li>
</ul>
</li>
<li>当组距不均匀时，plt.hist(a,[min(a)+i*组距 for i in range(组数)])，也就是可以传入一个列表，长度为组数，值为分组的依据</li>
<li>小技巧，设置x轴刻度：plt.xticks(range(min(a),man(a)+d,d)) ,因为range是去[),所以最后不加d的话会导致最后一个刻度丢失</li>
</ul>
</li>
<li>more<ul>
<li>matplotlib官网中有很多例子，网址：<a href="https://matplotlib.org/gallery/index.html" target="_blank" rel="noopener">https://matplotlib.org/gallery/index.html</a> ，需要的时候可根据问题自行查找。</li>
</ul>
</li>
</ul>
<h2 id="其他的一些绘制工具"><a href="#其他的一些绘制工具" class="headerlink" title="其他的一些绘制工具"></a>其他的一些绘制工具</h2><ul>
<li>百度的一个前端框架echarts：<a href="https://www.echartsjs.com/index.html" target="_blank" rel="noopener">https://www.echartsjs.com/index.html</a> ，讲道理确实很炫酷。可动态交互</li>
<li>python的图形库plotly：<a href="https://plot.ly/python/" target="_blank" rel="noopener">https://plot.ly/python/</a> ， 可以在线生成交互式、高质量的图形。可动态交互</li>
<li>Seaborn，基于matplotlib的Python数据可视化库：<a href="http://seaborn.pydata.org/" target="_blank" rel="noopener">http://seaborn.pydata.org/</a> 。 静态</li>
</ul>
<h2 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h2><p>什么是Numpy</p>
<blockquote>
<p>NumPy系统是Python的一种开源的数值计算扩展，是python科学计算的基础包。这种工具可用来存储和处理大型矩阵，具有快速高效的多维数组对象ndarray，比Python自身的嵌套列表（nested list structure)结构要高效的多。NumPy（Numeric Python）提供了许多高级的数值编程工具，如：矩阵数据类型、矢量处理，以及精密的运算库。专为进行严格的数字处理而产生。可以用来处理线性代数运算、傅里叶变换以及随机数生成等。</p>
</blockquote>
<p>下面列出了一些列numpy常用的API可供参考：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding = utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># numpy 创建数组</span></span><br><span class="line">t1 = numpy.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])  <span class="comment"># [1,2,3]</span></span><br><span class="line">t2 = numpy.array(range(<span class="number">1</span>, <span class="number">6</span>))  <span class="comment"># [1,2,3,4,5]</span></span><br><span class="line">t3 = numpy.arange(<span class="number">4</span>, <span class="number">10</span>, <span class="number">2</span>)  <span class="comment"># [4,6,8]</span></span><br><span class="line"></span><br><span class="line">print(t3.dtype)  <span class="comment"># int32 与平台有关，还可能是int64,float32等</span></span><br><span class="line">t4 = numpy.arange(<span class="number">4</span>, <span class="number">10</span>, <span class="number">2</span>, dtype=<span class="string">"int64"</span>)  <span class="comment"># 可以手动指定数值类型</span></span><br><span class="line"><span class="comment"># 调整数据类型</span></span><br><span class="line">t5 = t4.astype(<span class="string">"float32"</span>)</span><br><span class="line">print(t5.dtype)</span><br><span class="line"></span><br><span class="line">t6 = random.random()</span><br><span class="line">t6 = numpy.round(t6, <span class="number">2</span>)  <span class="comment"># round保留两位小数</span></span><br><span class="line">print(t6)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">numpy读取本地CSV文件</span></span><br><span class="line"><span class="string">api：numpy.loadtxt(fname, dtype=, comments='#', delimiter=None, converters=None, skiprows=0, usecols=None, unpack=False, ndmin=0)</span></span><br><span class="line"><span class="string">fname :文件或字符串</span></span><br><span class="line"><span class="string">dtype : 数据类型</span></span><br><span class="line"><span class="string">comments :用于指示注释开头的字符，默认值为 : '#'.</span></span><br><span class="line"><span class="string">delimiter :用于分隔值的字符，缺省值为任何空白字符，如空格 ，制表符  </span></span><br><span class="line"><span class="string">converters : dict字典 ，用来定义将对应的列转换为浮点数的函数。列入：0列是日期字符串："converters=&#123;0:datestr2num&#125;".convertes 同时也能够为丢失的数据设置缺省值："convertes=&#123;3:lambda s:float(s.strip() or 0)&#125;"</span></span><br><span class="line"><span class="string">skiprows:跳过开头的行数</span></span><br><span class="line"><span class="string">usecols : 确定那几列被读取</span></span><br><span class="line"><span class="string">unpack :是否进行转置</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="comment"># n1 = numpy.loadtxt("test.csv",delimiter=",",dtype="int") # t1= [ [0行0列 0行1列 0行2列] [1行0列 1行1列 1行2列] [2行0列 2行1列 2行2列] ]</span></span><br><span class="line"></span><br><span class="line">n2 = numpy.array([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">    [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 索引和切片</span></span><br><span class="line"><span class="comment"># print(n2[行,列]) ‘：’表所有</span></span><br><span class="line"><span class="comment"># 取一行</span></span><br><span class="line">print(n2[<span class="number">1</span>])  <span class="comment"># [5 6 7 8]</span></span><br><span class="line"><span class="comment"># 取连续多行</span></span><br><span class="line">print(n2[<span class="number">1</span>:])  <span class="comment"># [[5 6 7 8] [9 10 11 12]]</span></span><br><span class="line"><span class="comment"># 取不连续多行</span></span><br><span class="line">print(n2[[<span class="number">0</span>, <span class="number">2</span>]])  <span class="comment"># [ [1 2 3 4] [9 10 11 12]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取列</span></span><br><span class="line">print(n2[:, <span class="number">0</span>])  <span class="comment"># [1 5 9]</span></span><br><span class="line"><span class="comment"># 连续多列</span></span><br><span class="line">print(n2[:, <span class="number">2</span>:])  <span class="comment"># [ [3 4] [7 8 ] [11 12]]</span></span><br><span class="line"><span class="comment"># 不连续的列</span></span><br><span class="line">print(n2[:, [<span class="number">0</span>, <span class="number">2</span>]])  <span class="comment"># [ [1 3][5 7][9 11] ]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取单行单列和多行多列</span></span><br><span class="line">print(n2[[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>]])  <span class="comment"># 注意，这里是 [2 7]</span></span><br><span class="line">print(n2[<span class="number">1</span>, <span class="number">2</span>])  <span class="comment"># [7]</span></span><br><span class="line">print(n2[<span class="number">0</span>:<span class="number">2</span>, <span class="number">0</span>:<span class="number">3</span>])  <span class="comment"># [[1 2 3][5 6 7]] ，这里的多行多列，‘:’的区间是[)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据拼接</span></span><br><span class="line">n3 = numpy.array([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]</span><br><span class="line">])</span><br><span class="line">n4 = numpy.array([</span><br><span class="line">    [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">    [<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 横向和竖向拼接</span></span><br><span class="line">numpy.vstack((n3, n4))  <span class="comment"># [[1 2 3 4] [5 6 7 8] [9 10 11 12] [...]]</span></span><br><span class="line">numpy.hstack((n3, n4))  <span class="comment"># [ [1 2 3 4 9 10 11 12] [5 6 7 8 11 12 13 14]]</span></span><br><span class="line"><span class="comment"># numpy数组可以支持 a b = b a语法来换行或者换列，如</span></span><br><span class="line">n2[[<span class="number">1</span>, <span class="number">2</span>], :] = n2[[<span class="number">2</span>, <span class="number">1</span>], :]  <span class="comment"># 行交换</span></span><br><span class="line">n2[:, [<span class="number">0</span>, <span class="number">1</span>]] = n2[:, [<span class="number">1</span>, <span class="number">0</span>]]  <span class="comment"># 列交换</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h2><p>为什么需要pandas？</p>
<blockquote>
<p>numpy能够帮我们处理数值型数据，但这样还不足以满足需求，有时候还需要处理字符串、时间序列等。不过pandas的数值处理模块也是基于numpy的。</p>
</blockquote>
<p>什么是pandas？</p>
<blockquote>
<p>pandas 是是python的一个数据分析包、基于NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具。pandas提供了大量能使我们快速便捷地处理数据的函数和方法。对于金融行业的用户，pandas提供了大量适用于金融数据的高性能时间序列功能和工具。</p>
</blockquote>
<p>pandas的数据结构：</p>
<ul>
<li>Series：带标签(索引)的一维数组，与Numpy中的一维array类似。二者与Python基本的数据结构List也很相近，其区别是：List中的元素可以是不同的数据类型，而Array和Series中则只允许存储相同的数据类型，这样可以更有效的使用内存，提高运算效率。</li>
<li>Time- Series：以时间为索引的Series。</li>
<li>DataFrame：二维的表格型数据结构。很多功能与R中的data.frame类似。可以将DataFrame理解为Series的容器。</li>
<li>Panel ：三维的数组，可以理解为DataFrame的容器。</li>
</ul>
<p>pandas的相关用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding = 'utf-8'</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Series相关</span></span><br><span class="line"></span><br><span class="line">t1 = pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">33</span>, <span class="number">44</span>, <span class="number">5</span>])</span><br><span class="line">t2 = pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], index=list(<span class="string">"abcde"</span>))  <span class="comment"># 此时 1的索引为a,2的索引为b,以此类推</span></span><br><span class="line">print(t2.dtype)  <span class="comment"># int64</span></span><br><span class="line">temp_list = &#123;<span class="string">"name"</span>: <span class="string">"q"</span>, <span class="string">"age"</span>: <span class="number">26</span>, <span class="string">"tel"</span>: <span class="string">"10086"</span>&#125;</span><br><span class="line">t3 = pd.Series(temp_list)  <span class="comment"># 此时索引为字典的键，值为字典对应键的值</span></span><br><span class="line"><span class="comment"># tip:对一个series重新指定索引后，如果能够对应上，就取其值，如果不能，则为Nan</span></span><br><span class="line">print(t3.dtype)  <span class="comment"># object</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Series的切片和索引</span></span><br><span class="line">print(t3[:<span class="number">2</span>])  <span class="comment"># 取前3行</span></span><br><span class="line">print(t3[[<span class="string">"name"</span>, <span class="string">"age"</span>]])  <span class="comment"># 取name和age</span></span><br><span class="line">print(t1[t1 &gt; <span class="number">2</span>])  <span class="comment"># 取值大于2的</span></span><br><span class="line">print(t3.index)  <span class="comment"># 获取索引列表，类型为Index，可以使用list(t3.index) 强制转换为列表</span></span><br><span class="line">print(t3.values)  <span class="comment"># 获取值的列表,类型为numpy.ndarry，其实就是一个数组</span></span><br><span class="line"><span class="comment"># tips：ndarry中很多方法在pandas中都可以用。但pandas中的where是不同的,可以用来查找符合条件的数值，而不符合的显示为指定的值，不指定则为Nan</span></span><br><span class="line">print(<span class="string">'*'</span> * <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame相关</span></span><br><span class="line"></span><br><span class="line">p1 = pd.DataFrame(np.arange(<span class="number">12</span>).reshape(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">print(p1)</span><br><span class="line"><span class="comment"># 带索引的创建,指定行索引和列索引</span></span><br><span class="line">p2 = pd.DataFrame(np.arange(<span class="number">12</span>).reshape(<span class="number">3</span>, <span class="number">4</span>), index=list(<span class="string">"abc"</span>), columns=list(<span class="string">"WXYZ"</span>))</span><br><span class="line">print(p2)</span><br><span class="line"><span class="comment"># 传入字典创建,键为列名,每一行为具体的值，值可以对应一个列表，列表内有多少数据就有多少行。也可以传入一个内容是字典的列表，有几个字典就有几行。没有的数据为NaN</span></span><br><span class="line">d1 = &#123;<span class="string">"name"</span>: [<span class="string">"q1"</span>, <span class="string">"q2"</span>], <span class="string">"age"</span>: [<span class="number">25</span>, <span class="number">26</span>], <span class="string">"tel"</span>: [<span class="number">10086</span>, <span class="number">10010</span>]&#125;</span><br><span class="line">p3 = pd.DataFrame(d1)</span><br><span class="line">print(p3)</span><br><span class="line">d2 = [&#123;<span class="string">"name"</span>: <span class="string">"q1"</span>, <span class="string">"age"</span>: <span class="number">25</span>, <span class="string">"tel"</span>: <span class="number">10010</span>&#125;, &#123;<span class="string">"name"</span>: <span class="string">"q2"</span>, <span class="string">"tel"</span>: <span class="number">10000</span>&#125;, &#123;<span class="string">"name"</span>: <span class="string">"q3"</span>, <span class="string">"age"</span>: <span class="number">22</span>, <span class="string">"tel"</span>: <span class="number">10086</span>&#125;]</span><br><span class="line">p4 = pd.DataFrame(d2)</span><br><span class="line">print(p4)</span><br><span class="line"><span class="comment"># 前几行，tail为尾几行。info可以查看DataFrame的内存情况，多少行多少列占用多少内存等</span></span><br><span class="line">p4.head(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame取行取列，方括号内写数组，表示取行，写字符串，表示取列的索引</span></span><br><span class="line">p5 = pd.DataFrame(np.arange(<span class="number">100</span>).reshape(<span class="number">10</span>, <span class="number">10</span>), index=[<span class="string">"r1"</span>, <span class="string">"r2"</span>, <span class="string">"r3"</span>, <span class="string">"r4"</span>, <span class="string">"r5"</span>, <span class="string">"r6"</span>, <span class="string">"r7"</span>, <span class="string">"r8"</span>, <span class="string">"r9"</span>, <span class="string">"r10"</span>],</span><br><span class="line">                  columns=[<span class="string">"c1"</span>, <span class="string">"c2"</span>, <span class="string">"c3"</span>, <span class="string">"c4"</span>, <span class="string">"c5"</span>, <span class="string">"c6"</span>, <span class="string">"c7"</span>, <span class="string">"c8"</span>, <span class="string">"c9"</span>, <span class="string">"c10"</span>])</span><br><span class="line"><span class="comment"># 取前5行</span></span><br><span class="line">print(p5[:<span class="number">5</span>])</span><br><span class="line"><span class="comment"># 取固定列</span></span><br><span class="line">print(p5[<span class="string">"c2"</span>])</span><br><span class="line"><span class="comment"># 通过pandas优化过的选择方式：</span></span><br><span class="line"><span class="comment"># loc通过标签索引行数据，loc中冒号两边的数据取的方式是闭合区间[]，而不是[)</span></span><br><span class="line"><span class="comment"># iloc通过位置获取行数据</span></span><br><span class="line">print(p5.loc[<span class="string">"r4"</span>, <span class="string">"c5"</span>])  <span class="comment"># 取r4行r5列</span></span><br><span class="line">print(p5.loc[<span class="string">"r4"</span>])  <span class="comment"># 取r4行</span></span><br><span class="line">print(p5.loc[[<span class="string">"r1"</span>, <span class="string">"r3"</span>]])  <span class="comment"># 取多行多列语法 如r1行和r3行</span></span><br><span class="line">print(p5.iloc[:, <span class="number">2</span>])  <span class="comment"># 取第2列,索引从0开始</span></span><br><span class="line">print(p5.iloc[<span class="number">5</span>:, <span class="number">3</span>:])  <span class="comment">#取第6行以后和第3列以后的数据</span></span><br><span class="line">print(p5[p5[<span class="string">"c7"</span>]&gt;<span class="number">70</span>])  <span class="comment">#取c7列大于70的所有行</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#缺失数据NaN的处理</span></span><br><span class="line">p6 = pd.DataFrame(np.array([<span class="number">0</span>,<span class="number">1</span>,np.NaN,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,np.NaN,<span class="number">10</span>,<span class="number">11</span>]).reshape(<span class="number">3</span>, <span class="number">4</span>), index=list(<span class="string">"abc"</span>), columns=list(<span class="string">"WXYZ"</span>))</span><br><span class="line">print(p6)</span><br><span class="line">print(pd.notnull(p6))   <span class="comment">#p6中，不为NaN的值显示为true，为NaN的值显示为false</span></span><br><span class="line">print(p6[pd.notnull(p6[<span class="string">"X"</span>])])  <span class="comment">#选取p6的X列中的值不为NaN的所有行</span></span><br><span class="line">print(p6.dropna(axis=<span class="number">0</span>,how=<span class="string">"any"</span>))    <span class="comment">#删除DataFrame中值为NaN的axis，axis=0为行，1为列。how的值为“any”表示这个轴只要有NaN就删，值为“all”表示所有值都为NaN才删</span></span><br><span class="line"><span class="comment">#填充NaN为指定数据</span></span><br><span class="line">print(p6.fillna(<span class="number">0</span>))     <span class="comment"># 将NaN填充为0</span></span><br><span class="line"><span class="comment"># 不过为了数据完整性，一般不会填充为0或者随便填一个数值，而是会填充为均值,Pandas计算均值的时候不会算上NaN值</span></span><br><span class="line">print(p6.mean())    <span class="comment">#此时mean算的均值是每一列的均值</span></span><br><span class="line">print(p6.fillna(p6.mean()))</span><br><span class="line">print(p6[<span class="string">"X"</span>].fillna(p6[<span class="string">"X"</span>].mean()))   <span class="comment">#只填充某一列的NaN值，如果要改变原数据,可以前面添加 p6["X"] =</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pandas读取外部数据</span></span><br><span class="line"><span class="comment"># csv文件</span></span><br><span class="line"><span class="comment"># result = pd.read_csv("./test.csv")</span></span><br><span class="line"><span class="comment"># 读取sql，传入sql语句和connection</span></span><br><span class="line"><span class="comment"># pd.read_sql(sql_sentence,connection)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="写到这里"><a href="#写到这里" class="headerlink" title="写到这里"></a>写到这里</h1><p>先上一张python数据分析的知识框架</p>
<img src="/2020/01/11/python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80/python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6.jpg" class="" title="数据库结果">
<p>其中有一些还没有写到，也不打算继续挖坑了，暂且记录在下面：</p>
<ul>
<li>Scipy<ul>
<li>Scipy是一组专门解决科学计算中各种标准问题域的包的集合。</li>
</ul>
</li>
<li>statsmodels<ul>
<li><a href="https://github.com/statsmodels/statsmodels" target="_blank" rel="noopener">https://github.com/statsmodels/statsmodels</a></li>
</ul>
</li>
<li>scikit-learn<ul>
<li><a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">http://scikit-learn.org/stable/</a></li>
</ul>
</li>
</ul>
<p>这段时间大概了解了一下python数据分析的知识框架，写的不是很详细，这里有一篇更详细的文章写得不错先mark一下：<a href="https://www.cnblogs.com/nxld/p/6058998.html" target="_blank" rel="noopener">https://www.cnblogs.com/nxld/p/6058998.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/11/python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80/" data-id="ck9sdq080000670vbcghg2hln" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" rel="tag">数据分析</a></li></ul>

    </footer>
  </div>
  
</article>




  
    <article id="post-scrapy爬取腾讯招聘岗位到mongoDB中" class="wow slideInRight article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/01/11/scrapy%E7%88%AC%E5%8F%96%E8%85%BE%E8%AE%AF%E6%8B%9B%E8%81%98%E5%B2%97%E4%BD%8D%E5%88%B0mongoDB%E4%B8%AD/" class="article-date">
  <time class="post-time" datetime="2020-01-11T05:42:20.000Z" itemprop="datePublished">
    <span class="post-month">1月</span><br/>
    <span class="post-day">11</span>
  </time>
</a>
   
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/11/scrapy%E7%88%AC%E5%8F%96%E8%85%BE%E8%AE%AF%E6%8B%9B%E8%81%98%E5%B2%97%E4%BD%8D%E5%88%B0mongoDB%E4%B8%AD/">scrapy爬取腾讯招聘岗位到mongoDB中</a>
    </h1>
  

        <div>
          
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a>
  </div>

          
              

          
        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近正值一年一度的春招时刻，看到一堆学弟学妹忙着准备简历和面试，默默地打开了腾讯的招聘网站，<a href="https://hr.tencent.com/position.php" target="_blank" rel="noopener">https://hr.tencent.com/position.php</a> ，打算爬取一下所有的岗位信息，工作地点，类型，以及日期。</p>
<p>借此来复习一下Scrapy这个框架的基本使用，爬取的数据则是存储在本地的MongoDB数据库中。<br>Scrapy的架构图如下：</p>
<img src="/2020/01/11/scrapy%E7%88%AC%E5%8F%96%E8%85%BE%E8%AE%AF%E6%8B%9B%E8%81%98%E5%B2%97%E4%BD%8D%E5%88%B0mongoDB%E4%B8%AD/Scrapy.jpg" class="" title="Scrapy架构">

<p>Scrapy主要包括了以下组件：</p>
<ul>
<li>引擎(Scrapy)<ul>
<li>用来处理整个系统的数据流, 触发事务(框架核心)</li>
</ul>
</li>
<li>调度器(Scheduler)<ul>
<li>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>
</ul>
</li>
<li>下载器(Downloader)<ul>
<li>用于下载网页内容, 并将网页内容返回给spider(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
</ul>
</li>
<li>爬虫(Spiders)<ul>
<li>爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</li>
</ul>
</li>
<li>项目管道(Pipeline)<ul>
<li>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
</ul>
</li>
<li>下载器中间件(Downloader Middlewares)<ul>
<li>位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</li>
</ul>
</li>
<li>爬虫中间件(Spider Middlewares)<ul>
<li>介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。</li>
</ul>
</li>
<li>调度中间件(Scheduler Middewares)<ul>
<li>介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</li>
</ul>
</li>
</ul>
<p>Scrapy运行流程大概如下：</p>
<ul>
<li>引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>
<li>引擎把URL封装成一个请求(Request)传给下载器</li>
<li>下载器把资源下载下来，并封装成应答包(Response)</li>
<li>爬虫解析Response</li>
<li>解析出实体（Item）,则交给实体管道进行进一步的处理</li>
<li>解析出的是链接（URL）,则把URL交给调度器等待抓取</li>
</ul>
<h2 id="建立项目"><a href="#建立项目" class="headerlink" title="建立项目"></a>建立项目</h2><p>假设已经安装好mongodb数据库和scrapy。<br>windows下，cd到对应的文件夹目录</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject myspiders</span><br><span class="line">#cd到 myspiders 文件夹下</span><br><span class="line">scrapy genspider myspider tencent.com</span><br></pre></td></tr></table></figure>

<p>这之后在项目的spiders文件夹下，会生成一个 myspider.py 的文件，修改里面的 start_urls为我们的起始地址 <a href="https://hr.tencent.com/position.php" target="_blank" rel="noopener">https://hr.tencent.com/position.php</a><br>settings.py文件中，去掉 ITEM_PIPELINES 的注释，以使pipelines.py文件可以使用<br>settings.py中添加一行log等级，过滤一下多余log，LOG_LEVEL = “WARNING”<br>settings.py中的USER_AGENT取消注释，并改为自己浏览器的useragent</p>
<blockquote>
<p>Request参数参考：scrapy.http.Request(url[, callback, method=’GET’, headers, body, cookies, meta, encoding=’utf-8’, priority=0, dont_filter=False, errback])</p>
</blockquote>
<p>myspider.py代码如下：</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MyspiderSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &#39;myspider&#39;</span><br><span class="line">    allowed_domains &#x3D; [&#39;tencent.com&#39;]</span><br><span class="line">    start_urls &#x3D; [&#39;https:&#x2F;&#x2F;hr.tencent.com&#x2F;position.php&#39;]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        tr_list &#x3D; response.xpath(&quot;&#x2F;&#x2F;table[@class&#x3D;&#39;tablelist&#39;]&#x2F;tr&quot;)[1:-1]</span><br><span class="line">        for tr in tr_list:</span><br><span class="line">            item&#x3D;&#123;&#125;</span><br><span class="line">            item[&quot;title&quot;] &#x3D; tr.xpath(&quot;.&#x2F;td[1]&#x2F;a&#x2F;text()&quot;).extract_first()</span><br><span class="line">            item[&quot;type&quot;] &#x3D; tr.xpath(&quot;.&#x2F;td[2]&#x2F;text()&quot;).extract_first()</span><br><span class="line">            item[&quot;position&quot;] &#x3D; tr.xpath(&quot;.&#x2F;td[4]&#x2F;text()&quot;).extract_first()</span><br><span class="line">            item[&quot;date&quot;] &#x3D; tr.xpath(&quot;.&#x2F;td[5]&#x2F;text()&quot;).extract_first()</span><br><span class="line">            yield item</span><br><span class="line"></span><br><span class="line">        #找下一页地址</span><br><span class="line">        next_url &#x3D; response.xpath(&quot;&#x2F;&#x2F;a[@id&#x3D;&#39;next&#39;]&#x2F;@href&quot;).extract_first()</span><br><span class="line">        if next_url !&#x3D; &quot;javascript:;&quot;:</span><br><span class="line">            next_url &#x3D; &quot;http:&#x2F;&#x2F;hr.tencent.com&#x2F;&quot;+next_url</span><br><span class="line">            yield scrapy.Request(</span><br><span class="line">                next_url,</span><br><span class="line">                callback&#x3D;self.parse</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>

<p>pipelines.py如下：</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># Define your item pipelines here</span><br><span class="line">#</span><br><span class="line"># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="line"># See: https:&#x2F;&#x2F;doc.scrapy.org&#x2F;en&#x2F;latest&#x2F;topics&#x2F;item-pipeline.html</span><br><span class="line"></span><br><span class="line">from pymongo import MongoClient</span><br><span class="line"></span><br><span class="line">client &#x3D; MongoClient(host&#x3D;&quot;127.0.0.1&quot;,port&#x3D;27017)</span><br><span class="line">collection &#x3D; client[&quot;tencent&quot;][&quot;job&quot;]</span><br><span class="line"></span><br><span class="line">class MyspidersPipeline(object):</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        print(item)</span><br><span class="line">        collection.insert(item)</span><br><span class="line">        return item</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>写完之后返回命令行，运行</p>
<figure class="highlight plain"><figcaption><span>python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl myspider</span><br></pre></td></tr></table></figure>

<p>程序结果，MongoDB数据库中的结果如下：</p>
<img src="/2020/01/11/scrapy%E7%88%AC%E5%8F%96%E8%85%BE%E8%AE%AF%E6%8B%9B%E8%81%98%E5%B2%97%E4%BD%8D%E5%88%B0mongoDB%E4%B8%AD/jobresult.jpg" class="" title="数据库结果">


<h2 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h2><p>scrapy这个框架极其强大，它也提供了多种类型爬虫的基类，如BaseSpider、sitemap爬虫等，这里只是用到了最基本的几个用法。之后有机会借用其他例子来更深入理解。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/01/11/scrapy%E7%88%AC%E5%8F%96%E8%85%BE%E8%AE%AF%E6%8B%9B%E8%81%98%E5%B2%97%E4%BD%8D%E5%88%B0mongoDB%E4%B8%AD/" data-id="ck9sdq084000c70vb0od8eogh" class="article-share-link">分享</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Scrapy/" rel="tag">Scrapy</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%88%AC%E8%99%AB/" rel="tag">爬虫</a></li></ul>

    </footer>
  </div>
  
</article>




  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">next &amp;raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <h1 class="blog-title">Pokemonlei的博客</h1>
    <h2 class="blog-subtitle"></h2>
    <ul class="blog-link">
     
          <a href="/" title="Home">
            <li>主页</li>
          </a>
        
          <a href="/archives" title="Archives">
            <li>归档</li>
          </a>
        
          <a href="/categories" title="Categories">
            <li>分类</li>
          </a>
        
          <a href="/tags" title="Tags">
            <li>标签</li>
          </a>
        
    </ul>
  </div>
</div>

  
    <div class="widget-wrap">
  <h3 class="widget-title"></h3>
  <div class="widget">
    <img class="avatar" src="https://avatars0.githubusercontent.com/u/20333903?v=3&amp;s=460">
    <h2 class="author">pokemonlei</h2>
    <h3 class="description">陈磊的博客 | pokemonlei</h3>
    <div class="count-box">
      <a href="/archives"><div><strong>11</strong><br>文章</div></a>
      <a href="/categories"><div><strong>6</strong><br>分类</div></a>
      <a href="/tags"><div><strong>11</strong><br>标签</div></a>
    </div>



    <div class="social-link">
      
        <a class="hvr-bounce-in" href="http://github.com/ShanaMaid" target="_blank" title="Github">
          Github
        </a>
      
    </div>

    <div class="friend-link">
      <h2>友情链接</h2>
      
        <a class="hvr-bounce-in" href="http://blog.shanamaid.top/" target="_blank" title="ShanaMaid">
          ShanaMaid
        </a>
      
    </div>
  </div>
</div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy;2019 - 2020 pokemonlei<br>
      由<a href="http://hexo.io/" target="_blank">Hexo</a>强力驱动 | 
      主题-<a href="https://github.com/ShanaMaid/hexo-theme-shana" target="_blank" rel="noopener">Shana</a>
      
    </div>
    
  </div>
</footer>
    </div>
    

<script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="//apps.bdimg.com/libs/wow/0.1.6/wow.min.js"></script>
<script>
new WOW().init();
</script>   


  
<link rel="stylesheet" href="/plugin/fancybox/jquery.fancybox.css">

  
<script src="/plugin/fancybox/jquery.fancybox.pack.js"></script>




  
<link rel="stylesheet" href="/plugin/galmenu/GalMenu.css">

  
<script src="/plugin/galmenu/GalMenu.js"></script>

  <div class="GalMenu GalDropDown">
      <div class="circle" id="gal">
        <div class="ring">
          
            <a href="/" title="" class="menuItem">首页</a>
          
            <a href="/tags" title="" class="menuItem">标签</a>
          
            <a href="/categories" title="" class="menuItem">分类</a>
          
            <a href="/archives" title="" class="menuItem">归档</a>
          
            <a href="/xxxxxxxxx" title="" class="menuItem">xxx</a>
          
            <a href="/xxxxxxx" title="" class="menuItem">xxxx</a>
          
        </div>
        
          <audio id="audio" src="#"></audio>
        
      </div> 
</div>
<div id="overlay" style="opacity: 1; cursor: pointer;"></div>
  <script type="text/javascript">var items = document.querySelectorAll('.menuItem');
    for (var i = 0,
    l = items.length; i < l; i++) {
      items[i].style.left = (50 - 35 * Math.cos( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%";
      items[i].style.top = (50 + 35 * Math.sin( - 0.5 * Math.PI - 2 * (1 / l) * i * Math.PI)).toFixed(4) + "%"
    }</script>
<script type="text/javascript">
  $(document).ready(function() {
    $('body').GalMenu({
      'menu': 'GalDropDown'
    })
  });
</script>

  <section class="hidden-xs"> 
  <ul class="cb-slideshow"> 
    <li><span>苟利</span></li> 
    <li><span>国家</span></li> 
    <li><span>生死以</span></li> 
    <li><span>岂能</span></li> 
    <li><span>祸福</span></li> 
    <li><span>趋避之</span></li> 
  </ul>
</section>

<script src="/js/script.js"></script>




  </div>
</body>
</html>